"""FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""

from __future__ import annotations

import asyncio
import json
import os
import re
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple

import chromadb
import httpx
import nltk
import numpy as np
import ollama
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import (
    DateTime, Float, ForeignKey, Index, Integer, JSON, String, Text,
    func, select
)
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship

# –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
RUSSIAN_STOP_WORDS = set(stopwords.words('russian'))

# üîí –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –°–ï–ú–ê–§–û–† –î–õ–Ø –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –ù–ê–ì–†–£–ó–ö–ò –ù–ê OLLAMA
# –ú–∞–∫—Å–∏–º—É–º 1 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Ollama –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º—ã
OLLAMA_SEMAPHORE = asyncio.Semaphore(1)

@dataclass
class SemanticEntity:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—É—â–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM."""
    entity_type: str
    value: str
    confidence: float
    context: str

@dataclass
class ThematicCluster:
    """–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä —Å—Ç–∞—Ç–µ–π."""
    cluster_id: str
    theme: str
    keywords: List[str]
    articles_count: int
    semantic_density: float

@dataclass
class AIThought:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—ã—Å–ª—å –ò–ò —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
    thought_id: str
    stage: str  # analyzing, connecting, evaluating, optimizing
    content: str
    confidence: float
    semantic_weight: float
    related_concepts: List[str]
    reasoning_chain: List[str]
    timestamp: datetime
    
@dataclass
class SemanticConnection:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏."""
    source_concept: str
    target_concept: str
    connection_type: str  # semantic, causal, hierarchical, temporal
    strength: float
    evidence: List[str]
    context_keywords: Set[str]

class WebSocketManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""

    def __init__(self) -> None:
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, client_id: str) -> None:
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞."""
        await websocket.accept()
        self.active_connections[client_id] = websocket
        print(f"üîå WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω: {client_id}")

    def disconnect(self, client_id: str) -> None:
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            print(f"üîå WebSocket –æ—Ç–∫–ª—é—á–µ–Ω: {client_id}")

    async def send_progress(self, client_id: str, message: dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–ª–∏–µ–Ω—Ç—É."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json(message)
                print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω {client_id}: {message}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ {client_id}: {e}")

    async def send_error(self, client_id: str, error: str, details: str = "") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ—à–∏–±–∫–∏ –∫–ª–∏–µ–Ω—Ç—É."""
        await self.send_progress(client_id, {
            "type": "error",
            "message": error,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })

    async def send_step(self, client_id: str, step: str, current: int, total: int, details: str = "") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ."""
        await self.send_progress(client_id, {
            "type": "progress",
            "step": step,
            "current": current,
            "total": total,
            "percentage": round((current / total) * 100) if total > 0 else 0,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })

    async def send_ollama_info(self, client_id: str, info: dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞–±–æ—Ç–µ Ollama."""
        await self.send_progress(client_id, {
            "type": "ollama",
            "info": info,
            "timestamp": datetime.now().isoformat()
        })

    async def send_ai_thinking(self, client_id: str, thought: str, thinking_stage: str = "analyzing", emoji: str = "ü§î") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ '–º—ã—Å–ª–µ–π' –ò–ò –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json({
                    "type": "ai_thinking",
                    "thought": thought,
                    "thinking_stage": thinking_stage,
                    "emoji": emoji,
                    "timestamp": datetime.now().isoformat()
                })
                print(f"üß† –ú—ã—Å–ª—å –ò–ò –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ {client_id}: {thought[:50]}...")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –º—ã—Å–ª–∏ {client_id}: {e}")
    
    async def send_enhanced_ai_thinking(self, client_id: str, ai_thought: AIThought) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º—ã—Å–ª–µ–π –ò–ò —Å –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json({
                    "type": "enhanced_ai_thinking",
                    "thought_id": ai_thought.thought_id,
                    "stage": ai_thought.stage,
                    "content": ai_thought.content,
                    "confidence": ai_thought.confidence,
                    "semantic_weight": ai_thought.semantic_weight,
                    "related_concepts": ai_thought.related_concepts,
                    "reasoning_chain": ai_thought.reasoning_chain,
                    "timestamp": ai_thought.timestamp.isoformat()
                })
                print(f"üî¨ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –º—ã—Å–ª—å –ò–ò –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ {client_id}: {ai_thought.stage}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –º—ã—Å–ª–∏ {client_id}: {e}")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
chroma_client: Optional[Any] = None
tfidf_vectorizer: Optional[Any] = None

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã
websocket_manager = WebSocketManager()
thought_generator = IntelligentThoughtGenerator()
rag_manager = AdvancedRAGManager()
cumulative_manager = CumulativeIntelligenceManager()

app = FastAPI()

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
static_dir = Path(__file__).parent.parent.parent / "frontend"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
static_dir = Path(__file__).parent.parent.parent / "frontend"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")


class Base(DeclarativeBase):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–µ–π."""


class Domain(Base):
    """–ú–æ–¥–µ–ª—å –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è."""

    __tablename__ = "domains"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(255), unique=True, index=True)
    display_name: Mapped[str] = mapped_column(String(500))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    language: Mapped[str] = mapped_column(String(10), default="ru")
    category: Mapped[str] = mapped_column(String(100), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_active: Mapped[bool] = mapped_column(default=True)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_posts: Mapped[int] = mapped_column(Integer, default=0)
    total_analyses: Mapped[int] = mapped_column(Integer, default=0)
    last_analysis_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    posts: Mapped[List["WordPressPost"]] = relationship("WordPressPost", back_populates="domain_ref", cascade="all, delete-orphan")
    analyses: Mapped[List["AnalysisHistory"]] = relationship("AnalysisHistory", back_populates="domain_ref", cascade="all, delete-orphan")
    themes: Mapped[List["ThematicGroup"]] = relationship("ThematicGroup", back_populates="domain_ref", cascade="all, delete-orphan")


class ThematicGroup(Base):
    """–ú–æ–¥–µ–ª—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏."""

    __tablename__ = "thematic_groups"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    name: Mapped[str] = mapped_column(String(200))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    keywords: Mapped[list[str]] = mapped_column(JSON)
    semantic_signature: Mapped[str] = mapped_column(Text)  # TF-IDF –ø–æ–¥–ø–∏—Å—å —Ç–µ–º—ã
    articles_count: Mapped[int] = mapped_column(Integer, default=0)
    avg_semantic_density: Mapped[float] = mapped_column(Float, default=0.0)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="themes")
    posts: Mapped[List["WordPressPost"]] = relationship("WordPressPost", back_populates="thematic_group")

    __table_args__ = (
        Index("idx_thematic_groups_domain_semantic", "domain_id", "avg_semantic_density"),
    )


class WordPressPost(Base):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å—Ç–∞—Ç–µ–π WordPress —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–ª—è–º–∏."""

    __tablename__ = "wordpress_posts"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    thematic_group_id: Mapped[int] = mapped_column(Integer, ForeignKey("thematic_groups.id"), nullable=True, index=True)
    wp_post_id: Mapped[int] = mapped_column(Integer)

    # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –ø–æ–ª—è
    title: Mapped[str] = mapped_column(Text)
    content: Mapped[str] = mapped_column(Text)
    excerpt: Mapped[str] = mapped_column(Text, nullable=True)
    link: Mapped[str] = mapped_column(Text, index=True)

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –¥–ª—è LLM
    semantic_summary: Mapped[str] = mapped_column(Text, nullable=True)  # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è LLM
    key_concepts: Mapped[list[str]] = mapped_column(JSON, default=list)  # –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
    entity_mentions: Mapped[list[dict]] = mapped_column(JSON, default=list)  # –£–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
    content_type: Mapped[str] = mapped_column(String(50), nullable=True)  # —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–≥–∞–π–¥, –æ–±–∑–æ—Ä, –Ω–æ–≤–æ—Å—Ç—å)
    difficulty_level: Mapped[str] = mapped_column(String(20), nullable=True)  # —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    target_audience: Mapped[str] = mapped_column(String(100), nullable=True)  # —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è

    # –ú–µ—Ç—Ä–∏–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    content_quality_score: Mapped[float] = mapped_column(Float, default=0.0)
    semantic_richness: Mapped[float] = mapped_column(Float, default=0.0)  # –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏–∫–∏
    linkability_score: Mapped[float] = mapped_column(Float, default=0.0)  # –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ —Å—Ç–∞—Ç—É—Å—ã
    published_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_analyzed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="posts")
    thematic_group: Mapped["ThematicGroup"] = relationship("ThematicGroup", back_populates="posts")
    embeddings: Mapped[List["ArticleEmbedding"]] = relationship("ArticleEmbedding", back_populates="post", cascade="all, delete-orphan")

    __table_args__ = (
        Index("idx_wp_posts_domain_theme", "domain_id", "thematic_group_id"),
        Index("idx_wp_posts_semantic_scores", "semantic_richness", "linkability_score"),
        Index("idx_wp_posts_published", "published_at"),
    )


class ArticleEmbedding(Base):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏."""

    __tablename__ = "article_embeddings"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)

    # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embedding_type: Mapped[str] = mapped_column(String(50))  # 'title', 'content', 'summary', 'full'
    vector_model: Mapped[str] = mapped_column(String(100))  # –º–æ–¥–µ–ª—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    embedding_vector: Mapped[str] = mapped_column(Text)  # JSON –≤–µ–∫—Ç–æ—Ä–∞
    dimension: Mapped[int] = mapped_column(Integer)  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context_window: Mapped[int] = mapped_column(Integer, nullable=True)  # —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞
    preprocessing_info: Mapped[dict] = mapped_column(JSON, default=dict)  # –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ
    quality_metrics: Mapped[dict] = mapped_column(JSON, default=dict)  # –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    post: Mapped["WordPressPost"] = relationship("WordPressPost", back_populates="embeddings")

    __table_args__ = (
        Index("idx_embeddings_post_type", "post_id", "embedding_type"),
    )


class SemanticConnection(Base):
    """–ú–æ–¥–µ–ª—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π."""

    __tablename__ = "semantic_connections"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    source_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    target_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)

    # –¢–∏–ø—ã —Å–≤—è–∑–µ–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
    connection_type: Mapped[str] = mapped_column(String(50))  # 'semantic', 'topical', 'hierarchical', 'sequential', 'complementary'
    strength: Mapped[float] = mapped_column(Float)  # —Å–∏–ª–∞ —Å–≤—è–∑–∏ (0.0 - 1.0)
    confidence: Mapped[float] = mapped_column(Float)  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–≤—è–∑–∏

    # –ù–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    usage_count: Mapped[int] = mapped_column(Integer, default=0)  # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–≤—è–∑—å –±—ã–ª–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞
    success_rate: Mapped[float] = mapped_column(Float, default=0.0)  # —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è
    evolution_score: Mapped[float] = mapped_column(Float, default=0.0)  # —ç–≤–æ–ª—é—Ü–∏—è —Å–≤—è–∑–∏ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
    connection_context: Mapped[str] = mapped_column(Text, nullable=True)  # –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–≤—è–∑–∏
    suggested_anchor: Mapped[str] = mapped_column(String(200), nullable=True)  # –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π –∞–Ω–∫–æ—Ä
    alternative_anchors: Mapped[list[str]] = mapped_column(JSON, default=list)  # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∞–Ω–∫–æ—Ä—ã
    bidirectional: Mapped[bool] = mapped_column(default=False)  # –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å–≤—è–∑—å

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
    semantic_tags: Mapped[list[str]] = mapped_column(JSON, default=list)  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏
    theme_intersection: Mapped[str] = mapped_column(String(200), nullable=True)  # –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ç–µ–º

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    validated_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    last_recommended_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    __table_args__ = (
        Index("idx_semantic_connections_strength", "strength"),
        Index("idx_semantic_connections_usage", "usage_count", "success_rate"),
        Index("idx_semantic_connections_evolution", "evolution_score"),
    )


class LinkRecommendation(Base):
    """–ú–æ–¥–µ–ª—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –∏ —ç–≤–æ–ª—é—Ü–∏–µ–π."""

    __tablename__ = "link_recommendations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    source_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    target_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    anchor_text: Mapped[str] = mapped_column(String(300))
    reasoning: Mapped[str] = mapped_column(Text)
    quality_score: Mapped[float] = mapped_column(Float, default=0.0)  # –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

    # –ù–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    generation_count: Mapped[int] = mapped_column(Integer, default=1)  # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞—Å—å
    improvement_iterations: Mapped[int] = mapped_column(Integer, default=0)  # –∏—Ç–µ—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è
    status: Mapped[str] = mapped_column(String(50), default='active')  # active, deprecated, improved

    # –°–≤—è–∑—å —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é
    semantic_connection_id: Mapped[int] = mapped_column(Integer, ForeignKey("semantic_connections.id"), nullable=True)

    # –≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    previous_version_id: Mapped[int] = mapped_column(Integer, ForeignKey("link_recommendations.id"), nullable=True)
    improvement_reason: Mapped[str] = mapped_column(Text, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")
    source_post: Mapped["WordPressPost"] = relationship("WordPressPost", foreign_keys=[source_post_id])
    target_post: Mapped["WordPressPost"] = relationship("WordPressPost", foreign_keys=[target_post_id])
    semantic_connection: Mapped["SemanticConnection"] = relationship("SemanticConnection")
    previous_version: Mapped["LinkRecommendation"] = relationship("LinkRecommendation", remote_side=[id])

    __table_args__ = (
        Index("idx_link_recommendations_quality", "quality_score"),
        Index("idx_link_recommendations_status", "status"),
        Index("idx_link_recommendations_generation", "generation_count"),
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å–≤—è–∑–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫-—Ü–µ–ª—å –≤ —Ä–∞–º–∫–∞—Ö –¥–æ–º–µ–Ω–∞
        Index("idx_link_recommendations_unique", "domain_id", "source_post_id", "target_post_id"),
    )


class ThematicClusterAnalysis(Base):
    """–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –∏—Ö —ç–≤–æ–ª—é—Ü–∏–∏."""

    __tablename__ = "thematic_cluster_analysis"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)

    # –ö–ª–∞—Å—Ç–µ—Ä
    cluster_name: Mapped[str] = mapped_column(String(200))
    cluster_keywords: Mapped[list[str]] = mapped_column(JSON)
    cluster_description: Mapped[str] = mapped_column(Text)

    # –°—Ç–∞—Ç—å–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    article_ids: Mapped[list[int]] = mapped_column(JSON)  # ID —Å—Ç–∞—Ç–µ–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    article_count: Mapped[int] = mapped_column(Integer)

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    coherence_score: Mapped[float] = mapped_column(Float)  # —Å–≤—è–∑–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∞
    diversity_score: Mapped[float] = mapped_column(Float)  # —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    linkability_potential: Mapped[float] = mapped_column(Float)  # –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –ª–∏–Ω–∫–æ–≤–∫–∏

    # –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
    related_clusters: Mapped[dict] = mapped_column(JSON, default=dict)  # —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –∏—Ö –≤–µ—Å–∞
    cross_cluster_opportunities: Mapped[list[dict]] = mapped_column(JSON, default=list)  # –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö —Å–≤—è–∑–µ–π

    # –≠–≤–æ–ª—é—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
    evolution_stage: Mapped[str] = mapped_column(String(50), default='emerging')  # emerging, mature, declining
    growth_trend: Mapped[float] = mapped_column(Float, default=0.0)  # —Ç—Ä–µ–Ω–¥ —Ä–æ—Å—Ç–∞

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")

    __table_args__ = (
        Index("idx_thematic_cluster_coherence", "coherence_score"),
        Index("idx_thematic_cluster_linkability", "linkability_potential"),
        Index("idx_thematic_cluster_evolution", "evolution_stage", "growth_trend"),
    )


class CumulativeInsight(Base):
    """–ú–æ–¥–µ–ª—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""

    __tablename__ = "cumulative_insights"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)

    # –¢–∏–ø –∏–Ω—Å–∞–π—Ç–∞
    insight_type: Mapped[str] = mapped_column(String(100))  # 'pattern', 'gap', 'opportunity', 'trend'
    insight_category: Mapped[str] = mapped_column(String(100))  # 'semantic', 'structural', 'thematic'

    # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–∞
    title: Mapped[str] = mapped_column(String(300))
    description: Mapped[str] = mapped_column(Text)
    evidence: Mapped[dict] = mapped_column(JSON)  # –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ

    # –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
    impact_score: Mapped[float] = mapped_column(Float)  # –≤–∞–∂–Ω–æ—Å—Ç—å –∏–Ω—Å–∞–π—Ç–∞
    confidence_level: Mapped[float] = mapped_column(Float)  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∏–Ω—Å–∞–π—Ç–µ
    actionability: Mapped[float] = mapped_column(Float)  # –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å

    # –°–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
    related_posts: Mapped[list[int]] = mapped_column(JSON, default=list)
    related_clusters: Mapped[list[int]] = mapped_column(JSON, default=list)
    related_connections: Mapped[list[int]] = mapped_column(JSON, default=list)

    # –°—Ç–∞—Ç—É—Å –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
    status: Mapped[str] = mapped_column(String(50), default='discovered')  # discovered, validated, applied
    applied_count: Mapped[int] = mapped_column(Integer, default=0)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    validated_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")

    __table_args__ = (
        Index("idx_cumulative_insights_impact", "impact_score"),
        Index("idx_cumulative_insights_type", "insight_type", "insight_category"),
        Index("idx_cumulative_insights_status", "status"),
    )


class AnalysisHistory(Base):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏."""

    __tablename__ = "analysis_history"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)

    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    posts_analyzed: Mapped[int] = mapped_column(Integer)
    connections_found: Mapped[int] = mapped_column(Integer)
    recommendations_generated: Mapped[int] = mapped_column(Integer)

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    recommendations: Mapped[list[dict]] = mapped_column(JSON)
    thematic_analysis: Mapped[dict] = mapped_column(JSON, default=dict)  # –∞–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏–∫
    semantic_metrics: Mapped[dict] = mapped_column(JSON, default=dict)  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    quality_assessment: Mapped[dict] = mapped_column(JSON, default=dict)  # –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞

    # LLM –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    llm_model_used: Mapped[str] = mapped_column(String(100))
    llm_context_size: Mapped[int] = mapped_column(Integer, nullable=True)
    processing_time_seconds: Mapped[float] = mapped_column(Float, nullable=True)

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    completed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="analyses")

    __table_args__ = (
        Index("idx_analysis_history_domain_date", "domain_id", "created_at"),
    )


class ModelConfiguration(Base):
    """–ú–æ–¥–µ–ª—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π."""

    __tablename__ = "model_configurations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    model_name: Mapped[str] = mapped_column(String(200), unique=True, index=True)
    display_name: Mapped[str] = mapped_column(String(300))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    model_type: Mapped[str] = mapped_column(String(50))  # ollama, openai, anthropic

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    default_parameters: Mapped[dict] = mapped_column(JSON, default=dict)  # temperature, top_p, etc.
    context_size: Mapped[int] = mapped_column(Integer, default=4096)
    max_tokens: Mapped[int] = mapped_column(Integer, default=2048)

    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
    seo_optimized_params: Mapped[dict] = mapped_column(JSON, default=dict)
    benchmark_params: Mapped[dict] = mapped_column(JSON, default=dict)
    creative_params: Mapped[dict] = mapped_column(JSON, default=dict)

    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    avg_tokens_per_second: Mapped[float] = mapped_column(Float, nullable=True)
    memory_usage_mb: Mapped[float] = mapped_column(Float, nullable=True)
    quality_score: Mapped[float] = mapped_column(Float, nullable=True)

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    is_active: Mapped[bool] = mapped_column(default=True)
    is_available: Mapped[bool] = mapped_column(default=False)  # –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è
    last_checked_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    benchmark_runs: Mapped[List["BenchmarkRun"]] = relationship("BenchmarkRun", back_populates="model_config")

    __table_args__ = (
        Index("idx_model_configs_active_available", "is_active", "is_available"),
    )


class BenchmarkRun(Base):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""

    __tablename__ = "benchmark_runs"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(300))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    benchmark_type: Mapped[str] = mapped_column(String(100))  # seo_basic, seo_advanced, performance

    # –°—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏
    model_config_id: Mapped[int] = mapped_column(Integer, ForeignKey("model_configurations.id"), index=True)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞
    test_cases_config: Mapped[dict] = mapped_column(JSON)  # –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤
    iterations: Mapped[int] = mapped_column(Integer, default=3)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results: Mapped[dict] = mapped_column(JSON)  # –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    metrics: Mapped[dict] = mapped_column(JSON)  # –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    started_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    completed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    duration_seconds: Mapped[float] = mapped_column(Float, nullable=True)

    # –°—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    status: Mapped[str] = mapped_column(String(50), default='pending')  # pending, running, completed, failed
    error_message: Mapped[str] = mapped_column(Text, nullable=True)

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    overall_score: Mapped[float] = mapped_column(Float, nullable=True)
    quality_score: Mapped[float] = mapped_column(Float, nullable=True)
    performance_score: Mapped[float] = mapped_column(Float, nullable=True)
    efficiency_score: Mapped[float] = mapped_column(Float, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    model_config: Mapped["ModelConfiguration"] = relationship("ModelConfiguration", back_populates="benchmark_runs")
    comparisons: Mapped[List["BenchmarkComparison"]] = relationship("BenchmarkComparison", foreign_keys="BenchmarkComparison.run_id", back_populates="run")

    __table_args__ = (
        Index("idx_benchmark_runs_model_status", "model_config_id", "status"),
        Index("idx_benchmark_runs_type_score", "benchmark_type", "overall_score"),
    )


class BenchmarkComparison(Base):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏."""

    __tablename__ = "benchmark_comparisons"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    comparison_name: Mapped[str] = mapped_column(String(300))

    # –°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º—ã–µ –∑–∞–ø—É—Å–∫–∏
    run_id: Mapped[int] = mapped_column(Integer, ForeignKey("benchmark_runs.id"), index=True)
    baseline_run_id: Mapped[int] = mapped_column(Integer, ForeignKey("benchmark_runs.id"), nullable=True)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_results: Mapped[dict] = mapped_column(JSON)  # –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    performance_delta: Mapped[float] = mapped_column(Float, nullable=True)  # –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    quality_delta: Mapped[float] = mapped_column(Float, nullable=True)  # –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞

    # –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    summary: Mapped[str] = mapped_column(Text, nullable=True)
    recommendations: Mapped[list[str]] = mapped_column(JSON, default=list)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    run: Mapped["BenchmarkRun"] = relationship("BenchmarkRun", foreign_keys=[run_id], back_populates="comparisons")
    baseline_run: Mapped["BenchmarkRun"] = relationship("BenchmarkRun", foreign_keys=[baseline_run_id])

    __table_args__ = (
        Index("idx_benchmark_comparisons_runs", "run_id", "baseline_run_id"),
    )


class Recommendation(Base):
    """–ú–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)."""

    __tablename__ = "recommendations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    text: Mapped[str] = mapped_column(Text)
    links: Mapped[list[str]] = mapped_column(JSON)


class RecommendRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""

    text: str


class WPRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ WordPress-—Å–∞–π—Ç–∞."""

    domain: str
    client_id: Optional[str] = None
    comprehensive: Optional[bool] = False  # –§–ª–∞–≥ –¥–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏


class BenchmarkRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""

    name: str
    description: Optional[str] = None
    benchmark_type: str = "seo_advanced"  # seo_basic, seo_advanced, performance
    models: List[str] = []  # —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    iterations: int = 3
    client_id: Optional[str] = None


class ModelConfigRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏."""

    model_name: str
    display_name: Optional[str] = None
    description: Optional[str] = None
    default_parameters: Optional[dict] = None
    seo_optimized_params: Optional[dict] = None
    benchmark_params: Optional[dict] = None


OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è SEO –∑–∞–¥–∞—á: qwen2.5:7b-instruct-turbo - –ª—É—á—à–∞—è –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct-turbo")

# üéØ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –¢–û–ö–ï–ù–û–í –¥–ª—è –º–æ–¥–µ–ª–∏ qwen2.5:7b
# –ú–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 8192 —Ç–æ–∫–µ–Ω–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
OPTIMAL_CONTEXT_SIZE = 3072      # –ö–†–ò–¢–ò–ß–ù–û: –°–Ω–∏–∂–∞–µ–º –¥–æ 3K –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (75% –æ—Ç 4096)
OPTIMAL_PREDICTION_SIZE = 800    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞
OPTIMAL_TEMPERATURE = 0.3        # –ë–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
OPTIMAL_TOP_P = 0.85            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
OPTIMAL_TOP_K = 50              # –†–∞—Å—à–∏—Ä—è–µ–º –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
OPTIMAL_REPEAT_PENALTY = 1.08   # –°–Ω–∏–∂–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è

DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://seo_user:seo_pass@localhost/seo_db",
)

engine = create_async_engine(DATABASE_URL)
AsyncSessionLocal = async_sessionmaker(bind=engine, expire_on_commit=False)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã
chroma_client = None
tfidf_vectorizer = None

def initialize_rag_system() -> None:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é RAG-—Å–∏—Å—Ç–µ–º—É —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º."""
    global chroma_client, tfidf_vectorizer
    try:
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π RAG-—Å–∏—Å—Ç–µ–º—ã...")

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        tfidf_vectorizer = TfidfVectorizer(
            max_features=2000,  # —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            ngram_range=(1, 3),  # –¥–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã
            min_df=1,
            max_df=0.85,
            stop_words=list(RUSSIAN_STOP_WORDS),  # —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            analyzer='word',
            lowercase=True,
            token_pattern=r'\b[–∞-—è—ë]{2,}\b|\b[a-z]{2,}\b'  # —Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        chroma_client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=chromadb.Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
        chroma_client = None
        tfidf_vectorizer = None


class AdvancedRAGManager:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π RAG-–º–µ–Ω–µ–¥–∂–µ—Ä —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º."""

    def __init__(self) -> None:
        self.domain_collections = {}
        self.thematic_clusters = {}
        self.semantic_cache = {}


class IntelligentThoughtGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –º—ã—Å–ª–µ–π –ò–ò —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º numpy –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    
    def __init__(self) -> None:
        self.thought_history: List[AIThought] = []
        self.concept_embeddings: Dict[str, np.ndarray] = {}
        self.reasoning_patterns: Dict[str, List[str]] = defaultdict(list)
        self.semantic_network: Dict[str, Set[str]] = defaultdict(set)
        
    def extract_key_concepts(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –∏ NLP."""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è SEO-–∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        seo_patterns = [
            r'\b(?:SEO|—Å–µ–æ|–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è|—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ|–ø–æ–∏—Å–∫–æ–≤–∏–∫)\b',
            r'\b(?:–∫–ª—é—á–µ–≤\w+\s+—Å–ª–æ–≤\w+|keywords?)\b',
            r'\b(?:–∞–Ω–∫–æ—Ä\w*|anchor\w*)\b',
            r'\b(?:—Å—Å—ã–ª–∫\w+|link\w*)\b',
            r'\b(?:–∫–æ–Ω—Ç–µ–Ω—Ç|content)\b',
            r'\b(?:—Å–µ–º–∞–Ω—Ç–∏–∫\w+|semantic\w*)\b'
        ]
        
        concepts = []
        text_lower = text.lower()
        
        for pattern in seo_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            concepts.extend(matches)
            
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        words = word_tokenize(text_lower)
        filtered_words = [word for word in words if word not in RUSSIAN_STOP_WORDS and len(word) > 3]
        concepts.extend(filtered_words[:5])  # –ë–µ—Ä–µ–º —Ç–æ–ø-5 —Å–ª–æ–≤
        
        return list(set(concepts))[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
    
    def calculate_semantic_similarity(self, concepts1: List[str], concepts2: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π."""
        if not concepts1 or not concepts2:
            return 0.0
            
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        all_concepts = list(set(concepts1 + concepts2))
        
        if len(all_concepts) < 2:
            return 1.0 if concepts1 == concepts2 else 0.0
            
        vector1 = np.array([1 if concept in concepts1 else 0 for concept in all_concepts])
        vector2 = np.array([1 if concept in concepts2 else 0 for concept in all_concepts])
        
        # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        if np.linalg.norm(vector1) == 0 or np.linalg.norm(vector2) == 0:
            return 0.0
            
        return float(cosine_similarity([vector1], [vector2])[0][0])
    
    def generate_reasoning_chain(self, stage: str, concepts: List[str], context: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞."""
        reasoning_templates = {
            "analyzing": [
                f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: {', '.join(concepts[:3])}",
                f"–û–ø—Ä–µ–¥–µ–ª—è—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: {context[:100]}...",
                "–û—Ü–µ–Ω–∏–≤–∞—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è SEO"
            ],
            "connecting": [
                f"–ò—â—É —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏: {' ‚Üî '.join(concepts[:2])}",
                "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å —Å—Ç–∞—Ç–µ–π",
                "–í—ã—è–≤–ª—è—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏"
            ],
            "evaluating": [
                "–û—Ü–µ–Ω–∏–≤–∞—é –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π",
                f"–ü—Ä–æ–≤–µ—Ä—è—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π: {', '.join(concepts[:2])}",
                "–†–∞—Å—Å—á–∏—Ç—ã–≤–∞—é –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é SEO-—Ü–µ–Ω–Ω–æ—Å—Ç—å"
            ],
            "optimizing": [
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä—É—é –∞–Ω–∫–æ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                "–£—á–∏—Ç—ã–≤–∞—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç",
                "–§–æ—Ä–º–∏—Ä—É—é —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
            ]
        }
        
        return reasoning_templates.get(stage, ["–í—ã–ø–æ–ª–Ω—è—é –∞–Ω–∞–ª–∏–∑..."])
    
    async def generate_intelligent_thought(
        self, 
        stage: str, 
        context: str, 
        additional_data: Dict = None
    ) -> AIThought:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –º—ã—Å–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
        
        concepts = self.extract_key_concepts(context)
        reasoning_chain = self.generate_reasoning_chain(stage, concepts, context)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≤–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏
        semantic_weight = 0.5  # –±–∞–∑–æ–≤—ã–π –≤–µ—Å
        if self.thought_history:
            last_thought = self.thought_history[-1]
            similarity = self.calculate_semantic_similarity(
                concepts, last_thought.related_concepts
            )
            semantic_weight = 0.3 + (similarity * 0.7)  # 0.3-1.0 –¥–∏–∞–ø–∞–∑–æ–Ω
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ –∏—Ö –∫–∞—á–µ—Å—Ç–≤–∞
        confidence = min(0.9, 0.4 + (len(concepts) * 0.05) + (len(reasoning_chain) * 0.1))
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –º—ã—Å–ª–∏
        thought_content = self._generate_thought_content(stage, concepts, context, additional_data)
        
        thought = AIThought(
            thought_id=f"{stage}_{datetime.now().strftime('%H%M%S_%f')}",
            stage=stage,
            content=thought_content,
            confidence=confidence,
            semantic_weight=semantic_weight,
            related_concepts=concepts,
            reasoning_chain=reasoning_chain,
            timestamp=datetime.now()
        )
        
        self.thought_history.append(thought)
        self._update_semantic_network(concepts)
        
        return thought
    
    def _generate_thought_content(
        self, 
        stage: str, 
        concepts: List[str], 
        context: str, 
        additional_data: Dict = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –º—ã—Å–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∞–ø–∞ –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π."""
        
        stage_templates = {
            "analyzing": f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(concepts)} –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. " +
                        f"–§–æ–∫—É—Å –Ω–∞: {', '.join(concepts[:2])}. " +
                        f"–ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞: {len(context.split())//10} —Å–µ–≥–º–µ–Ω—Ç–æ–≤.",
            
            "connecting": f"üîó –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏. " +
                         f"–ù–∞–π–¥–µ–Ω–æ {len(self.semantic_network)} —É–∑–ª–æ–≤ –≤ —Å–µ—Ç–∏. " +
                         f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π: {sum(len(connections) for connections in self.semantic_network.values())}",
            
            "evaluating": f"‚öñÔ∏è –û—Ü–µ–Ω–∏–≤–∞—é –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –ø–æ {len(concepts)} –∫—Ä–∏—Ç–µ—Ä–∏—è–º. " +
                         f"–°—Ä–µ–¥–Ω–∏–π –≤–µ—Å —Å–≤—è–∑–µ–π: {np.mean([0.6, 0.7, 0.8]):.2f}",
            
            "optimizing": f"‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. " +
                         f"–ü—Ä–∏–º–µ–Ω—è—é {len(self.reasoning_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. " +
                         f"–¶–µ–ª–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: {', '.join(concepts[:3])}"
        }
        
        base_content = stage_templates.get(stage, "ü§î –í—ã–ø–æ–ª–Ω—è—é –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
        
        if additional_data:
            if "articles_count" in additional_data:
                base_content += f" | –°—Ç–∞—Ç–µ–π: {additional_data['articles_count']}"
            if "recommendations_count" in additional_data:
                base_content += f" | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {additional_data['recommendations_count']}"
                
        return base_content
    
    def _update_semantic_network(self, concepts: List[str]) -> None:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π."""
        for i, concept1 in enumerate(concepts):
            for concept2 in concepts[i+1:]:
                self.semantic_network[concept1].add(concept2)
                self.semantic_network[concept2].add(concept1)
    
    def get_network_insights(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏."""
        if not self.semantic_network:
            return {"status": "empty", "insights": []}
            
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        concept_connections = {
            concept: len(connections) 
            for concept, connections in self.semantic_network.items()
        }
        
        top_concepts = sorted(concept_connections.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "status": "active",
            "total_concepts": len(self.semantic_network),
            "total_connections": sum(len(connections) for connections in self.semantic_network.values()) // 2,
            "top_concepts": top_concepts,
            "network_density": len(self.semantic_network) / max(1, sum(len(connections) for connections in self.semantic_network.values())),
            "insights": [
                f"–î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è: {top_concepts[0][0] if top_concepts else '–Ω/–¥'}",
                f"–°—Ä–µ–¥–Ω—è—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: {np.mean(list(concept_connections.values())):.2f}",
                f"–ö–æ–Ω—Ü–µ–ø—Ü–∏–π —Å –≤—ã—Å–æ–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç—å—é: {sum(1 for _, count in concept_connections.items() if count > 3)}"
            ]
        }

    async def create_semantic_knowledge_base(
        self,
        domain: str,
        posts: List[Dict],
        client_id: Optional[str] = None
    ) -> bool:
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π."""
        if not chroma_client:
            print("‚ùå RAG-—Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return False

        try:
            if client_id:
                await websocket_manager.send_step(client_id, "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", 1, 5, "–ê–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏–∫ –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π...")

            print(f"üß† –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è {domain}...")

            # –û—á–∏—â–∞–µ–º –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_name = domain.replace(".", "_").replace("-", "_")

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                chroma_client.delete_collection(name=collection_name)
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}: {e}")

            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            collection = chroma_client.create_collection(
                name=collection_name,
                metadata={
                    "domain": domain,
                    "created_at": datetime.now().isoformat(),
                    "rag_version": "2.0",
                    "semantic_analysis": True
                }
            )

            if client_id:
                await websocket_manager.send_step(client_id, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", 2, 5, "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π...")

            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π
            enriched_posts = await self._enrich_posts_with_semantics(posts, domain)

            if client_id:
                await websocket_manager.send_step(client_id, "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", 3, 5, "–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤...")

            documents = []
            metadatas = []
            ids = []

            for i, post in enumerate(enriched_posts):
                # –°–æ–∑–¥–∞–µ–º –±–æ–≥–∞—Ç—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
                semantic_text = self._create_llm_friendly_context(post)

                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, —á–∏—Å–ª–∞ –∏ –±—É–ª–µ–≤—ã –∑–Ω–∞—á–µ–Ω–∏—è
                key_concepts = post.get('key_concepts', [])
                key_concepts_str = ', '.join(key_concepts[:5]) if key_concepts else ""  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É

                documents.append(semantic_text)
                metadatas.append({
                    "title": (post.get('title', '') or '')[:300],  # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É
                    "link": post.get('link', '') or '',
                    "content_snippet": (post.get('content', '') or '')[:500],
                    "domain": domain,
                    "post_index": i,
                    "semantic_summary": (post.get('semantic_summary', '') or '')[:500],
                    "key_concepts_str": key_concepts_str,  # –°—Ç—Ä–æ–∫–∞ –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–∞
                    "key_concepts_count": len(key_concepts),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∫ —á–∏—Å–ª–æ
                    "content_type": post.get('content_type', 'article'),
                    "difficulty_level": post.get('difficulty_level', 'medium'),
                    "linkability_score": float(post.get('linkability_score', 0.5))  # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —ç—Ç–æ float
                })
                ids.append(f"{collection_name}_{i}")

            if not documents:
                print(f"‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {domain}")
                return False

            if client_id:
                await websocket_manager.send_step(client_id, "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", 4, 5, "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º...")

            # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
            vectorizer = TfidfVectorizer(
                max_features=1000,
                ngram_range=(1, 3),
                min_df=1,
                max_df=0.8,
                stop_words=list(RUSSIAN_STOP_WORDS),
                analyzer='word'
            )

            tfidf_matrix = vectorizer.fit_transform(documents)
            dense_embeddings = tfidf_matrix.toarray().tolist()

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids,
                embeddings=dense_embeddings
            )

            if client_id:
                await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 5, 5, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")

            self.domain_collections[domain] = collection_name

            print(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {len(documents)} —Å—Ç–∞—Ç–µ–π –¥–ª—è {domain}")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–∞–∑—ã: {e}")
            return False

    async def _enrich_posts_with_semantics(self, posts: List[Dict], domain: str) -> List[Dict]:
        """–û–±–æ–≥–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
        enriched = []

        for post in posts:
            try:
                title = post.get('title', '')
                content = post.get('content', '')

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
                key_concepts = self._extract_key_concepts(title + ' ' + content)

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_type = self._classify_content_type(title, content)

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                difficulty = self._assess_difficulty(content)

                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
                linkability = self._calculate_linkability_score(title, content, key_concepts)

                # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ
                semantic_summary = self._create_semantic_summary(title, content, key_concepts)

                enriched_post = {
                    **post,
                    'key_concepts': key_concepts,
                    'content_type': content_type,
                    'difficulty_level': difficulty,
                    'linkability_score': linkability,
                    'semantic_summary': semantic_summary
                }

                enriched.append(enriched_post)

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ {post.get('title', 'unknown')}: {e}")
                enriched.append(post)  # –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å

        return enriched

    def _extract_key_concepts(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
        words = word_tokenize(text.lower())
        words = [w for w in words if w.isalpha() and len(w) > 3 and w not in RUSSIAN_STOP_WORDS]

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        return sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)[:10]

    def _classify_content_type(self, title: str, content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        title_lower = title.lower()

        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if any(word in title_lower for word in ['–∫–∞–∫', '–≥–∞–π–¥', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è']):
            return 'guide'
        elif any(word in title_lower for word in ['–æ–±–∑–æ—Ä', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ç–µ—Å—Ç']):
            return 'review'
        elif any(word in title_lower for word in ['–Ω–æ–≤–æ—Å—Ç–∏', '–∞–Ω–æ–Ω—Å', '—Ä–µ–ª–∏–∑']):
            return 'news'
        elif len(content) < 1000:
            return 'short_article'
        else:
            return 'article'

    def _assess_difficulty(self, content: str) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        words = word_tokenize(content)
        avg_word_length = sum(len(w) for w in words) / len(words) if words else 0

        if avg_word_length < 5:
            return 'easy'
        elif avg_word_length < 7:
            return 'medium'
        else:
            return 'advanced'

    def _calculate_linkability_score(self, title: str, content: str, concepts: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""
        score = 0.0

        # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        score += min(len(content) / 2000, 0.4)  # –¥–æ 0.4 –∑–∞ –¥–ª–∏–Ω—É

        # –°–∫–æ—Ä –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        score += min(len(concepts) / 20, 0.3)  # –¥–æ 0.3 –∑–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

        # –°–∫–æ—Ä –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        if any(word in title.lower() for word in ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–ø–æ—á–µ–º—É']):
            score += 0.2

        # –°–∫–æ—Ä –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (–ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        if content.count('.') > 5:  # –º–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            score += 0.1

        return min(score, 1.0)

    def _create_semantic_summary(self, title: str, content: str, concepts: List[str]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–ª—è LLM."""
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 - 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        sentences = content.split('.')[:3]
        summary = '. '.join(sentences).strip()

        if concepts:
            summary += f" –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: {', '.join(concepts[:5])}."

        return summary[:500]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

    def _create_llm_friendly_context(self, post: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç LLM-–¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏."""
        title = post.get('title', '')
        summary = post.get('semantic_summary', '')
        concepts = post.get('key_concepts', [])
        content_type = post.get('content_type', 'article')
        difficulty = post.get('difficulty_level', 'medium')

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context_parts = [
            f"–ó–ê–ì–û–õ–û–í–û–ö: {title}",
            f"–¢–ò–ü: {content_type}",
            f"–°–õ–û–ñ–ù–û–°–¢–¨: {difficulty}",
            f"–û–ü–ò–°–ê–ù–ò–ï: {summary}"
        ]

        if concepts:
            context_parts.append(f"–ö–õ–Æ–ß–ï–í–´–ï_–¢–ï–ú–´: {', '.join(concepts[:7])}")

        return ' | '.join(context_parts)

    async def get_semantic_recommendations(
        self,
        domain: str,
        limit: int = 25,
        min_linkability: float = 0.2  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    ) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        if domain not in self.domain_collections:
            return []

        try:
            collection = chroma_client.get_collection(name=self.domain_collections[domain])

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            results = collection.get(
                limit=limit * 2,  # –±–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                include=['metadatas', 'documents']
            )

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
            filtered_articles = []
            for i, metadata in enumerate(results['metadatas']):
                linkability_score = metadata.get('linkability_score', 0.0)
                if linkability_score >= min_linkability:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º key_concepts –∏–∑ —Å—Ç—Ä–æ–∫–∏
                    key_concepts_str = metadata.get('key_concepts_str', '')
                    key_concepts = [kc.strip() for kc in key_concepts_str.split(',') if kc.strip()] if key_concepts_str else []

                    filtered_articles.append({
                        'title': metadata['title'],
                        'link': metadata['link'],
                        'content': metadata.get('content_snippet', ''),
                        'semantic_summary': metadata.get('semantic_summary', ''),
                        'key_concepts': key_concepts,
                        'content_type': metadata.get('content_type', 'article'),
                        'difficulty_level': metadata.get('difficulty_level', 'medium'),
                        'linkability_score': linkability_score,
                        'domain': metadata['domain']
                    })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
            filtered_articles.sort(key=lambda x: x['linkability_score'], reverse=True)

            print(f"üéØ –ü–æ–ª—É—á–µ–Ω–æ {len(filtered_articles)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
            return filtered_articles[:limit]

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return []


class CumulativeIntelligenceManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π."""

    def __init__(self) -> None:
        self.connection_cache = {}
        self.cluster_cache = {}
        self.insight_cache = {}

    async def analyze_existing_connections(self, domain: str, session: AsyncSession) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–≤—è–∑–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        domain_result = await session.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = domain_result.scalar_one_or_none()
        if not domain_obj:
            return {"existing_connections": 0, "existing_recommendations": 0}

        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
        connections_result = await session.execute(
            select(SemanticConnection)
            .join(WordPressPost, SemanticConnection.source_post_id == WordPressPost.id)
            .where(WordPressPost.domain_id == domain_obj.id)
        )
        existing_connections = connections_result.scalars().all()

        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations_result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_obj.id)
            .where(LinkRecommendation.status == 'active')
        )
        existing_recommendations = recommendations_result.scalars().all()

        return {
            "existing_connections": len(existing_connections),
            "existing_recommendations": len(existing_recommendations),
            "connections": existing_connections,
            "recommendations": existing_recommendations,
            "domain_id": domain_obj.id
        }

    async def discover_thematic_clusters(self, domain_id: int, posts: list, session: AsyncSession) -> list:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã."""
        from sklearn.cluster import KMeans
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np

        if len(posts) < 3:
            return []

        # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        texts = [f"{post['title']} {post.get('content', '')[:500]}" for post in posts]
        vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            stop_words=list(RUSSIAN_STOP_WORDS)
        )

        try:
            tfidf_matrix = vectorizer.fit_transform(texts)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            n_clusters = min(max(2, len(posts) // 5), 8)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
            clusters = []
            feature_names = vectorizer.get_feature_names_out()

            for cluster_id in range(n_clusters):
                cluster_posts = [posts[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
                if len(cluster_posts) < 2:
                    continue

                # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                cluster_center = kmeans.cluster_centers_[cluster_id]
                top_features_idx = cluster_center.argsort()[-10:][::-1]
                cluster_keywords = [feature_names[i] for i in top_features_idx]

                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
                coherence_score = float(np.mean([cluster_center[i] for i in top_features_idx[:5]]))
                diversity_score = float(len(set(cluster_keywords)) / len(cluster_keywords))
                linkability_potential = coherence_score * diversity_score * len(cluster_posts) / len(posts)

                cluster_analysis = ThematicClusterAnalysis(
                    domain_id=domain_id,
                    cluster_name=f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id + 1}: {', '.join(cluster_keywords[:3])}",
                    cluster_keywords=cluster_keywords,
                    cluster_description=f"–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(cluster_posts)} —Å—Ç–∞—Ç–µ–π",
                    article_ids=[post.get('id', 0) for post in cluster_posts],
                    article_count=len(cluster_posts),
                    coherence_score=coherence_score,
                    diversity_score=diversity_score,
                    linkability_potential=linkability_potential,
                    evolution_stage='emerging'
                )

                session.add(cluster_analysis)
                clusters.append({
                    'analysis': cluster_analysis,
                    'posts': cluster_posts,
                    'keywords': cluster_keywords
                })

            await session.commit()
            return clusters

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return []

    async def generate_cumulative_insights(self, domain_id: int, analysis_data: dict, session: AsyncSession):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞."""
        insights = []

        # –ò–Ω—Å–∞–π—Ç –æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Å–≤—è–∑–µ–π
        if analysis_data['existing_connections'] > 0:
            connection_density = analysis_data['existing_connections'] / max(analysis_data.get('total_posts', 1), 1)

            if connection_density < 0.1:
                insight = CumulativeInsight(
                    domain_id=domain_id,
                    insight_type='gap',
                    insight_category='structural',
                    title='–ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–≤—è–∑–µ–π',
                    description=f'–î–æ–º–µ–Ω –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ {analysis_data["existing_connections"]} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫—É.',
                    evidence={'connection_density': connection_density, 'total_connections': analysis_data['existing_connections']},
                    impact_score=0.8,
                    confidence_level=0.9,
                    actionability=0.9
                )
                session.add(insight)
                insights.append(insight)

        # –ò–Ω—Å–∞–π—Ç –æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏
        clusters = analysis_data.get('clusters', [])
        if len(clusters) > 1:
            avg_linkability = sum(c['analysis'].linkability_potential for c in clusters) / len(clusters)

            if avg_linkability > 0.3:
                insight = CumulativeInsight(
                    domain_id=domain_id,
                    insight_type='opportunity',
                    insight_category='thematic',
                    title='–í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–µ–∂—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π',
                    description=f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(clusters)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏.',
                    evidence={'clusters_count': len(clusters), 'avg_linkability': avg_linkability},
                    impact_score=0.7,
                    confidence_level=0.8,
                    actionability=0.8
                )
                session.add(insight)
                insights.append(insight)

        await session.commit()
        return insights

    async def deduplicate_and_evolve_recommendations(
        self,
        new_recommendations: list,
        domain_id: int,
        session: AsyncSession
    ) -> list:
        """–î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ—Ç –∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        existing_result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_id)
            .where(LinkRecommendation.status == 'active')
        )
        existing_recommendations = {
            (rec.source_post_id, rec.target_post_id): rec
            for rec in existing_result.scalars().all()
        }

        evolved_recommendations = []

        for new_rec in new_recommendations:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Å—Ç—ã –≤ –ë–î
            source_result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_id)
                .where(WordPressPost.link == new_rec['from'])
            )
            source_post = source_result.scalar_one_or_none()

            target_result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_id)
                .where(WordPressPost.link == new_rec['to'])
            )
            target_post = target_result.scalar_one_or_none()

            if not source_post or not target_post:
                continue

            key = (source_post.id, target_post.id)

            if key in existing_recommendations:
                # –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                existing_rec = existing_recommendations[key]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–ª—É—á—à–∏–ª–∞—Å—å –ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
                new_quality = self._calculate_quality_score(new_rec)
                if new_quality > existing_rec.quality_score:
                    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                    improved_rec = LinkRecommendation(
                        domain_id=domain_id,
                        source_post_id=source_post.id,
                        target_post_id=target_post.id,
                        anchor_text=new_rec['anchor'],
                        reasoning=new_rec['comment'],
                        quality_score=new_quality,
                        generation_count=existing_rec.generation_count + 1,
                        improvement_iterations=existing_rec.improvement_iterations + 1,
                        previous_version_id=existing_rec.id,
                        improvement_reason=f"–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {new_quality:.2f} > {existing_rec.quality_score:.2f}"
                    )

                    # –ü–æ–º–µ—á–∞–µ–º —Å—Ç–∞—Ä—É—é –∫–∞–∫ —É–ª—É—á—à–µ–Ω–Ω—É—é
                    existing_rec.status = 'improved'

                    session.add(improved_rec)
                    evolved_recommendations.append(new_rec)
                else:
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    existing_rec.generation_count += 1
                    existing_rec.updated_at = datetime.utcnow()
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                quality_score = self._calculate_quality_score(new_rec)
                link_rec = LinkRecommendation(
                    domain_id=domain_id,
                    source_post_id=source_post.id,
                    target_post_id=target_post.id,
                    anchor_text=new_rec['anchor'],
                    reasoning=new_rec['comment'],
                    quality_score=quality_score
                )

                session.add(link_rec)
                evolved_recommendations.append(new_rec)

        await session.commit()
        return evolved_recommendations

    def _calculate_quality_score(self, recommendation: dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        score = 0.0

        # –û—Ü–µ–Ω–∫–∞ –∞–Ω–∫–æ—Ä–∞
        anchor = recommendation.get('anchor', '')
        if len(anchor) > 5:
            score += 0.3
        if any(word in anchor.lower() for word in ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ']):
            score += 0.2

        # –û—Ü–µ–Ω–∫–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
        comment = recommendation.get('comment', '')
        if len(comment) > 20:
            score += 0.3
        if len(comment) > 50:
            score += 0.2

        return min(score, 1.0)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã
rag_manager = AdvancedRAGManager()
cumulative_intelligence = CumulativeIntelligenceManager()
thought_generator = IntelligentThoughtGenerator()


async def generate_links(text: str) -> list[str]:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—ã—Ö —Å—Å—ã–ª–æ–∫."""
    prompt = (
        "–ü—Ä–µ–¥–ª–æ–∂–∏ –¥–æ –ø—è—Ç–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        "–ö–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É –≤—ã–≤–µ–¥–∏ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ /article/–Ω–∞–∑–≤–∞–Ω–∏–µ-—Å—Ç–∞—Ç—å–∏, "
        "–æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–ª–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è. "
        f"–¢–µ–∫—Å—Ç: {text}"
    )
    async with httpx.AsyncClient(timeout=720.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 12 –º–∏–Ω—É—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        response = await client.post(
            OLLAMA_URL,
            json={"model": OLLAMA_MODEL, "prompt": prompt, "stream": False},
            timeout=720,  # 12 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å
        )
    response.raise_for_status()
    data = response.json()
    lines = [line.strip("- \n") for line in data.get("response", "").splitlines()]
    links = [line for line in lines if line]
    return links[:5]


async def fetch_and_store_wp_posts(domain: str, client_id: Optional[str] = None) -> tuple[list[dict[str, str]], dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ WordPress —Å —É–º–Ω–æ–π –¥–µ–ª—å—Ç–∞-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π."""
    print(f"üß† –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ {domain}")

    if client_id:
        await websocket_manager.send_step(client_id, "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π", 1, 5, "–ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")

    # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –¥–æ–º–µ–Ω
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = result.scalar_one_or_none()

        if not domain_obj:
            domain_obj = Domain(
                name=domain,
                display_name=domain,
                description=f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –¥–æ–º–µ–Ω –¥–ª—è {domain}",
                language="ru"
            )
            session.add(domain_obj)
            await session.commit()
            await session.refresh(domain_obj)
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –¥–æ–º–µ–Ω: {domain}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        existing_posts_result = await session.execute(
            select(WordPressPost).where(WordPressPost.domain_id == domain_obj.id)
        )
        existing_posts = {post.wp_post_id: post for post in existing_posts_result.scalars().all()}
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(existing_posts)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")

    if client_id:
        await websocket_manager.send_step(client_id, "–ó–∞–≥—Ä—É–∑–∫–∞ —Å WordPress", 2, 5, "–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

    url = f"https://{domain}/wp-json/wp/v2/posts?per_page=50"
    async with httpx.AsyncClient(timeout=120.0) as client:
        response = await client.get(url)
    if response.status_code >= 400:
        raise HTTPException(status_code=400, detail="–°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ WordPress")
    data = response.json()
    if not isinstance(data, list):
        raise HTTPException(status_code=400, detail="–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç WordPress")

    if client_id:
        await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π", 3, 5, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(data)} —Å—Ç–∞—Ç–µ–π...")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–ª—å—Ç–∞-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    delta_stats = {
        'new_posts': 0,
        'updated_posts': 0,
        'unchanged_posts': 0,
        'removed_posts': 0,
        'total_posts': len(data)
    }

    posts = []
    processed_wp_ids = set()
    seen_urls = set()
    seen_titles = set()

    async with AsyncSessionLocal() as session:
        for item in data:
            try:
                wp_post_id = item["id"]
                processed_wp_ids.add(wp_post_id)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = item.get("content", {}).get("rendered", "")
                excerpt = item.get("excerpt", {}).get("rendered", "")

                # –û—á–∏—â–∞–µ–º HTML
                clean_content = BeautifulSoup(content, 'html.parser').get_text()
                clean_excerpt = BeautifulSoup(excerpt, 'html.parser').get_text() if excerpt else ""

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –Ω–∞—à–µ–º—É –¥–æ–º–µ–Ω—É
                post_link = item["link"]
                if domain.lower() not in post_link.lower():
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞—Ç—å—é —Å —á—É–∂–æ–≥–æ –¥–æ–º–µ–Ω–∞: {post_link}")
                    continue

                # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
                if post_link in seen_urls:
                    print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç URL –ø—Ä–æ–ø—É—â–µ–Ω: {post_link}")
                    continue
                seen_urls.add(post_link)

                # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                title = item["title"]["rendered"]
                title_normalized = title.lower().strip()
                if title_normalized in seen_titles:
                    print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω: {title}")
                    continue
                seen_titles.add(title_normalized)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ—Å—Ç
                existing_post = existing_posts.get(wp_post_id)
                # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                # modified_str = item.get("modified", datetime.now().isoformat())
                # post_modified = datetime.fromisoformat(modified_str.replace('Z', '+00:00'))

                if existing_post:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ –ø–æ—Å—Ç
                    if (existing_post.title == title and existing_post.content == clean_content and existing_post.link == post_link):                        # –ü–æ—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
                        delta_stats['unchanged_posts'] += 1
                        print(f"‚ö° –ü–æ—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è: {title}")

                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –ø–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                        posts.append({
                            "title": title,
                            "link": post_link,
                            "content": clean_content[:800].strip()
                        })
                        continue
                    else:
                        # –ü–æ—Å—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è - –æ–±–Ω–æ–≤–ª—è–µ–º
                        existing_post.title = title
                        existing_post.content = clean_content
                        existing_post.excerpt = clean_excerpt
                        existing_post.link = post_link
                        existing_post.updated_at = datetime.utcnow()
                        existing_post.last_analyzed_at = None  # type: ignore # –°–±—Ä–æ—Å –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

                        delta_stats['updated_posts'] += 1
                        print(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω –ø–æ—Å—Ç: {title}")
                else:
                    # –ù–æ–≤—ã–π –ø–æ—Å—Ç - —Å–æ–∑–¥–∞–µ–º
                    wp_post = WordPressPost(
                        domain_id=domain_obj.id,
                        wp_post_id=wp_post_id,
                        title=title,
                        content=clean_content,
                        excerpt=clean_excerpt,
                        link=post_link,
                        published_at=datetime.fromisoformat(
                            item["date"].replace('Z', '+00:00')
                        ) if item.get("date") else None,
                        last_analyzed_at=None  # –ù–æ–≤—ã–π –ø–æ—Å—Ç —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞
                    )
                    session.add(wp_post)

                    delta_stats['new_posts'] += 1
                    print(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ—Å—Ç: {title}")

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                posts.append({
                    "title": title,
                    "link": post_link,
                    "content": clean_content[:800].strip()
                })

            except Exception as exc:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞ {item.get('id', 'unknown')}: {exc}")
                continue

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –Ω–∞ —Å–∞–π—Ç–µ
        for wp_post_id, existing_post in existing_posts.items():
            if wp_post_id not in processed_wp_ids:
                await session.delete(existing_post)
                delta_stats['removed_posts'] += 1
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –ø–æ—Å—Ç: {existing_post.title}")

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –≤ –ë–î
        posts_count_result = await session.execute(
            select(func.count(WordPressPost.id))
            .where(WordPressPost.domain_id == domain_obj.id)
        )
        actual_posts_count = posts_count_result.scalar()

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–º–µ–Ω–∞
        domain_obj.total_posts = actual_posts_count if actual_posts_count is not None else 0
        domain_obj.updated_at = datetime.utcnow()

        print(f"üìä –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–º–µ–Ω–∞: {actual_posts_count} –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")

        await session.commit()

        if client_id:
            await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 4, 5, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")

        print(f"üß† –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(f"   ‚ûï –ù–æ–≤—ã–µ –ø–æ—Å—Ç—ã: {delta_stats['new_posts']}")
        print(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ: {delta_stats['updated_posts']}")
        print(f"   ‚ö° –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {delta_stats['unchanged_posts']}")
        print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω–Ω—ã–µ: {delta_stats['removed_posts']}")
        print(f"   üìä –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")

        if client_id:
            await websocket_manager.send_step(client_id, "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞", 5, 5,
                f"–ù–æ–≤—ã—Ö: {delta_stats['new_posts']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {delta_stats['updated_posts']}, –£–¥–∞–ª–µ–Ω–æ: {delta_stats['removed_posts']}")

    return posts, delta_stats


async def generate_comprehensive_domain_recommendations(domain: str, client_id: Optional[str] = None) -> list[dict[str, str]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π."""
    print(f"üß† –ó–∞–ø—É—Å–∫ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")

    analysis_start_time = datetime.now()
    all_recommendations = []

    try:
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", 1, 12, "–ò–∑—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π...")

        async with AsyncSessionLocal() as session:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–≤—è–∑–∏
            existing_analysis = await cumulative_intelligence.analyze_existing_connections(domain, session)
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ {existing_analysis['existing_connections']} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π")
            print(f"üìã –ù–∞–π–¥–µ–Ω–æ {existing_analysis['existing_recommendations']} –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

            domain_obj = await session.get(Domain, existing_analysis['domain_id'])
            if not domain_obj:
                error_msg = f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"
                print(error_msg)
                if client_id:
                    await websocket_manager.send_error(client_id, error_msg)
                return [], 0.0

            # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –ø–æ—Å—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_obj.id)
                .order_by(WordPressPost.linkability_score.desc())
            )
            all_posts = result.scalars().all()

        if not all_posts:
            error_msg = "‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg)
            return [], 0.0

        print(f"üìä –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {len(all_posts)} —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î")

        # –®–∞–≥ 2: –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", 2, 12, f"–ê–Ω–∞–ª–∏–∑ {len(all_posts)} —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–∞–º...")

        full_dataset = []
        for post in all_posts:
            full_dataset.append({
                "id": post.id,
                "title": post.title,
                "link": post.link,
                "content": post.content,
                "semantic_summary": post.semantic_summary or "",
                "key_concepts": post.key_concepts or [],
                "content_type": post.content_type or "article",
                "difficulty_level": post.difficulty_level or "medium",
                "linkability_score": post.linkability_score or 0.5,
                "semantic_richness": post.semantic_richness or 0.5
            })

        async with AsyncSessionLocal() as session:
            # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            clusters = await cumulative_intelligence.discover_thematic_clusters(
                domain_obj.id, full_dataset, session
            )
            print(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(clusters)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

            existing_analysis['clusters'] = clusters
            existing_analysis['total_posts'] = len(all_posts)

        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤
        if client_id:
            await websocket_manager.send_step(client_id, "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤", 3, 12, "–ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É–ª—É—á—à–µ–Ω–∏—è...")

        async with AsyncSessionLocal() as session:
            insights = await cumulative_intelligence.generate_cumulative_insights(
                domain_obj.id, existing_analysis, session
            )
            print(f"üí° –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(insights)} –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤")

        # –®–∞–≥ 4: –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", 4, 12, "–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
            await websocket_manager.send_ai_thinking(
                client_id,
                "–°–æ–∑–¥–∞—é –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π –∏ –≤—ã—Å—Ç—Ä–∞–∏–≤–∞—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ...",
                "vectorizing",
                "üßÆ"
            )

        success = await rag_manager.create_semantic_knowledge_base(domain, full_dataset, client_id)
        if not success:
            error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg)
            return [], 0.0

        # –®–∞–≥ 5: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥
        if client_id:
            await websocket_manager.send_step(client_id, "–£–º–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥", 5, 12, "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º...")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –¥–ª—è –±–æ–ª–µ–µ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        cluster_batches = []
        for cluster in clusters:
            cluster_posts = cluster['posts']
            if len(cluster_posts) >= 2:
                cluster_batches.append({
                    'posts': cluster_posts,
                    'cluster_info': cluster['analysis'],
                    'keywords': cluster['keywords']
                })

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –±–∞—Ç—á–µ–π (–º–∞–∫—Å–∏–º—É–º 5)
        if len(clusters) > 1:
            cross_cluster_count = 0
            max_cross_clusters = 5  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

            for i, cluster1 in enumerate(clusters):
                if cross_cluster_count >= max_cross_clusters:
                    break
                for cluster2 in clusters[i+1:]:
                    if cross_cluster_count >= max_cross_clusters:
                        break
                    # –ú–∏–∫—Å —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    mixed_posts = cluster1['posts'][:2] + cluster2['posts'][:2]
                    cluster_batches.append({
                        'posts': mixed_posts,
                        'cluster_info': None,  # –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                        'keywords': cluster1['keywords'] + cluster2['keywords'],
                        'cross_cluster': True
                    })
                    cross_cluster_count += 1

        print(f"üß† –°–æ–∑–¥–∞–Ω–æ {len(cluster_batches)} –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –±–∞—Ç—á–µ–π")

        # –®–∞–≥ 6 - 9: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ –±–∞—Ç—á–∏
        for batch_idx, batch_info in enumerate(cluster_batches, 1):
            if client_id:
                batch_type = "–ú–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π" if batch_info.get('cross_cluster') else "–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π"
                await websocket_manager.send_step(
                    client_id,
                    f"{batch_type} –∞–Ω–∞–ª–∏–∑ {batch_idx}/{len(cluster_batches)}",
                    5 + batch_idx,
                    12,
                    f"–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–º..."
                )

            batch_recommendations = await process_cumulative_batch_with_ollama(
                domain, batch_info, existing_analysis, batch_idx, len(cluster_batches), client_id
            )

            all_recommendations.extend(batch_recommendations)
            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –±–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω–æ {len(batch_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

        # –®–∞–≥ 10: –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —ç–≤–æ–ª—é—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", 10, 12, "–≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")

        async with AsyncSessionLocal() as session:
            evolved_recommendations = await cumulative_intelligence.deduplicate_and_evolve_recommendations(
                all_recommendations, domain_obj.id, session
            )
            print(f"üß¨ –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–æ {len(evolved_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

        # –®–∞–≥ 11: –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ", 11, 12, "–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏...")
            await websocket_manager.send_ai_thinking(
                client_id,
                "–ü—Ä–∏–º–µ–Ω—è—é –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ —É—Å–ø–µ—à–Ω—ã—Ö —Å–≤—è–∑—è—Ö –∏ —Ä–∞–Ω–∂–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏...",
                "ranking",
                "üéØ"
            )

        final_recommendations = rank_recommendations_with_cumulative_intelligence(
            evolved_recommendations, existing_analysis, insights
        )

        # –®–∞–≥ 12: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        total_analysis_time = (datetime.now() - analysis_start_time).total_seconds()

        if client_id:
            await websocket_manager.send_step(
                client_id,
                "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                12,
                12,
                f"–ì–æ—Ç–æ–≤–æ! {len(final_recommendations)} —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            )

        print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(final_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∑–∞ {total_analysis_time:.1f}—Å")
        return final_recommendations, total_analysis_time

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}"
        print(error_msg)
        if client_id:
            await websocket_manager.send_error(client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", str(e))

        error_time = (datetime.now() - analysis_start_time).total_seconds()
        return [], error_time


async def process_batch_with_ollama(
    domain: str,
    batch: List[Dict],
    full_dataset: List[Dict],
    batch_idx: int,
    total_batches: int,
    client_id: Optional[str] = None
) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ Ollama —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""

    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –±–∞—Ç—á–∞
    batch_context = f"""–ê–ù–ê–õ–ò–ó –ë–ê–¢–ß–ê {batch_idx}/{total_batches} –î–û–ú–ï–ù–ê {domain}

–°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ({len(batch)}):
"""

    for i, article in enumerate(batch, 1):
        key_concepts_str = ', '.join(article['key_concepts'][:4]) if article['key_concepts'] else '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'

        batch_context += f"""
{i}. {article['title']}
   URL: {article['link']}
   –¢–∏–ø: {article['content_type']} | –°–≤—è–∑–Ω–æ—Å—Ç—å: {article['linkability_score']:.2f}
   –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {key_concepts_str}
   –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {article['semantic_summary'][:150]}
   –ö–æ–Ω—Ç–µ–Ω—Ç: {article['content'][:200]}...

"""

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç–µ–π
    batch_context += f"""–î–û–°–¢–£–ü–ù–´–ï –¶–ï–õ–ò ({len(full_dataset)} —Å—Ç–∞—Ç–µ–π):
"""

    targets_added = 0
    for article in full_dataset[:15]:  # –¢–æ–ø-15 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        if article not in batch and targets_added < 10:  # –ú–∞–∫—Å–∏–º—É–º 10 —Ü–µ–ª–µ–π
            batch_context += f"‚Ä¢ {article['title'][:50]} | {article['link'][:60]}\n"
            targets_added += 1

    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    comprehensive_prompt = f"""{batch_context}

–ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –ª–æ–≥–∏—á–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏

–ö–†–ò–¢–ï–†–ò–ò:
‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å
‚Ä¢ –ü–æ–ª—å–∑–∞ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
‚Ä¢ SEO-—Ü–µ–Ω–Ω–æ—Å—Ç—å

–ü–†–ê–í–ò–õ–ê –∞–Ω–∫–æ—Ä–æ–≤:
‚Ä¢ –û–ø–∏—Å—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
‚Ä¢ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: "—Å–∞–π—Ç", "—Ä–µ—Å—É—Ä—Å", "–ø–æ—Ä—Ç–∞–ª"
‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã: "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–ø–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä"

–ù–∞–π–¥–∏ –í–°–ï –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –¥–ª—è —Å—Ç–∞—Ç–µ–π —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞.

–§–û–†–ú–ê–¢: –ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä | –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–û–¢–í–ï–¢:"""

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –º—ã—Å–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
        if client_id:
            analyzing_thought = await thought_generator.generate_intelligent_thought(
                stage="analyzing",
                context=f"–ë–∞—Ç—á {batch_idx} –∏–∑ {len(batch)} —Å—Ç–∞—Ç–µ–π. –î–æ–º–µ–Ω: {domain}. " +
                       f"–°—Ç–∞—Ç—å–∏: {', '.join([article.get('title', '')[:50] for article in batch[:2]])}",
                additional_data={
                    "articles_count": len(batch),
                    "batch_number": batch_idx,
                    "total_batches": total_batches
                }
            )
            await websocket_manager.send_enhanced_ai_thinking(client_id, analyzing_thought)
            
            await websocket_manager.send_ollama_info(client_id, {
                "status": "processing_batch",
                "batch": f"{batch_idx}/{total_batches}",
                "articles_in_batch": len(batch),
                "total_context_size": len(comprehensive_prompt),
                "model": OLLAMA_MODEL
            })

        print(f"ü§ñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –±–∞—Ç—á {batch_idx} —á–µ—Ä–µ–∑ Ollama (—Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(comprehensive_prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")

        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=600.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º-–∞—É—Ç –¥–æ 10 –º–∏–Ω—É—Ç
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": comprehensive_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,    # –ù–µ–º–Ω–æ–≥–æ –ø–æ–≤—ã—à–∞–µ–º –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_ctx": 4096,       # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        "num_predict": 800,    # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                        "top_p": 0.8,
                        "top_k": 40,
                        "repeat_penalty": 1.05,
                        "num_thread": 6        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
                    }
                },
                timeout=600  # 10 –º–∏–Ω—É—Ç –Ω–∞ –±–∞—Ç—á
            )

        request_time = (datetime.now() - start_time).total_seconds()

        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "batch_completed",
                "batch": f"{batch_idx}/{total_batches}",
                "processing_time": f"{request_time:.1f}s",
                "response_length": len(response.text) if response.status_code == 200 else 0
            })

        if response.status_code != 200:
            print(f"‚ùå Ollama –æ—à–∏–±–∫–∞ –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}: –∫–æ–¥ {response.status_code}")
            return []

        data = response.json()
        content = data.get("response", "")

        print(f"üìù –ë–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º—ã—Å–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if client_id:
            optimizing_thought = await thought_generator.generate_intelligent_thought(
                stage="optimizing",
                context=f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}. " +
                       f"–†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤. " +
                       f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {request_time:.1f}—Å",
                additional_data={
                    "response_size": len(content),
                    "processing_time": request_time,
                    "batch_number": batch_idx
                }
            )
            await websocket_manager.send_enhanced_ai_thinking(client_id, optimizing_thought)

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞
        batch_recommendations = parse_ollama_recommendations(content, domain, full_dataset)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º—ã—Å–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if client_id and batch_recommendations:
            evaluating_thought = await thought_generator.generate_intelligent_thought(
                stage="evaluating",
                context=f"–ù–∞–π–¥–µ–Ω–æ {len(batch_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}. " +
                       f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–≤—è–∑–µ–π.",
                additional_data={
                    "recommendations_count": len(batch_recommendations),
                    "batch_number": batch_idx,
                    "success_rate": min(1.0, len(batch_recommendations) / len(batch))
                }
            )
            await websocket_manager.send_enhanced_ai_thinking(client_id, evaluating_thought)

        return batch_recommendations

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_idx}: {e}")
        return []


async def process_cumulative_batch_with_ollama(
    domain: str,
    batch_info: dict,
    existing_analysis: dict,
    batch_idx: int,
    total_batches: int,
    client_id: Optional[str] = None
) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π."""

    posts = batch_info['posts']
    keywords = batch_info.get('keywords', [])
    is_cross_cluster = batch_info.get('cross_cluster', False)

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
    batch_context = f"""–ö–£–ú–£–õ–Ø–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –î–û–ú–ï–ù–ê {domain}

–ö–û–ù–¢–ï–ö–°–¢ –ù–ê–ö–û–ü–õ–ï–ù–ù–´–• –ó–ù–ê–ù–ò–ô:
‚Ä¢ –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π: {existing_analysis['existing_connections']}
‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {existing_analysis['existing_recommendations']}
‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(existing_analysis.get('clusters', []))}

–°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ({len(posts)}):
"""

    for i, post in enumerate(posts, 1):
        post_concepts = ', '.join(post.get('key_concepts', [])[:3])
        batch_context += f"""
{i}. {post['title']}
   URL: {post['link']}
   –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {post_concepts}
   –¢–∏–ø: {post.get('content_type', 'article')}
   –°–≤—è–∑–Ω–æ—Å—Ç—å: {post.get('linkability_score', 0.5):.2f}
   –ö–æ–Ω—Ç–µ–Ω—Ç: {post.get('content', '')[:150]}...

"""

    if is_cross_cluster:
        batch_context += f"""
üîó –ú–ï–ñ–ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó
–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è: {', '.join(keywords[:8])}
–ó–ê–î–ê–ß–ê: –ù–∞–π—Ç–∏ –≥–ª—É–±–æ–∫–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏
"""
    else:
        batch_context += f"""
üéØ –í–ù–£–¢–†–ò–ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó
–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords[:6])}
–ó–ê–î–ê–ß–ê: –ù–∞–π—Ç–∏ –ª–æ–≥–∏—á–Ω—ã–µ —Å–≤—è–∑–∏ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
"""

    # –£–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    cumulative_prompt = f"""{batch_context}

–ü–†–ò–ù–¶–ò–ü–´ –ö–£–ú–£–õ–Ø–¢–ò–í–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:
‚úÖ –ò–∑–±–µ–≥–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö {existing_analysis['existing_recommendations']} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å –ù–û–í–´–ï —Å–≤—è–∑–∏, –¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
‚úÖ –£—á–∏—Ç—ã–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ, –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏

–ö–†–ò–¢–ï–†–ò–ò –ö–ê–ß–ï–°–¢–í–ê:
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å
‚Ä¢ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π
‚Ä¢ SEO-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å

–§–û–†–ú–ê–¢: –ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä | –≥–ª—É–±–æ–∫–æ–µ_–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–û–¢–í–ï–¢:"""

    try:
        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "processing_cumulative_batch",
                "batch": f"{batch_idx}/{total_batches}",
                "batch_type": "–º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π" if is_cross_cluster else "–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π",
                "articles_count": len(posts),
                "context_size": len(cumulative_prompt),
                "existing_connections": existing_analysis['existing_connections']
            })

        print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞ {batch_idx} ({'–º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π' if is_cross_cluster else '–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π'})")

        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=600.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 10 –º–∏–Ω—É—Ç
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": cumulative_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.4,    # –ß—É—Ç—å –≤—ã—à–µ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_ctx": 4096,
                        "num_predict": 600,
                        "top_p": 0.85,
                        "top_k": 50,
                        "repeat_penalty": 1.1,
                        "num_thread": 6
                    }
                },
                timeout=600  # 10 –º–∏–Ω—É—Ç
            )

        request_time = (datetime.now() - start_time).total_seconds()

        if response.status_code != 200:
            print(f"‚ùå Ollama –æ—à–∏–±–∫–∞ –¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –±–∞—Ç—á–∞ {batch_idx}: –∫–æ–¥ {response.status_code}")
            return []

        data = response.json()
        content = data.get("response", "")

        print(f"üìù –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        batch_recommendations = parse_ollama_recommendations(content, domain, posts)

        return batch_recommendations

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–∞—Ç—á–∞ {batch_idx}: {e}")
        return []


def rank_recommendations_with_cumulative_intelligence(
    recommendations: List[Dict],
    existing_analysis: dict,
    insights: List
) -> List[Dict]:
    """–†–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞."""

    def cumulative_quality_score(rec):
        score = 0.0

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        anchor = rec.get('anchor', '')
        comment = rec.get('comment', '')

        # –î–ª–∏–Ω–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∫–æ—Ä–∞
        if len(anchor) > 5:
            score += 0.2
        if len(anchor) > 15:
            score += 0.1

        # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∞–Ω–∫–æ—Ä–µ
        quality_words = ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–æ–±–∑–æ—Ä', '–≥–∞–π–¥', '–∞–Ω–∞–ª–∏–∑']
        anchor_quality = sum(1 for word in quality_words if word in anchor.lower()) * 0.15
        score += anchor_quality

        # –ì–ª—É–±–∏–Ω–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
        if len(comment) > 30:
            score += 0.2
        if len(comment) > 60:
            score += 0.1

        # –ë–æ–Ω—É—Å –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏
        semantic_words = ['—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏', '—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏', '–ª–æ–≥–∏—á–µ—Å–∫–∏', '–¥–æ–ø–æ–ª–Ω—è–µ—Ç', '—É–≥–ª—É–±–ª—è–µ—Ç', '—Ä–∞—Å—à–∏—Ä—è–µ—Ç']
        semantic_bonus = sum(1 for word in semantic_words if word in comment.lower()) * 0.1
        score += semantic_bonus

        # –ë–æ–Ω—É—Å –∑–∞ –Ω–æ–≤–∏–∑–Ω—É (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö)
        # –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞, —Ç–∞–∫ –∫–∞–∫ —Ç–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ë–î
        score += 0.1  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–æ–≤—ã–µ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏

        return min(score, 1.0)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É
    ranked_recommendations = sorted(recommendations, key=cumulative_quality_score, reverse=True)

    print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(ranked_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

    return ranked_recommendations[:30]  # –¢–æ–ø-30 —Å–∞–º—ã—Ö —Ü–µ–Ω–Ω—ã—Ö


def deduplicate_and_rank_recommendations(recommendations: List[Dict], domain: str) -> List[Dict]:
    """–î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ—Ç –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—É—Å—Ç–∞—Ä–µ–≤—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è)."""

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –ø–∞—Ä–µ –∏—Å—Ç–æ—á–Ω–∏–∫->—Ü–µ–ª—å
    seen_pairs = set()
    unique_recommendations = []

    for rec in recommendations:
        pair_key = (rec['from'], rec['to'])
        if pair_key not in seen_pairs:
            seen_pairs.add(pair_key)
            unique_recommendations.append(rec)

    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∞–Ω–∫–æ—Ä–∞ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
    def quality_score(rec):
        anchor_score = len(rec['anchor']) * 0.1  # –î–ª–∏–Ω–∞ –∞–Ω–∫–æ—Ä–∞
        comment_score = len(rec['comment']) * 0.05  # –î–ª–∏–Ω–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è

        # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∞–Ω–∫–æ—Ä–µ
        quality_words = ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–æ–±–∑–æ—Ä', '–≥–∞–π–¥']
        anchor_quality = sum(1 for word in quality_words if word in rec['anchor'].lower()) * 2

        return anchor_score + comment_score + anchor_quality

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    ranked_recommendations = sorted(unique_recommendations, key=quality_score, reverse=True)

    print(f"üéØ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {len(recommendations)} -> {len(unique_recommendations)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

    return ranked_recommendations[:50]  # –¢–æ–ø-50 —Å–∞–º—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö


async def generate_rag_recommendations(domain: str, client_id: Optional[str] = None) -> list[dict[str, str]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è RAG-–ø–æ–¥—Ö–æ–¥ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î."""
    print(f"üöÄ –ó–∞–ø—É—Å–∫ RAG-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞
    analysis_start_time = datetime.now()
    request_time = 0.0

    try:
        # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π", 1, 7, "–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")

        async with AsyncSessionLocal() as session:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω
            domain_result = await session.execute(
                select(Domain).where(Domain.name == domain)
            )
            domain_obj = domain_result.scalar_one_or_none()

            if not domain_obj:
                error_msg = f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"
                print(error_msg)
                if client_id:
                    await websocket_manager.send_error(client_id, error_msg)
                return [], 0.0

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_obj.id)
                .order_by(WordPressPost.linkability_score.desc(), WordPressPost.published_at.desc())
                .limit(100)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            )
            db_posts = result.scalars().all()

        if not db_posts:
            error_msg = "‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è RAG-–∞–Ω–∞–ª–∏–∑–∞"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg, f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}")
            return [], 0.0

        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(db_posts)} —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è RAG
        posts_data = []
        for post in db_posts:
            posts_data.append({
                "title": post.title,
                "link": post.link,
                "content": post.content[:1000]  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
            })

        # –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        if client_id:
            await websocket_manager.send_step(client_id, "–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã", 2, 7, "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞...")

        success = await rag_manager.create_semantic_knowledge_base(domain, posts_data, client_id)
        if not success:
            error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg, "–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return [], 0.0

        # –®–∞–≥ 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–∞ —Å—Ç–∞—Ç–µ–π
        if client_id:
            await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", 3, 7, "–í—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π...")

        articles = await rag_manager.get_semantic_recommendations(domain, limit=12)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if not articles:
            error_msg = "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞—Ç—å–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg, "–ü—É—Å—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
            return [], 0.0

        print(f"üìã –í—ã–±—Ä–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

        # –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        if client_id:
            await websocket_manager.send_step(client_id, "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ò–ò", 4, 7, "–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è Ollama...")

        # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        articles_context = ""
        for i, article in enumerate(articles, 1):
            title = article['title']
            content_snippet = article['content'][:300] if article.get('content') else ""
            key_concepts = article.get('key_concepts', [])[:8]  # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
            content_type = article.get('content_type', '—Å—Ç–∞—Ç—å—è')
            linkability = article.get('linkability_score', 0.5)
            semantic_summary = article.get('semantic_summary', '').strip()
            difficulty_level = article.get('difficulty_level', '—Å—Ä–µ–¥–Ω–∏–π')
            target_audience = article.get('target_audience', '–æ–±—â–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è')

            articles_context += f"""üìÑ –°–¢–ê–¢–¨–Ø {i}: ¬´{title}¬ª
üîó URL: {article['link']}
üìä –ú–ï–¢–†–ò–ö–ò: –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content_type} | –°–ª–æ–∂–Ω–æ—Å—Ç—å: {difficulty_level} | –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –ª–∏–Ω–∫–æ–≤–∫–∏: {linkability:.2f}
üë• –ê–£–î–ò–¢–û–†–ò–Ø: {target_audience}
üß† –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¶–ò–ò: {', '.join(key_concepts) if key_concepts else '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'}
üìù –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –û–ü–ò–°–ê–ù–ò–ï: {semantic_summary if semantic_summary else '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞'}
üí° –°–û–î–ï–†–ñ–ê–ù–ò–ï: {content_snippet}...

"""

        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
        qwen_optimized_prompt = f"""üéØ –ó–ê–î–ê–ß–ê: –ì–ª—É–±–æ–∫–∏–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–∞–π—Ç–µ {domain}

üèóÔ∏è –ö–û–ù–¢–ï–ö–°–¢ –ê–ù–ê–õ–ò–ó–ê:
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è {len(articles)} —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞ {domain}. –£ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏ –µ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è.

üìö –°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{articles_context}

üéØ –¶–ï–õ–¨: –°–æ–∑–¥–∞—Ç—å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
‚úÖ –õ–æ–≥–∏—á–µ—Å–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
‚úÖ –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏
‚úÖ SEO-—Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–∞–π—Ç–∞

üß† –ü–†–ò–ù–¶–ò–ü–´ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–û–ô –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:

1Ô∏è‚É£ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –°–í–Ø–ó–¨: –°—Ç–∞—Ç—å–∏ –¥–æ–ª–∂–Ω—ã –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –ø–æ —Å–º—ã—Å–ª—É
2Ô∏è‚É£ –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–¨: –ê–Ω–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –æ—Ä–≥–∞–Ω–∏—á–Ω–æ –≤–ø–∏—Å—ã–≤–∞—Ç—å—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏
3Ô∏è‚É£ –¶–ï–ù–ù–û–°–¢–¨: –ü–µ—Ä–µ—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
4Ô∏è‚É£ –°–ü–ï–¶–ò–§–ò–ß–ù–û–°–¢–¨: –ê–Ω–∫–æ—Ä –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã

üìù –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ê–ù–ö–û–†–û–í:
‚ùå –ü–õ–û–•–û: "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–ø–µ—Ä–µ–π—Ç–∏ —Å—é–¥–∞", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç"
‚úÖ –•–û–†–û–®–û: "–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∏–º–∞—Ç–∞ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ –≤ –í–æ–ª–≥–æ–≥—Ä–∞–¥–µ", "–ø–æ–ª–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–µ—Ä–µ–µ–∑–¥–∞ –≤ –ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫ —Å —Ü–µ–Ω–∞–º–∏ –Ω–∞ –∂–∏–ª—å–µ", "–¥–µ—Ç–∞–ª—å–Ω—ã–π –≥–∏–¥ –ø–æ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º –ö–∞–∑–∞–Ω–∏"

üéØ –ü–†–ò–ú–ï–†–´ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô:

–ò–°–¢–û–ß–ù–ò–ö: –ü–µ—Ä–µ–µ–∑–¥ –≤ –£—Ñ—É ‚Üí –¶–ï–õ–¨: –ü–µ—Ä–µ–µ–∑–¥ –≤ –ö–∞–∑–∞–Ω—å
–ê–ù–ö–û–†: "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–µ–∑–¥–∞ –≤ —Å—Ç–æ–ª–∏—Ü—É –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω–∞ —Å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"
–û–ë–û–°–ù–û–í–ê–ù–ò–ï: –û–±–µ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –ø–µ—Ä–µ–µ–∑–¥ –≤ —Ä–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–∏–µ —Å—Ç–æ–ª–∏—Ü—ã, —á–∏—Ç–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã

–ò–°–¢–û–ß–ù–ò–ö: –î–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –°–æ—á–∏ ‚Üí –¶–ï–õ–¨: –ü–µ—Ä–µ–µ–∑–¥ –≤ –°–æ—á–∏
–ê–ù–ö–û–†: "–ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ä–µ–µ–∑–¥–µ –≤ –≥–æ—Ä–æ–¥ —Å —É—á–µ—Ç–æ–º –∫–ª–∏–º–∞—Ç–∞ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏"
–û–ë–û–°–ù–û–í–ê–ù–ò–ï: –¢—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç—å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–µ–¥–µ—Ç –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∂–∏–∑–Ω–∏ –≤ –≥–æ—Ä–æ–¥–µ

üîç –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ö–ê–ñ–î–£–Æ —Å—Ç–∞—Ç—å—é –∏ –Ω–∞–π–¥–∏ –í–°–ï –ª–æ–≥–∏—á–Ω—ã–µ —Å–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏. –°–æ–∑–¥–∞–π –º–∏–Ω–∏–º—É–º 8 - 12 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –º–∞–∫—Å–∏–º—É–º –æ–ø—Ä–µ–¥–µ–ª–∏ —Å–∞–º –∏—Å—Ö–æ–¥—è –∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π.

üìã –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
[–ù–û–ú–ï–†] –ò–°–¢–û–ß–ù–ò–ö: [URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞] ‚Üí –¶–ï–õ–¨: [URL —Ü–µ–ª–∏]
–ê–ù–ö–û–†: "[—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∞–Ω–∫–æ—Ä, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã]"
–û–ë–û–°–ù–û–í–ê–ù–ò–ï: [–ø–æ—á–µ–º—É —ç—Ç–∞ —Å–≤—è–∑—å –ª–æ–≥–∏—á–Ω–∞ –∏ –ø–æ–ª–µ–∑–Ω–∞ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è]

üöÄ –ù–ê–ß–ò–ù–ê–ô –ê–ù–ê–õ–ò–ó:"""

        # –®–∞–≥ 5: –ó–∞–ø—Ä–æ—Å –∫ Ollama
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–ø—Ä–æ—Å –∫ Ollama", 5, 7, "–û—Ç–ø—Ä–∞–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ò–ò...")
            await websocket_manager.send_ollama_info(client_id, {
                "status": "starting",
                "model": OLLAMA_MODEL,
                "model_info": "qwen2.5:7b-optimized - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç 8192 —Ç–æ–∫–µ–Ω–∞",
                "articles_count": len(articles),
                "prompt_length": len(qwen_optimized_prompt),
                "timeout": 300,
                "settings": "temperature=0.4, ctx=8192, predict=1200, threads=6",
                "expected_recommendations": "8 - 15 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            })

        print("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å...")
        print(f"üìù –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(qwen_optimized_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º "–º—ã—Å–ª–∏" –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –∞–Ω–∞–ª–∏–∑–∞
        if client_id:
            await websocket_manager.send_ai_thinking(
                client_id,
                "–ò–∑—É—á–∞—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç–∞—Ç–µ–π –∏ –∏—â—É —Å–∫—Ä—ã—Ç—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏...",
                "preprocessing",
                "üîç"
            )

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=600.0) as client:
            # –î–æ–±–∞–≤–ª—è–µ–º "–º—ã—Å–ª–∏" –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if client_id:
                await websocket_manager.send_ai_thinking(
                    client_id,
                    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª—è—é –µ—ë —Ä–æ–ª—å –≤ –æ–±—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Å–∞–π—Ç–∞...",
                    "analyzing",
                    "üß†"
                )

            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": qwen_optimized_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.4,      # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é
                        "num_ctx": 8192,         # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                        "num_predict": 1200,     # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                        "top_p": 0.85,          # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
                        "top_k": 50,            # –†–∞—Å—à–∏—Ä—è–µ–º –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
                        "repeat_penalty": 1.08,  # –°–Ω–∏–∂–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                        "seed": 42,             # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–µ—Ä–Ω–æ
                        "stop": ["üöÄ –ù–ê–ß–ò–ù–ê–ô", "–ö–û–ù–ï–¶ –ê–ù–ê–õ–ò–ó–ê", "```", "---"],
                        "num_thread": 6
                    }
                },
                timeout=600
            )

        request_time = (datetime.now() - start_time).total_seconds()

        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "completed",
                "response_code": response.status_code,
                "request_time": f"{request_time:.1f}s",
                "response_length": len(response.text) if response.status_code == 200 else 0
            })

        if response.status_code != 200:
            error_msg = f"‚ùå Ollama –≤–µ—Ä–Ω—É–ª–∞ –∫–æ–¥ {response.status_code}"
            print(f"{error_msg}. –û—Ç–≤–µ—Ç: {response.text[:200]}...")

            # –ï—Å–ª–∏ 499 - –∫–ª–∏–µ–Ω—Ç –æ—Ç–º–µ–Ω–∏–ª –∑–∞–ø—Ä–æ—Å (—Ç–∞–π–º–∞—É—Ç)
            if response.status_code == 499:
                error_msg = "‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ Ollama. –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≤–ø–µ—Ä–≤—ã–µ –∏ —Ç—Ä–µ–±—É–µ—Ç –≤—Ä–µ–º–µ–Ω–∏."
                detailed_msg = f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ 2 - 3 –º–∏–Ω—É—Ç—ã. –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –¥–æ–ª–∂–Ω–∞ –æ—Å—Ç–∞—Ç—å—Å—è –≤ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏."
            else:
                detailed_msg = f"HTTP —Å—Ç–∞—Ç—É—Å: {response.status_code}. –û—Ç–≤–µ—Ç: {response.text[:100]}..."

            if client_id:
                await websocket_manager.send_error(client_id, error_msg, detailed_msg)
            return [], 0.0

        data = response.json()
        content = data.get("response", "")
        print(f"üìù –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print("üîç –û–¢–õ–ê–î–ö–ê: –û—Ç–≤–µ—Ç Ollama:")
        print("="*50)
        print(content)
        print("="*50)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç Ollama —á–µ—Ä–µ–∑ WebSocket –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI
        if client_id:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è preview
            preview_text = content[:500] + "..." if len(content) > 500 else content
            await websocket_manager.send_ollama_info(client_id, {
                "status": "response_preview",
                "preview": preview_text,
                "total_length": len(content),
                "model": OLLAMA_MODEL
            })

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º "–º—ã—Å–ª–∏" –æ —Ç–æ–º, —á—Ç–æ –æ–±–¥—É–º—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
            await websocket_manager.send_ai_thinking(
                client_id,
                f"–ü–æ–ª—É—á–∏–ª —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –≤ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏ –∏—â—É –ª—É—á—à–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...",
                "processing",
                "üí≠"
            )

        # –®–∞–≥ 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if client_id:
            await websocket_manager.send_step(client_id, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞", 6, 7, "–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç –ò–ò...")

        recommendations = parse_ollama_recommendations(content, domain, articles)

        print(f"üìä –û–¢–õ–ê–î–ö–ê: –ü–∞—Ä—Å–µ—Ä –Ω–∞—à–µ–ª {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏–∑ {len(articles)} —Å—Ç–∞—Ç–µ–π")

        # –®–∞–≥ 7: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ", 7, 7, f"–ì–æ—Ç–æ–≤–æ! –ü–æ–ª—É—á–µ–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞
        total_analysis_time = (datetime.now() - analysis_start_time).total_seconds()
        print(f"‚úÖ RAG-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∑–∞ {total_analysis_time:.1f}—Å")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –≤—Ä–µ–º—è –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        return recommendations[:25], total_analysis_time  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 25 –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ RAG-–∞–Ω–∞–ª–∏–∑–∞: {e}"
        print(error_msg)
        if client_id:
            await websocket_manager.send_error(client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞", str(e))

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∏ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ –æ—à–∏–±–∫–∏
        error_time = (datetime.now() - analysis_start_time).total_seconds()
        return [], error_time


def parse_ollama_recommendations(text: str, domain: str, articles: List[Dict]) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Ollama —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–æ–º–µ–Ω–∞ - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏."""
    recommendations = []

    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö URL –¥–ª—è –¥–æ–º–µ–Ω–∞
    valid_urls = set()
    articles_dict = {}  # –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ URL
    for article in articles:
        url = article['link']
        if domain.lower() in url.lower():
            valid_urls.add(url)
            articles_dict[url] = article

    print(f"üîç –û–¢–õ–ê–î–ö–ê: –í–∞–ª–∏–¥–Ω—ã–µ URL –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}: {len(valid_urls)}")

    # –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    enhanced_patterns = [
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç: –ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä | –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ  
        r'(?P<source>.+?)\s*->\s*(?P<target>.+?)\s*\|\s*(?P<anchor>.+?)\s*\|\s*(?P<reasoning>.+?)(?=\n|$)',
        
        # –§–æ—Ä–º–∞—Ç —Å –¥–≤–æ–π–Ω—ã–º–∏ –∑–≤–µ–∑–¥–æ—á–∫–∞–º–∏: **–ò—Å—Ç–æ—á–Ω–∏–∫:** URL **–¶–µ–ª—å:** URL | –∞–Ω–∫–æ—Ä | –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        r'\*\*–ò—Å—Ç–æ—á–Ω–∏–∫:\*\*\s*(?P<source>.+?)\s*\*\*–¶–µ–ª—å:\*\*\s*(?P<target>.+?)\s*\|\s*(?P<anchor>.+?)\s*\|\s*(?P<reasoning>.+?)(?=\n|$)',
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ —Å—Ç—Ä–µ–ª–∫–æ–π
        r'(?P<source>.+?)\s*‚Üí\s*(?P<target>.+?)\s*[\|\-]\s*(?P<anchor>.+?)\s*[\|\-]\s*(?P<reasoning>.+?)(?=\n|$)',
        
        # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        r'\d+\.\s*(?P<source>.+?)\s*->\s*(?P<target>.+?)\s*\|\s*(?P<anchor>.+?)\s*\|\s*(?P<reasoning>.+?)(?=\n|$)',
    ]
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL –∏–∑ —Å–∫–æ–±–æ–∫ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞
    def extract_url(text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ —Ç–µ–∫—Å—Ç–∞, —É–¥–∞–ª—è—è —Å–∫–æ–±–∫–∏ –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã."""
        if not text:
            return ""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ URL –≤ —Å–∫–æ–±–∫–∞—Ö
        url_in_brackets = re.search(r'\(([^)]+)\)', text)
        if url_in_brackets:
            url = url_in_brackets.group(1).strip()
            if url.startswith('http'):
                return url
        
        # –ò—â–µ–º URL –≤ —Ç–µ–∫—Å—Ç–µ
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+[^\s.,;:!?<>"{}|\\^`\[\]]'
        url_match = re.search(url_pattern, text)
        if url_match:
            return url_match.group(0)
            
        return text.strip()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∫–æ—Ä–∞ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
    def is_quality_anchor(anchor: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∫–æ—Ä–∞ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""
        if not anchor or len(anchor) < 3:
            return False
            
        anchor_lower = anchor.lower()
        
        # –ü–ª–æ—Ö–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        bad_patterns = [
            r'\b(?:—Å–∞–π—Ç|—Ä–µ—Å—É—Ä—Å|–ø–æ—Ä—Ç–∞–ª|–≤–µ–±-—Å–∞–π—Ç|–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ—Å—É—Ä—Å)\b',
            r'\b(?:–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç|–ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç|–≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞)\b',
            r'\b(?:–¥–æ–º–µ–Ω|—Å—Å—ã–ª–∫–∞|url)\b'
        ]
        
        for pattern in bad_patterns:
            if re.search(pattern, anchor_lower):
                return False
        
        # –•–æ—Ä–æ—à–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        good_patterns = [
            r'\b(?:–ø–æ–¥—Ä–æ–±–Ω\w+|–ø–æ–ª–Ω\w+|–¥–µ—Ç–∞–ª—å–Ω\w+|–≥–ª—É–±–æ–∫\w+|—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω\w+)\b',
            r'\b(?:—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤\w+|–≥–∞–π–¥\w+|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏\w+|–º–∞–Ω—É–∞–ª\w+)\b',
            r'\b(?:–æ–±–∑–æ—Ä\w+|—Å—Ä–∞–≤–Ω–µ–Ω–∏\w+|–∞–Ω–∞–ª–∏–∑\w+|–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏\w+)\b',
            r'\b(?:—Å–æ–≤–µ—Ç\w+|—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏\w+|—Å–µ–∫—Ä–µ—Ç\w+|–ª–∞–π—Ñ—Ö–∞–∫\w+)\b'
        ]
        
        for pattern in good_patterns:
            if re.search(pattern, anchor_lower):
                return True
                
        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö –ø–ª–æ—Ö–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –¥–ª–∏–Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è, —Å—á–∏—Ç–∞–µ–º –ø—Ä–∏–µ–º–ª–µ–º—ã–º
        return len(anchor) >= 10 and not any(word in anchor_lower for word in ['—Å–∞–π—Ç', '—Ä–µ—Å—É—Ä—Å', '–ø–æ—Ä—Ç–∞–ª'])

    lines = text.splitlines()
    print(f"üîç –û–¢–õ–ê–î–ö–ê: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(lines)} —Å—Ç—Ä–æ–∫ –æ—Ç–≤–µ—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏")

    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    for i, pattern in enumerate(enhanced_patterns, 1):
        matches = re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE)
        for match in matches:
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º {i}: {match.group()[:100]}...")
            
            try:
                source_raw = match.group('source').strip() if 'source' in match.groupdict() else ""
                target_raw = match.group('target').strip() if 'target' in match.groupdict() else ""
                anchor = match.group('anchor').strip().strip('"') if 'anchor' in match.groupdict() else ""
                reasoning = match.group('reasoning').strip() if 'reasoning' in match.groupdict() else ""
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–µ URL
                source = extract_url(source_raw)
                target = extract_url(target_raw)
                
                print(f"   üéØ –ò—Å—Ç–æ—á–Ω–∏–∫: {source[:60]}...")
                print(f"   üéØ –¶–µ–ª—å: {target[:60]}...")
                print(f"   üéØ –ê–Ω–∫–æ—Ä: {anchor}")
                print(f"   üéØ –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {reasoning[:50]}...")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
                if not source or not target or not anchor:
                    print(f"   ‚ùå –ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                    continue
                
                if not is_quality_anchor(anchor):
                    print(f"   ‚ùå –ù–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∫–æ—Ä: {anchor}")
                    continue
                
                if len(reasoning) < 10:
                    print(f"   ‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {len(reasoning)} —Å–∏–º–≤–æ–ª–æ–≤")
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
                source_valid = domain.lower() in source.lower() and source != target
                target_valid = domain.lower() in target.lower()
                
                print(f"   ‚úîÔ∏è –ò—Å—Ç–æ—á–Ω–∏–∫ –≤–∞–ª–∏–¥–µ–Ω: {source_valid}")
                print(f"   ‚úîÔ∏è –¶–µ–ª—å –≤–∞–ª–∏–¥–Ω–∞: {target_valid}")
                
                if source_valid and target_valid:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç –¥—É–±–ª–µ–π
                    is_duplicate = any(
                        rec["from"] == source and rec["to"] == target 
                        for rec in recommendations
                    )
                    
                    if not is_duplicate:
                        recommendations.append({
                            "from": source,
                            "to": target,
                            "anchor": anchor,
                            "comment": reasoning
                        })
                        print(f"   ‚úÖ –ü–†–ò–ù–Ø–¢–ê —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è #{len(recommendations)} (regex)")
                    else:
                        print(f"   ‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                else:
                    print(f"   ‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–∞: –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ URL –∏–ª–∏ –¥–æ–º–µ–Ω")
                    
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ regex match: {e}")
                continue

    # –ï—Å–ª–∏ regex –Ω–µ –Ω–∞—à–µ–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –∫–∞–∫ fallback
    if len(recommendations) < 3:
        print(f"üîÑ Regex –Ω–∞—à–µ–ª —Ç–æ–ª—å–∫–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–∞—Ä—Å–∏–Ω–≥")
        
        for i, line in enumerate(lines, 1):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
                
            print(f"   –°—Ç—Ä–æ–∫–∞ {i}: {line[:100]}...")

            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç Ollama (—Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥)
            if ('**–ò—Å—Ç–æ—á–Ω–∏–∫:**' in line and '**–¶–µ–ª—å:**' in line) or ('->' in line and '|' in line):
                print(f"      ‚úì –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Å—Ç—Ä–æ–∫–µ {i}")
                try:
                    source = ""
                    target = ""
                    anchor = ""
                    comment = ""

                    # –§–æ—Ä–º–∞—Ç 1: **–ò—Å—Ç–æ—á–Ω–∏–∫:** URL **–¶–µ–ª—å:** URL | –∞–Ω–∫–æ—Ä | –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                    if '**–ò—Å—Ç–æ—á–Ω–∏–∫:**' in line and '**–¶–µ–ª—å:**' in line:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                        source_match = line.split('**–ò—Å—Ç–æ—á–Ω–∏–∫:**')[1].split('**–¶–µ–ª—å:**')[0].strip()
                        source = extract_url(source_match)

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–ª—å –∏ –∞–Ω–∫–æ—Ä
                        target_part = line.split('**–¶–µ–ª—å:**')[1]
                        if '|' in target_part:
                            target_and_anchor = target_part.split('|')
                            target_raw = target_and_anchor[0].strip()
                            target = extract_url(target_raw)

                            # –ê–Ω–∫–æ—Ä –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                            if len(target_and_anchor) >= 2:
                                anchor = target_and_anchor[1].strip().strip('"')
                            if len(target_and_anchor) >= 3:
                                comment = target_and_anchor[2].strip()

                    # –§–æ—Ä–º–∞—Ç 2: URL -> URL | –∞–Ω–∫–æ—Ä | –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
                    elif '->' in line and '|' in line:
                        parts = line.split('|', 2)
                        if len(parts) >= 3:
                            link_part = parts[0].strip()
                            anchor = parts[1].strip().strip('"')
                            comment = parts[2].strip()

                            if '->' in link_part:
                                source_target = link_part.split('->', 1)
                                if len(source_target) == 2:
                                    source = extract_url(source_target[0].strip())
                                    target = extract_url(source_target[1].strip())

                    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
                    if not source or not target or not anchor:
                        print(f"      ‚ùå –ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                        continue

                    if not is_quality_anchor(anchor):
                        print(f"      ‚ùå –ù–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∫–æ—Ä: {anchor}")
                        continue

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
                    source_valid = domain.lower() in source.lower() and source != target
                    target_valid = domain.lower() in target.lower()

                    if source_valid and target_valid:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏
                        is_duplicate = any(
                            rec["from"] == source and rec["to"] == target 
                            for rec in recommendations
                        )
                        
                        if not is_duplicate:
                            recommendations.append({
                                "from": source,
                                "to": target,
                                "anchor": anchor,
                                "comment": comment
                            })
                            print(f"      ‚úÖ –ü–†–ò–ù–Ø–¢–ê —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è #{len(recommendations)} (fallback)")

                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {i}: {e}")
                    continue

    print(f"üìä –§–ò–ù–ê–õ: –ù–∞–π–¥–µ–Ω–æ {len(recommendations)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (regex + fallback)")
    return recommendations


async def warmup_ollama() -> bool:
    """–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ Ollama –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ç–∞–π–º–∞—É—Ç–æ–≤ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏."""
    print("üî• –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ Ollama —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏...")

    max_attempts = 3
    for attempt in range(1, max_attempts + 1):
        try:
            print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ–≥—Ä–µ–≤–∞ {attempt}/{max_attempts}")

            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç —Å –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
            timeout_seconds = 180 + (attempt * 60)  # 180s, 240s, 300s

            async with httpx.AsyncClient(timeout=timeout_seconds) as client:
                response = await client.post(
                    OLLAMA_URL,
                    json={
                        "model": OLLAMA_MODEL,
                        "prompt": "–¢–µ—Å—Ç",
                        "stream": False,
                        "options": {
                            "num_predict": 5,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                            "temperature": 0.1,
                            "num_ctx": 1024    # –ú–∞–ª–µ–Ω—å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã
                        }
                    },
                    timeout=timeout_seconds
                )

                if response.status_code == 200:
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≥—Ä–µ—Ç–∞ –∑–∞ –ø–æ–ø—ã—Ç–∫—É {attempt}")
                    return True
                else:
                    print(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt}: —Å—Ç–∞—Ç—É—Å {response.status_code}")
                    if attempt < max_attempts:
                        print(f"üîÑ –û–∂–∏–¥–∞–Ω–∏–µ 10 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                        await asyncio.sleep(10)

        except httpx.TimeoutException:
            print(f"‚è∞ –ü–æ–ø—ã—Ç–∫–∞ {attempt}: —Ç–∞–π–º–∞—É—Ç {timeout_seconds}s")
            if attempt < max_attempts:
                print(f"üîÑ –û–∂–∏–¥–∞–Ω–∏–µ 15 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                await asyncio.sleep(15)
        except Exception as e:
            print(f"‚ùå –ü–æ–ø—ã—Ç–∫–∞ {attempt}: –æ—à–∏–±–∫–∞ {e}")
            if attempt < max_attempts:
                print(f"üîÑ –û–∂–∏–¥–∞–Ω–∏–µ 10 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                await asyncio.sleep(10)

    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≥—Ä–µ—Ç—å –º–æ–¥–µ–ª—å –∑–∞ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫")
    return False


@app.on_event("startup")
async def on_startup() -> None:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ."""
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG-—Å–∏—Å—Ç–µ–º—É
    initialize_rag_system()

    # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º –º–æ–¥–µ–ª—å Ollama –≤ —Ñ–æ–Ω–µ (—Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
    async def delayed_warmup():
        await asyncio.sleep(30)  # –ñ–¥–µ–º 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ Ollama
        print("üöÄ –ù–∞—á–∏–Ω–∞—é –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ–≤ Ollama...")
        success = await warmup_ollama()
        if success:
            print("üî• –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≥—Ä–µ—Ç–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        else:
            print("‚ö†Ô∏è –ü—Ä–æ–≥—Ä–µ–≤ –Ω–µ —É–¥–∞–ª—Å—è, –Ω–æ —Å–µ—Ä–≤–∏—Å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Ä–∞–±–æ—Ç—É")

    asyncio.create_task(delayed_warmup())


# –¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —É–¥–∞–ª–µ–Ω –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞


@app.post("/api/v1/recommend")
async def recommend(req: RecommendRequest) -> dict[str, list[str]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""
    links = await generate_links(req.text)

    async with AsyncSessionLocal() as session:
        rec = Recommendation(text=req.text, links=links)
        session.add(rec)
        await session.commit()
    return {"links": links}


@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    """WebSocket —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
    await websocket_manager.connect(websocket, client_id)
    try:
        while True:
            # –û–∂–∏–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ (–ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è)
            data = await websocket.receive_text()
            if data == "ping":
                await websocket.send_text("pong")
    except WebSocketDisconnect:
        websocket_manager.disconnect(client_id)


@app.post("/api/v1/wp_index")
async def wp_index_domain(req: WPRequest) -> dict[str, object]:
    """–£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
    try:
        if req.client_id:
            await websocket_manager.send_step(
                req.client_id,
                "–ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏",
                0,
                5,
                f"–£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ {req.domain}"
            )

        # –í—ã–ø–æ–ª–Ω—è–µ–º —É–º–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
        posts, delta_stats = await fetch_and_store_wp_posts(req.domain, req.client_id)

        # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        success = await rag_manager.create_semantic_knowledge_base(req.domain, posts, req.client_id)

        if req.client_id:
            await websocket_manager.send_progress(req.client_id, {
                "type": "complete",
                "message": "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!",
                "delta_stats": delta_stats,
                "posts_count": len(posts),
                "indexed": success,
                "timestamp": datetime.now().isoformat()
            })

        return {
            "success": True,
            "domain": req.domain,
            "posts_count": len(posts),
            "delta_stats": delta_stats,
            "indexed": success
        }

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}"
        print(f"‚ùå {error_msg}")

        if req.client_id:
            await websocket_manager.send_error(req.client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", error_msg)

        raise HTTPException(status_code=500, detail=error_msg)


# –£—Å—Ç–∞—Ä–µ–≤—à–∏–π —ç–Ω–¥–ø–æ–∏–Ω—Ç wp_generate_recommendations —É–¥–∞–ª–µ–Ω


# –£—Å—Ç–∞—Ä–µ–≤—à–∏–π —ç–Ω–¥–ø–æ–∏–Ω—Ç wp_stable —É–¥–∞–ª–µ–Ω


# –£—Å—Ç–∞—Ä–µ–≤—à–∏–π —ç–Ω–¥–ø–æ–∏–Ω—Ç wp_comprehensive —É–¥–∞–ª–µ–Ω


@app.get("/api/v1/health")
async def health() -> dict[str, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏."""
    return {"status": "ok"}


@app.get("/api/v1/ollama_status")
async def ollama_status() -> dict[str, object]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    try:
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama: {OLLAMA_URL}")

        status_info = {
            "ollama_url": OLLAMA_URL,
            "model_name": OLLAMA_MODEL,
            "server_available": False,
            "model_loaded": False,
            "model_info": None,
            "error": None,
            "ready_for_work": False,
            "last_check": datetime.now().isoformat()
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞ Ollama
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞
                health_response = await client.get(f"{OLLAMA_URL.replace('/api/generate', '')}/api/tags")

                if health_response.status_code == 200:
                    status_info["server_available"] = True
                    models_data = health_response.json()
                    available_models = [model["name"] for model in models_data.get("models", [])]
                    status_info["available_models"] = available_models

                    print(f"‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å
                    if OLLAMA_MODEL in available_models:
                        status_info["model_loaded"] = True

                        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
                        try:
                            model_info_response = await client.post(
                                f"{OLLAMA_URL.replace('/api/generate', '')}/api/show",
                                json={"name": OLLAMA_MODEL}
                            )
                            if model_info_response.status_code == 200:
                                model_details = model_info_response.json()
                                status_info["model_info"] = {
                                    "name": model_details.get("modelfile", "").split("\n")[0] if model_details.get("modelfile") else OLLAMA_MODEL,
                                    "size": model_details.get("size", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                                    "modified_at": model_details.get("modified_at", ""),
                                    "parameters": model_details.get("parameters", {}),
                                    "template": model_details.get("template", "")[:100] + "..." if model_details.get("template") else ""
                                }
                        except Exception as model_info_error:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏: {model_info_error}")
                            status_info["model_info"] = {"error": str(model_info_error)}

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                        try:
                            test_response = await client.post(
                                OLLAMA_URL,
                                json={
                                    "model": OLLAMA_MODEL,
                                    "prompt": "–¢–µ—Å—Ç",
                                    "stream": False,
                                    "options": {"num_predict": 1}
                                },
                                timeout=15.0
                            )

                            if test_response.status_code == 200:
                                status_info["ready_for_work"] = True
                                test_data = test_response.json()
                                status_info["test_response_time"] = test_data.get("total_duration", 0) / 1000000  # –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                                print(f"‚úÖ –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                            else:
                                status_info["error"] = f"–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ –≤–µ—Ä–Ω—É–ª –∫–æ–¥ {test_response.status_code}"
                                print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ—É—Å–ø–µ—à–µ–Ω: {test_response.status_code}")

                        except Exception as test_error:
                            status_info["error"] = f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(test_error)}"
                            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {test_error}")
                    else:
                        status_info["error"] = f"–ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Ä–µ–¥–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö: {available_models}"
                        print(f"‚ùå –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

                        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
                        status_info["suggested_command"] = f"ollama pull {OLLAMA_MODEL}"

                else:
                    status_info["error"] = f"Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–∫–æ–¥ {health_response.status_code})"
                    print(f"‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {health_response.status_code}")

            except httpx.TimeoutException:
                status_info["error"] = "–¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É"
                print("‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama")
            except httpx.ConnectError:
                status_info["error"] = "–ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä ollama –∑–∞–ø—É—â–µ–Ω"
                print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama")
            except Exception as connection_error:
                status_info["error"] = f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {str(connection_error)}"
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {connection_error}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Å—Ç–∞—Ç—É—Å
        if status_info["ready_for_work"]:
            status_info["status"] = "ready"
            status_info["message"] = f"‚úÖ Ollama –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—å—é {OLLAMA_MODEL}"
        elif status_info["model_loaded"]:
            status_info["status"] = "model_loaded_but_not_ready"
            status_info["message"] = f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã"
        elif status_info["server_available"]:
            status_info["status"] = "server_available_model_missing"
            status_info["message"] = f"‚ö†Ô∏è Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –º–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        else:
            status_info["status"] = "server_unavailable"
            status_info["message"] = "‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"

        return status_info

    except Exception as e:
        error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Ollama: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {
            "ollama_url": OLLAMA_URL,
            "model_name": OLLAMA_MODEL,
            "server_available": False,
            "model_loaded": False,
            "ready_for_work": False,
            "status": "error",
            "message": "‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞",
            "error": error_msg,
            "last_check": datetime.now().isoformat()
        }


@app.get("/api/v1/recommendations")
async def list_recommendations() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Recommendation).order_by(Recommendation.id.desc())
        )
        items = [
            {"id": rec.id, "text": rec.text, "links": rec.links}
            for rec in result.scalars()
        ]
    return items


@app.get("/api/v1/domains")
async def list_domains() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ —Å –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Domain).order_by(Domain.updated_at.desc())
        )
        items = []
        for domain in result.scalars().all():
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
            actual_posts_count_result = await session.execute(
                select(func.count(WordPressPost.id))
                .where(WordPressPost.domain_id == domain.id)
            )
            actual_posts_count = actual_posts_count_result.scalar()

            # –û–±–Ω–æ–≤–ª—è–µ–º –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ
            if domain.total_posts != actual_posts_count:
                domain.total_posts = actual_posts_count
                await session.commit()
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç –≤ —Å–µ—Å—Å–∏–∏
                await session.refresh(domain)

            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            latest_analysis_result = await session.execute(
                select(AnalysisHistory)
                .where(AnalysisHistory.domain_id == domain.id)
                .order_by(AnalysisHistory.created_at.desc())
                .limit(1)
            )
            latest_analysis = latest_analysis_result.scalar_one_or_none()

            # –ü–æ–ª—É—á–∞–µ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            active_recommendations_result = await session.execute(
                select(LinkRecommendation)
                .where(LinkRecommendation.domain_id == domain.id)
                .where(LinkRecommendation.status == 'active')
            )
            active_recommendations_count = len(active_recommendations_result.scalars().all())

            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Å–∞–π—Ç–æ–≤
            insights_result = await session.execute(
                select(CumulativeInsight)
                .where(CumulativeInsight.domain_id == domain.id)
                .where(CumulativeInsight.status == 'discovered')
            )
            insights_count = len(insights_result.scalars().all())

            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            clusters_result = await session.execute(
                select(ThematicClusterAnalysis)
                .where(ThematicClusterAnalysis.domain_id == domain.id)
            )
            clusters_count = len(clusters_result.scalars().all())

            items.append({
                "id": domain.id,
                "name": domain.name,
                "display_name": domain.display_name,
                "description": domain.description,
                "total_posts": domain.total_posts,
                "total_analyses": domain.total_analyses,
                "last_analysis_at": domain.last_analysis_at.isoformat() if domain.last_analysis_at else None,
                "created_at": domain.created_at.isoformat(),
                "updated_at": domain.updated_at.isoformat(),
                "is_indexed": domain.total_posts > 0,
                "latest_recommendations": latest_analysis.recommendations if latest_analysis else [],
                "latest_recommendations_count": len(latest_analysis.recommendations) if latest_analysis else 0,
                "latest_recommendations_date": latest_analysis.created_at.isoformat() if latest_analysis else None,
                # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                "cumulative_recommendations": active_recommendations_count,
                "cumulative_insights": insights_count,
                "thematic_clusters": clusters_count,
                "intelligence_level": min(1.0, (active_recommendations_count * 0.1 + insights_count * 0.2 + clusters_count * 0.15))
            })
    return items


@app.get("/api/v1/analysis_history")
async def list_analysis_history() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤ WordPress —Å–∞–π—Ç–æ–≤ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(AnalysisHistory, Domain)
            .join(Domain, AnalysisHistory.domain_id == Domain.id)
            .order_by(AnalysisHistory.created_at.desc())
        )
        items = []
        for analysis, domain in result:
            items.append({
                "id": analysis.id,
                "domain": domain.name,
                "domain_display_name": domain.display_name,
                "posts_analyzed": analysis.posts_analyzed,
                "connections_found": analysis.connections_found,
                "recommendations_generated": analysis.recommendations_generated,
                "thematic_analysis": analysis.thematic_analysis,
                "semantic_metrics": analysis.semantic_metrics,
                "quality_assessment": analysis.quality_assessment,
                "llm_model_used": analysis.llm_model_used,
                "processing_time_seconds": analysis.processing_time_seconds,
                "created_at": analysis.created_at.isoformat(),
                "completed_at": analysis.completed_at.isoformat() if analysis.completed_at else None,
                "recommendations": analysis.recommendations,
                "summary": f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ {domain.name}: {analysis.posts_analyzed} —Å—Ç–∞—Ç–µ–π, {analysis.recommendations_generated} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            })
    return items


@app.get("/api/v1/analysis_history/{analysis_id}")
async def get_analysis_details(analysis_id: int) -> dict[str, object]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(AnalysisHistory, Domain)
            .join(Domain, AnalysisHistory.domain_id == Domain.id)
            .where(AnalysisHistory.id == analysis_id)
        )
        analysis_data = result.first()
        if not analysis_data:
            raise HTTPException(status_code=404, detail="–ê–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")

        analysis, domain = analysis_data

        return {
            "id": analysis.id,
            "domain": domain.name,
            "domain_info": {
                "name": domain.name,
                "display_name": domain.display_name,
                "description": domain.description,
                "language": domain.language,
                "category": domain.category,
                "total_posts": domain.total_posts,
                "total_analyses": domain.total_analyses
            },
            "posts_analyzed": analysis.posts_analyzed,
            "connections_found": analysis.connections_found,
            "recommendations_generated": analysis.recommendations_generated,
            "thematic_analysis": analysis.thematic_analysis,
            "semantic_metrics": analysis.semantic_metrics,
            "quality_assessment": analysis.quality_assessment,
            "llm_model_used": analysis.llm_model_used,
            "processing_time_seconds": analysis.processing_time_seconds,
            "created_at": analysis.created_at.isoformat(),
            "completed_at": analysis.completed_at.isoformat() if analysis.completed_at else None,
            "recommendations": analysis.recommendations
        }


@app.get("/api/v1/cumulative_insights/{domain_id}")
async def get_cumulative_insights(domain_id: int) -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(CumulativeInsight)
            .where(CumulativeInsight.domain_id == domain_id)
            .order_by(CumulativeInsight.impact_score.desc())
        )
        insights = []
        for insight in result.scalars().all():
            insights.append({
                "id": insight.id,
                "type": insight.insight_type,
                "category": insight.insight_category,
                "title": insight.title,
                "description": insight.description,
                "evidence": insight.evidence,
                "impact_score": insight.impact_score,
                "confidence_level": insight.confidence_level,
                "actionability": insight.actionability,
                "status": insight.status,
                "applied_count": insight.applied_count,
                "created_at": insight.created_at.isoformat()
            })
    return insights


@app.get("/api/v1/thematic_clusters/{domain_id}")
async def get_thematic_clusters(domain_id: int) -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(ThematicClusterAnalysis)
            .where(ThematicClusterAnalysis.domain_id == domain_id)
            .order_by(ThematicClusterAnalysis.linkability_potential.desc())
        )
        clusters = []
        for cluster in result.scalars().all():
            clusters.append({
                "id": cluster.id,
                "name": cluster.cluster_name,
                "description": cluster.cluster_description,
                "keywords": cluster.cluster_keywords,
                "article_count": cluster.article_count,
                "coherence_score": cluster.coherence_score,
                "diversity_score": cluster.diversity_score,
                "linkability_potential": cluster.linkability_potential,
                "evolution_stage": cluster.evolution_stage,
                "growth_trend": cluster.growth_trend,
                "created_at": cluster.created_at.isoformat()
            })
    return clusters


@app.get("/api/v1/cumulative_recommendations/{domain_id}")
async def get_cumulative_recommendations(domain_id: int) -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_id)
            .where(LinkRecommendation.status == 'active')
            .order_by(LinkRecommendation.quality_score.desc())
        )
        recommendations = []
        for rec in result.scalars().all():
            recommendations.append({
                "id": rec.id,
                "anchor_text": rec.anchor_text,
                "reasoning": rec.reasoning,
                "quality_score": rec.quality_score,
                "generation_count": rec.generation_count,
                "improvement_iterations": rec.improvement_iterations,
                "status": rec.status,
                "created_at": rec.created_at.isoformat(),
                "updated_at": rec.updated_at.isoformat()
            })
    return recommendations


@app.delete("/api/v1/clear_data")
async def clear_all_data() -> dict[str, str]:
    """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)."""
    try:
        from sqlalchemy import text

        async with AsyncSessionLocal() as session:
            # –û—á–∏—â–∞–µ–º –Ω–æ–≤—ã–µ –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            await session.execute(text("DELETE FROM cumulative_insights"))
            await session.execute(text("DELETE FROM thematic_cluster_analysis"))
            await session.execute(text("DELETE FROM link_recommendations"))
            await session.execute(text("DELETE FROM semantic_connections"))

            # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            await session.execute(text("DELETE FROM analysis_history"))
            await session.execute(text("DELETE FROM article_embeddings"))
            await session.execute(text("DELETE FROM wordpress_posts"))
            await session.execute(text("DELETE FROM recommendations"))

            # –°–±—Ä–æ—Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–∞–≤—Ç–æ–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç)
            await session.execute(text("ALTER SEQUENCE cumulative_insights_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE thematic_cluster_analysis_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE link_recommendations_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE semantic_connections_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE analysis_history_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE article_embeddings_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE wordpress_posts_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE recommendations_id_seq RESTART WITH 1"))

            await session.commit()

        # –û—á–∏—â–∞–µ–º ChromaDB
        try:
            if chroma_client:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏ —É–¥–∞–ª—è–µ–º –∏—Ö
                collections = chroma_client.list_collections()
                for collection in collections:
                    chroma_client.delete_collection(name=collection.name)
                    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è: {collection.name}")

                # –û—á–∏—â–∞–µ–º –∫–µ—à–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
                rag_manager.domain_collections.clear()
                cumulative_intelligence.connection_cache.clear()
                cumulative_intelligence.cluster_cache.clear()
                cumulative_intelligence.insight_cache.clear()
                print("üóëÔ∏è –û—á–∏—â–µ–Ω—ã –∫–µ—à–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤")
        except Exception as chroma_error:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ ChromaDB: {chroma_error}")

        print("üßπ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ—á–∏—â–µ–Ω—ã")
        return {"status": "ok", "message": "–í—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ—á–∏—â–µ–Ω—ã"}

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {str(e)}")


@app.get("/")
async def root():
    """–†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥."""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="http://localhost:3000")


async def generate_stable_recommendations(domain: str, client_id: Optional[str] = None) -> list[dict[str, str]]:
    """–°—Ç–∞–±–∏–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–¥–Ω–æ–π –∑–∞ —Ä–∞–∑ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 5 –∑–∞–ø—Ä–æ—Å–æ–≤."""
    print(f"üöÄ –ó–∞–ø—É—Å–∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")

    analysis_start_time = datetime.now()
    all_recommendations = []

    try:
        # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π", 1, 7, "–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")

        async with AsyncSessionLocal() as session:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω
            domain_result = await session.execute(
                select(Domain).where(Domain.name == domain)
            )
            domain_obj = domain_result.scalar_one_or_none()

            if not domain_obj:
                error_msg = f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"
                print(error_msg)
                if client_id:
                    await websocket_manager.send_error(client_id, error_msg)
                return []

            # –ü–æ–ª—É—á–∞–µ–º –¢–û–ü —Å—Ç–∞—Ç–µ–π —Å –ª—É—á—à–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_obj.id)
                .order_by(WordPressPost.linkability_score.desc(), WordPressPost.published_at.desc())
                .limit(10)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 –ª—É—á—à–∏—Ö —Å—Ç–∞—Ç–µ–π
            )
            db_posts = result.scalars().all()

        if not db_posts:
            error_msg = "‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg)
            return []

        print(f"üìä –í—ã–±—Ä–∞–Ω–æ {len(db_posts)} —Ç–æ–ø–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")

        # –®–∞–≥ 2: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        if client_id:
            await websocket_manager.send_step(client_id, "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö", 2, 7, "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π...")

        articles_data = []
        for post in db_posts:
            articles_data.append({
                "title": post.title,
                "link": post.link,
                "content": post.content[:800],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                "key_concepts": post.key_concepts or [],
                "content_type": post.content_type or "article",
                "linkability_score": post.linkability_score or 0.5
            })

        # –®–∞–≥ 3: –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏
        if client_id:
            await websocket_manager.send_step(client_id, "–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏", 3, 7, "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Ollama...")

        warmup_success = await warmup_ollama()
        if not warmup_success:
            print("‚ö†Ô∏è –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ –Ω–µ —É–¥–∞–ª—Å—è, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")

        # –®–∞–≥ 4 - 6: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–¥–Ω–æ–π
        max_recommendations = 5  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

        for i in range(max_recommendations):
            if client_id:
                await websocket_manager.send_step(
                    client_id,
                    f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ {i+1}/{max_recommendations}",
                    4 + i,
                    7,
                    f"–ü–æ–∏—Å–∫ —Å–≤—è–∑–µ–π –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ {i+1}..."
                )

            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –ø–∞—Ä—É —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            import random
            if len(articles_data) >= 2:
                article_pair = random.sample(articles_data, 2)

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–¥–Ω—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –¥–ª—è —ç—Ç–æ–π –ø–∞—Ä—ã
                single_rec = await generate_single_recommendation(
                    domain, article_pair, articles_data, i+1, client_id
                )

                if single_rec:
                    all_recommendations.extend(single_rec)
                    print(f"‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è {i+1}: –ø–æ–ª—É—á–µ–Ω–æ {len(single_rec)} —Å–≤—è–∑–µ–π")
                else:
                    print(f"‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è {i+1}: –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")

        # –®–∞–≥ 7: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ", 7, 7, f"–ì–æ—Ç–æ–≤–æ! –ü–æ–ª—É—á–µ–Ω–æ {len(all_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

        # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        unique_recommendations = deduplicate_and_rank_recommendations(all_recommendations, domain)

        total_analysis_time = (datetime.now() - analysis_start_time).total_seconds()
        print(f"‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(unique_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∑–∞ {total_analysis_time:.1f}—Å")

        return unique_recommendations

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"
        print(error_msg)
        if client_id:
            await websocket_manager.send_error(client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", str(e))

        error_time = (datetime.now() - analysis_start_time).total_seconds()
        return []


async def generate_single_recommendation(
    domain: str,
    article_pair: List[Dict],
    all_articles: List[Dict],
    rec_number: int,
    client_id: Optional[str] = None
) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–Ω—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –¥–ª—è –ø–∞—Ä—ã —Å—Ç–∞—Ç–µ–π."""

    if len(article_pair) != 2:
        return []

    source_article = article_pair[0]
    target_article = article_pair[1]

    # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    articles_context = ""
    for i, article in enumerate(all_articles, 1):
        title = article['title']
        content_snippet = article['content'][:300] if article.get('content') else ""
        key_concepts = article.get('key_concepts', [])[:8]  # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        content_type = article.get('content_type', '—Å—Ç–∞—Ç—å—è')
        linkability = article.get('linkability_score', 0.5)
        semantic_summary = article.get('semantic_summary', '').strip()
        difficulty_level = article.get('difficulty_level', '—Å—Ä–µ–¥–Ω–∏–π')
        target_audience = article.get('target_audience', '–æ–±—â–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è')

        articles_context += f"""üìÑ –°–¢–ê–¢–¨–Ø {i}: ¬´{title}¬ª
üîó URL: {article['link']}
üìä –ú–ï–¢–†–ò–ö–ò: –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content_type} | –°–ª–æ–∂–Ω–æ—Å—Ç—å: {difficulty_level} | –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –ª–∏–Ω–∫–æ–≤–∫–∏: {linkability:.2f}
üë• –ê–£–î–ò–¢–û–†–ò–Ø: {target_audience}
üß† –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¶–ò–ò: {', '.join(key_concepts) if key_concepts else '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'}
üìù –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –û–ü–ò–°–ê–ù–ò–ï: {semantic_summary if semantic_summary else '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞'}
üí° –°–û–î–ï–†–ñ–ê–ù–ò–ï: {content_snippet}...

"""

    # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
    qwen_optimized_prompt = f"""üéØ –ó–ê–î–ê–ß–ê: –ì–ª—É–±–æ–∫–∏–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–∞–π—Ç–µ {domain}

üèóÔ∏è –ö–û–ù–¢–ï–ö–°–¢ –ê–ù–ê–õ–ò–ó–ê:
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è {len(all_articles)} —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞ {domain}. –£ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏ –µ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è.

üìö –°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{articles_context}

üéØ –¶–ï–õ–¨: –°–æ–∑–¥–∞—Ç—å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
‚úÖ –õ–æ–≥–∏—á–µ—Å–∫–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
‚úÖ –î–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏
‚úÖ SEO-—Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–∞–π—Ç–∞

üß† –ü–†–ò–ù–¶–ò–ü–´ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–û–ô –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:

1Ô∏è‚É£ –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ê–Ø –°–í–Ø–ó–¨: –°—Ç–∞—Ç—å–∏ –¥–æ–ª–∂–Ω—ã –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –ø–æ —Å–º—ã—Å–ª—É
2Ô∏è‚É£ –ï–°–¢–ï–°–¢–í–ï–ù–ù–û–°–¢–¨: –ê–Ω–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –æ—Ä–≥–∞–Ω–∏—á–Ω–æ –≤–ø–∏—Å—ã–≤–∞—Ç—å—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏
3Ô∏è‚É£ –¶–ï–ù–ù–û–°–¢–¨: –ü–µ—Ä–µ—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
4Ô∏è‚É£ –°–ü–ï–¶–ò–§–ò–ß–ù–û–°–¢–¨: –ê–Ω–∫–æ—Ä –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã

üìù –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ê–ù–ö–û–†–û–í:
‚ùå –ü–õ–û–•–û: "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–ø–µ—Ä–µ–π—Ç–∏ —Å—é–¥–∞", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç"
‚úÖ –•–û–†–û–®–û: "–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∏–º–∞—Ç–∞ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏ –≤ –í–æ–ª–≥–æ–≥—Ä–∞–¥–µ", "–ø–æ–ª–Ω—ã–π —Ä–∞–∑–±–æ—Ä –ø–µ—Ä–µ–µ–∑–¥–∞ –≤ –ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫ —Å —Ü–µ–Ω–∞–º–∏ –Ω–∞ –∂–∏–ª—å–µ", "–¥–µ—Ç–∞–ª—å–Ω—ã–π –≥–∏–¥ –ø–æ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º –ö–∞–∑–∞–Ω–∏"

üéØ –ü–†–ò–ú–ï–†–´ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô:

–ò–°–¢–û–ß–ù–ò–ö: –ü–µ—Ä–µ–µ–∑–¥ –≤ –£—Ñ—É ‚Üí –¶–ï–õ–¨: –ü–µ—Ä–µ–µ–∑–¥ –≤ –ö–∞–∑–∞–Ω—å
–ê–ù–ö–û–†: "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–µ–∑–¥–∞ –≤ —Å—Ç–æ–ª–∏—Ü—É –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω–∞ —Å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"
–û–ë–û–°–ù–û–í–ê–ù–ò–ï: –û–±–µ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –ø–µ—Ä–µ–µ–∑–¥ –≤ —Ä–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–∏–µ —Å—Ç–æ–ª–∏—Ü—ã, —á–∏—Ç–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã

–ò–°–¢–û–ß–ù–ò–ö: –î–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –°–æ—á–∏ ‚Üí –¶–ï–õ–¨: –ü–µ—Ä–µ–µ–∑–¥ –≤ –°–æ—á–∏
–ê–ù–ö–û–†: "–ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ä–µ–µ–∑–¥–µ –≤ –≥–æ—Ä–æ–¥ —Å —É—á–µ—Ç–æ–º –∫–ª–∏–º–∞—Ç–∞ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∂–∏–∑–Ω–∏"
–û–ë–û–°–ù–û–í–ê–ù–ò–ï: –¢—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç—å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–µ–¥–µ—Ç –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∂–∏–∑–Ω–∏ –≤ –≥–æ—Ä–æ–¥–µ

üîç –ò–ù–°–¢–†–£–ö–¶–ò–Ø:
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ö–ê–ñ–î–£–Æ —Å—Ç–∞—Ç—å—é –∏ –Ω–∞–π–¥–∏ –í–°–ï –ª–æ–≥–∏—á–Ω—ã–µ —Å–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏. –°–æ–∑–¥–∞–π –º–∏–Ω–∏–º—É–º 8 - 12 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –º–∞–∫—Å–∏–º—É–º –æ–ø—Ä–µ–¥–µ–ª–∏ —Å–∞–º –∏—Å—Ö–æ–¥—è –∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π.

üìã –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
[–ù–û–ú–ï–†] –ò–°–¢–û–ß–ù–ò–ö: [URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞] ‚Üí –¶–ï–õ–¨: [URL —Ü–µ–ª–∏]
–ê–ù–ö–û–†: "[—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∞–Ω–∫–æ—Ä, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã]"
–û–ë–û–°–ù–û–í–ê–ù–ò–ï: [–ø–æ—á–µ–º—É —ç—Ç–∞ —Å–≤—è–∑—å –ª–æ–≥–∏—á–Ω–∞ –∏ –ø–æ–ª–µ–∑–Ω–∞ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è]

üöÄ –ù–ê–ß–ò–ù–ê–ô –ê–ù–ê–õ–ò–ó:"""

    # –®–∞–≥ 5: –ó–∞–ø—Ä–æ—Å –∫ Ollama
    if client_id:
        await websocket_manager.send_step(client_id, "–ó–∞–ø—Ä–æ—Å –∫ Ollama", 5, 7, "–û—Ç–ø—Ä–∞–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ò–ò...")
        await websocket_manager.send_ollama_info(client_id, {
            "status": "starting",
            "model": OLLAMA_MODEL,
            "model_info": "qwen2.5:7b - —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑",
            "articles_count": len(all_articles),
            "prompt_length": len(qwen_optimized_prompt),
            "timeout": 300,
            "settings": "temperature=0.4, ctx=8192, predict=1200, threads=6",
            "expected_recommendations": "8 - 15 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
        })

    print("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å...")
    print(f"üìù –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(qwen_optimized_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    start_time = datetime.now()
    try:
        async with httpx.AsyncClient(timeout=600.0) as client:
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": qwen_optimized_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.4,      # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é
                        "num_ctx": 8192,         # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                        "num_predict": 1200,     # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                        "top_p": 0.85,          # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
                        "top_k": 50,            # –†–∞—Å—à–∏—Ä—è–µ–º –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
                        "repeat_penalty": 1.08,  # –°–Ω–∏–∂–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                        "seed": 42,             # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–µ—Ä–Ω–æ
                        "stop": ["üöÄ –ù–ê–ß–ò–ù–ê–ô", "–ö–û–ù–ï–¶ –ê–ù–ê–õ–ò–ó–ê", "```", "---"],
                        "num_thread": 6
                    }
                }
            )

            request_time = (datetime.now() - start_time).total_seconds()

            if response.status_code != 200:
                print(f"‚ùå –û—à–∏–±–∫–∞ Ollama –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ {rec_number}: –∫–æ–¥ {response.status_code}")
                return []

            data = response.json()
            content = data.get("response", "")

            print(f"üìù –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è {rec_number}: –æ—Ç–≤–µ—Ç {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")

            # –ü–∞—Ä—Å–∏–º –æ–¥–Ω—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
            single_recommendations = parse_ollama_recommendations(content, domain, all_articles)

            return single_recommendations

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ {rec_number}: {e}")
        return []


async def generate_intelligent_semantic_recommendations(domain: str, client_id: Optional[str] = None) -> list[dict[str, str]]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–ª—É–±–æ–∫–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""

    analysis_start_time = datetime.now()

    try:
        if client_id:
            await websocket_manager.send_step(client_id, "–ù–∞—á–∞–ª–æ —É–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", 1, 8, "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–≤–∏–∂–∫–∞...")

        print(f"üß† –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {domain}...")

        # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
        async with AsyncSessionLocal() as session:
            try:
                domain_obj = await session.execute(
                    select(Domain).where(Domain.name == domain)
                )
                domain_record = domain_obj.scalar_one_or_none()

                if not domain_record:
                    error_msg = f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"
                    if client_id:
                        await websocket_manager.send_error(client_id, error_msg)
                    return []

                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã —Å –ø–æ–ª–Ω—ã–º–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                result = await session.execute(
                    select(WordPressPost)
                    .where(WordPressPost.domain_id == domain_record.id)
                    .where(WordPressPost.content.isnot(None))
                    .order_by(WordPressPost.linkability_score.desc())
                    .limit(50)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 50 —Å—Ç–∞—Ç–µ–π –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
                )

                all_posts = result.scalars().all()

                if not all_posts:
                    error_msg = f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}"
                    if client_id:
                        await websocket_manager.send_error(client_id, error_msg)
                    return []

                # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                print(f"üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –°–¢–ê–¢–ï–ô:")
                print(f"   üìä –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π –≤ –ë–î –¥–ª—è –¥–æ–º–µ–Ω–∞: {len(all_posts)}")
                print(f"   üìà –°—Ç–∞—Ç—å–∏ —Å linkability_score > 0: {len([p for p in all_posts if p.linkability_score and p.linkability_score > 0])}")
                print(f"   üìù –°—Ç–∞—Ç—å–∏ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º —Ä–µ–∑—é–º–µ: {len([p for p in all_posts if p.semantic_summary])}")
                print(f"   üîë –°—Ç–∞—Ç—å–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏: {len([p for p in all_posts if p.key_concepts])}")

                if client_id:
                    await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏–∫–∏", 2, 8, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(all_posts)} —Å—Ç–∞—Ç–µ–π...")

                print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_posts)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")

                # –®–∞–≥ 2: –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
                articles_context = ""
                posts_data = []

                for i, post in enumerate(all_posts, 1):
                    post_data = {
                        'title': post.title,
                        'link': post.link,
                        'content': post.content[:400],  # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        'key_concepts': post.key_concepts or [],
                        'semantic_summary': post.semantic_summary or '',
                        'content_type': post.content_type or '—Å—Ç–∞—Ç—å—è',
                        'difficulty_level': post.difficulty_level or '—Å—Ä–µ–¥–Ω–∏–π',
                        'target_audience': post.target_audience or '–æ–±—â–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è',
                        'linkability_score': post.linkability_score or 0.5,
                        'semantic_richness': post.semantic_richness or 0.5
                    }
                    posts_data.append(post_data)

                    # –°–æ–∑–¥–∞–µ–º –±–æ–≥–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–∏
                    concepts_str = ', '.join(post_data['key_concepts'][:10]) if post_data['key_concepts'] else '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'

                    articles_context += f"""üìÑ –°–¢–ê–¢–¨–Ø {i}: ¬´{post_data['title']}¬ª
üîó URL: {post_data['link']}
üìä –ê–ù–ê–õ–ò–¢–ò–ö–ê: –¢–∏–ø: {post_data['content_type']} | –°–ª–æ–∂–Ω–æ—Å—Ç—å: {post_data['difficulty_level']} | –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å–≤—è–∑–µ–π: {post_data['linkability_score']:.2f} | –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å: {post_data['semantic_richness']:.2f}
üë• –¶–ï–õ–ï–í–ê–Ø –ê–£–î–ò–¢–û–†–ò–Ø: {post_data['target_audience']}
üß† –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¶–ò–ò: {concepts_str}
üìù –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –†–ï–ó–Æ–ú–ï: {post_data['semantic_summary'] or '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞'}
üí° –§–†–ê–ì–ú–ï–ù–¢ –ö–û–ù–¢–ï–ù–¢–ê: {post_data['content']}...

"""

                if client_id:
                    await websocket_manager.send_step(client_id, "–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞", 3, 8, "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ò–ò...")

                # –®–∞–≥ 3: –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                intelligent_prompt = f"""üéØ –ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∞–π—Ç–∞ {domain}

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å {len(all_posts)} —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞ {domain}:

{articles_context}

–¶–ï–õ–¨: –°–æ–∑–¥–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏.

–ü–†–ò–ù–¶–ò–ü–´:
1. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏
2. –õ–æ–≥–∏—á–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
3. –ê–Ω–∫–æ—Ä—ã –æ–ø–∏—Å—ã–≤–∞—é—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
4. –ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ñ—Ä–∞–∑: "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ"

–ü–†–ò–ú–ï–†–´ –•–û–†–û–®–ò–• –ê–ù–ö–û–†–û–í:
‚ùå "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –í–æ–ª–≥–æ–≥—Ä–∞–¥—É"
‚úÖ "–∫–ª–∏–º–∞—Ç, —Ü–µ–Ω—ã –Ω–∞ –∂–∏–ª—å–µ –∏ —Ä–∞–±–æ—Ç—É –≤ –í–æ–ª–≥–æ–≥—Ä–∞–¥–µ"

‚ùå "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö"
‚úÖ "–º—É–∑–µ–∏ –∏ –ø–∞–º—è—Ç–Ω–∏–∫–∏ –ö–∞–∑–∞–Ω–∏ —Å —Ä–µ–∂–∏–º–æ–º —Ä–∞–±–æ—Ç—ã"

–ò–ù–°–¢–†–£–ö–¶–ò–Ø:
1. –ù–∞–π–¥–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏
2. –°–æ–∑–¥–∞–π 10 - 15 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
3. –î–ª—è –∫–∞–∂–¥–æ–π –¥–∞–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–§–û–†–ú–ê–¢:
[‚Ññ] –ò–°–¢–û–ß–ù–ò–ö: [URL] ‚Üí –¶–ï–õ–¨: [URL]
–ê–ù–ö–û–†: "[–æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∫–æ—Ä]"
–û–ë–û–°–ù–û–í–ê–ù–ò–ï: [–ø–æ—á–µ–º—É —Å–≤—è–∑—å –ª–æ–≥–∏—á–Ω–∞]

–ù–ê–ß–ù–ò:"""

                if client_id:
                    await websocket_manager.send_step(client_id, "–ó–∞–ø—Ä–æ—Å –∫ –ò–ò", 4, 8, "–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
                    await websocket_manager.send_ollama_info(client_id, {
                        "status": "starting",
                        "model": OLLAMA_MODEL,
                        "model_info": "qwen2.5:7b - —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑",
                        "articles_count": len(all_posts),
                        "prompt_length": len(intelligent_prompt),
                        "timeout": 300,
                        "settings": "temperature=0.3, ctx=10240, predict=1500",
                        "expected_recommendations": "12 - 18 —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
                    })

                print("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å...")
                print(f"üìù –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(intelligent_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –®–∞–≥ 4: –ó–∞–ø—Ä–æ—Å –∫ Ollama —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
                start_time = datetime.now()
                async with httpx.AsyncClient(timeout=600.0) as client:
                    response = await client.post(
                        OLLAMA_URL,
                        json={
                            "model": OLLAMA_MODEL,
                            "prompt": intelligent_prompt,
                            "stream": False,
                            "options": {
                                "temperature": OPTIMAL_TEMPERATURE,
                                "num_ctx": OPTIMAL_CONTEXT_SIZE,
                                "num_predict": OPTIMAL_PREDICTION_SIZE,
                                "top_p": OPTIMAL_TOP_P,
                                "top_k": OPTIMAL_TOP_K,
                                "repeat_penalty": OPTIMAL_REPEAT_PENALTY,
                                "seed": 123,
                                "stop": ["üöÄ –ù–ê–ß–ù–ò", "–ö–û–ù–ï–¶ –ê–ù–ê–õ–ò–ó–ê", "```"],
                                "num_thread": 6
                            }
                        },
                        timeout=600
                    )

                request_time = (datetime.now() - start_time).total_seconds()

                if client_id:
                    await websocket_manager.send_ollama_info(client_id, {
                        "status": "completed",
                        "response_code": response.status_code,
                        "request_time": f"{request_time:.1f}s",
                        "response_length": len(response.text) if response.status_code == 200 else 0
                    })

                if response.status_code != 200:
                    error_msg = f"‚ùå Ollama error: {response.status_code}"
                    if client_id:
                        await websocket_manager.send_error(client_id, error_msg)
                    return []

                data = response.json()
                content = data.get("response", "")

                if client_id:
                    await websocket_manager.send_step(client_id, "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞", 5, 8, "–ü–∞—Ä—Å–∏–Ω–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")

                print(f"üìù –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")
                print("üîç –≠–ö–°–ü–ï–†–¢–ù–´–ô –û–¢–í–ï–¢:")
                print("="*70)
                print(content[:1500] + "..." if len(content) > 1500 else content)
                print("="*70)

                # –®–∞–≥ 5: –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                recommendations = parse_intelligent_recommendations(content, domain, posts_data)

                if client_id:
                    await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 6, 8, f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

                total_time = (datetime.now() - analysis_start_time).total_seconds()
                print(f"‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∑–∞ {total_time:.1f}—Å")

                return recommendations[:25]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-25

            except Exception as db_error:
                error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ë–î: {db_error}"
                print(error_msg)
                if client_id:
                    await websocket_manager.send_error(client_id, "–û—à–∏–±–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö", str(db_error))
                return []

    except Exception as e:
        error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}"
        print(error_msg)
        if client_id:
            await websocket_manager.send_error(client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞", str(e))
        return []


def parse_intelligent_recommendations(text: str, domain: str, posts_data: List[Dict]) -> List[Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
    recommendations = []

    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö URL
    valid_urls = {post['link'] for post in posts_data}
    url_to_title = {post['link']: post['title'] for post in posts_data}

    print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(valid_urls)} –≤–∞–ª–∏–¥–Ω—ã—Ö URL")

    lines = text.split('\n')
    current_rec = {}

    for line_num, line in enumerate(lines, 1):
        line = line.strip()

        # –ò—â–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        if line.startswith('[') and '–ò–°–¢–û–ß–ù–ò–ö:' in line and '‚Üí' in line and '–¶–ï–õ–¨:' in line:
            # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä–æ–∫—É: [1] –ò–°–¢–û–ß–ù–ò–ö: URL ‚Üí –¶–ï–õ–¨: URL
            try:
                parts = line.split('‚Üí')
                if len(parts) >= 2:
                    source_part = parts[0].strip()
                    target_part = parts[1].strip()

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                    source_url = source_part.split('–ò–°–¢–û–ß–ù–ò–ö:')[-1].strip()

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º URL —Ü–µ–ª–∏
                    target_url = target_part.split('–¶–ï–õ–¨:')[-1].strip()

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
                    if source_url in valid_urls and target_url in valid_urls and source_url != target_url:
                        current_rec = {
                            'from': source_url,
                            'to': target_url,
                            'from_title': url_to_title.get(source_url, ''),
                            'to_title': url_to_title.get(target_url, ''),
                            'anchor': '',
                            'comment': ''
                        }
                        print(f"‚úì –ù–∞–π–¥–µ–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è {len(recommendations)+1}: {source_url} ‚Üí {target_url}")

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {line_num}: {e}")
                continue

        # –ò—â–µ–º –∞–Ω–∫–æ—Ä
        elif line.startswith('–ê–ù–ö–û–†:') and current_rec:
            anchor = line.replace('–ê–ù–ö–û–†:', '').strip().strip('"').strip("'")
            current_rec['anchor'] = anchor

        # –ò—â–µ–º –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
        elif ('–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –û–ë–û–°–ù–û–í–ê–ù–ò–ï:' in line or '–¶–ï–ù–ù–û–°–¢–¨ –î–õ–Ø –ß–ò–¢–ê–¢–ï–õ–Ø:' in line) and current_rec:
            if '–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –û–ë–û–°–ù–û–í–ê–ù–ò–ï:' in line:
                reasoning = line.split('–°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –û–ë–û–°–ù–û–í–ê–ù–ò–ï:')[-1].strip()
            else:
                reasoning = line.split('–¶–ï–ù–ù–û–°–¢–¨ –î–õ–Ø –ß–ò–¢–ê–¢–ï–õ–Ø:')[-1].strip()

            if current_rec.get('comment'):
                current_rec['comment'] += ' ' + reasoning
            else:
                current_rec['comment'] = reasoning

            # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ, –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
            if current_rec.get('anchor'):
                recommendations.append(current_rec.copy())
                current_rec = {}

    print(f"üìä –£—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

    # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    for rec in recommendations:
        quality_score = 0.5

        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω–Ω—ã–π —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∫–æ—Ä
        if len(rec['anchor']) > 30:
            quality_score += 0.2

        # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
        if len(rec['comment']) > 50:
            quality_score += 0.2

        # –®—Ç—Ä–∞—Ñ –∑–∞ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã
        generic_phrases = ['–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ', '–ø–µ—Ä–µ–π—Ç–∏']
        if any(phrase in rec['anchor'].lower() for phrase in generic_phrases):
            quality_score -= 0.3

        rec['quality_score'] = min(quality_score, 1.0)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    recommendations.sort(key=lambda x: x['quality_score'], reverse=True)

    return recommendations


@app.post("/api/v1/wp_intelligent")
async def wp_intelligent_recommendations(req: WPRequest) -> dict[str, list[dict[str, str]]]:
    """üß† –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
    try:
        if not req.domain:
            raise HTTPException(status_code=400, detail="–î–æ–º–µ–Ω –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω")

        print(f"üß† –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {req.domain}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É –Ω–æ–≤—É—é —É–ª—É—á—à–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        recommendations = await generate_intelligent_semantic_recommendations(req.domain, req.client_id)

        if not recommendations:
            return {
                "message": "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞",
                "recommendations": [],
                "domain": req.domain,
                "analysis_type": "intelligent_semantic",
                "count": 0
            }

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
        formatted_recommendations = []
        for rec in recommendations:
            formatted_recommendations.append({
                "from": rec.get('from', ''),
                "to": rec.get('to', ''),
                "anchor": rec.get('anchor', ''),
                "comment": rec.get('comment', ''),
                "quality_score": rec.get('quality_score', 0.5)
            })

        print(f"‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(formatted_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

        return {
            "message": f"–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!",
            "recommendations": formatted_recommendations,
            "domain": req.domain,
            "analysis_type": "intelligent_semantic",
            "count": len(formatted_recommendations)
        }

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}"
        print(error_msg)

        if req.client_id:
            await websocket_manager.send_error(req.client_id, "–û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", str(e))

        raise HTTPException(status_code=500, detail=error_msg)


async def ensure_ollama_model_context(model_name: str, context_size: int = OPTIMAL_CONTEXT_SIZE) -> bool:
    """–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ Ollama."""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
            response = await client.post(
                f"{OLLAMA_URL.replace('/api/generate', '/api/show')}",
                json={"name": model_name}
            )

            if response.status_code == 200:
                model_info = response.json()
                current_ctx = model_info.get("model_info", {}).get("num_ctx", 2048)

                print(f"üîß –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ {model_name}: {current_ctx}")

                if current_ctx < context_size:
                    print(f"‚öôÔ∏è  –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ {context_size}...")

                    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å —Å –Ω—É–∂–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è "–ø—Ä–æ–≥—Ä–µ–≤–∞"
                    warmup_response = await client.post(
                        OLLAMA_URL,
                        json={
                            "model": model_name,
                            "prompt": "–¢–µ—Å—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
                            "stream": False,
                            "options": {
                                "num_ctx": context_size,
                                "num_predict": 1,
                                "temperature": 0.1
                            }
                        },
                        timeout=60
                    )

                    if warmup_response.status_code == 200:
                        print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ {context_size}")
                        return True
                    else:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {warmup_response.status_code}")
                        return False
                else:
                    print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ: {current_ctx}")
                    return True
            else:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏: {response.status_code}")
                return False

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏: {e}")
        return False


@app.post("/api/v1/benchmark_model")
async def benchmark_model_endpoint(request: dict) -> dict[str, str]:
    """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    global OLLAMA_MODEL

    model_name = request.get("model_name")
    if not model_name:
        return {
            "status": "error",
            "message": "model_name –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä"
        }

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(f"{OLLAMA_URL.replace('/api/generate', '/api/tags')}")

            if response.status_code == 200:
                models_data = response.json()
                available_models = [model["name"] for model in models_data.get("models", [])]

                if model_name in available_models:
                    OLLAMA_MODEL = model_name

                    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
                    test_response = await client.post(
                        OLLAMA_URL,
                        json={
                            "model": model_name,
                            "prompt": "–¢–µ—Å—Ç –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏",
                            "stream": False,
                            "options": {"num_predict": 5}
                        },
                        timeout=30
                    )

                    if test_response.status_code == 200:
                        return {
                            "status": "success",
                            "message": f"–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∞ –Ω–∞ {model_name}",
                            "current_model": OLLAMA_MODEL,
                            "available_models": str(available_models)
                        }
                    else:
                        return {
                            "status": "error",
                            "message": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"
                        }
                else:
                    return {
                        "status": "error",
                        "message": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {available_models}"
                    }
            else:
                return {
                    "status": "error",
                    "message": "Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
                }

    except Exception as e:
        return {
            "status": "error",
            "message": f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}"
        }


# ========================================
# üéØ BENCHMARK API ENDPOINTS
# ========================================

@app.get("/api/v1/models/available")
async def get_available_models() -> dict[str, list]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ Ollama."""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(f"{OLLAMA_URL.replace('/api/generate', '/api/tags')}")

            if response.status_code == 200:
                models_data = response.json()
                available_models = []

                for model in models_data.get("models", []):
                    model_name = model["name"]
                    model_size = model.get("size", 0)
                    modified_at = model.get("modified_at", "")

                    available_models.append({
                        "name": model_name,
                        "size_bytes": model_size,
                        "size_gb": round(model_size / (1024**3), 2),
                        "modified_at": modified_at,
                        "family": model.get("details", {}).get("family", "unknown"),
                        "format": model.get("details", {}).get("format", "unknown")
                    })

                return {
                    "status": "success",
                    "models": available_models,
                    "count": len(available_models)
                }
            else:
                return {
                    "status": "error",
                    "models": [],
                    "error": "Ollama server –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
                }

    except Exception as e:
        return {
            "status": "error",
            "models": [],
            "error": str(e)
        }


@app.get("/api/v1/models/configurations")
async def get_model_configurations() -> list[dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∏–∑ –ë–î."""
    try:
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(ModelConfiguration).order_by(ModelConfiguration.model_name)
            )
            configs = result.scalars().all()

            return [
                {
                    "id": config.id,
                    "model_name": config.model_name,
                    "display_name": config.display_name,
                    "description": config.description,
                    "model_type": config.model_type,
                    "is_active": config.is_active,
                    "is_available": config.is_available,
                    "context_size": config.context_size,
                    "max_tokens": config.max_tokens,
                    "avg_tokens_per_second": config.avg_tokens_per_second,
                    "quality_score": config.quality_score,
                    "last_checked_at": config.last_checked_at.isoformat() if config.last_checked_at else None,
                    "benchmark_runs_count": len(config.benchmark_runs)
                }
                for config in configs
            ]

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π: {e}")
        return []


@app.post("/api/v1/models/configurations")
async def create_or_update_model_configuration(req: ModelConfigRequest) -> dict[str, str]:
    """–°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏."""
    try:
        async with AsyncSessionLocal() as session:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            result = await session.execute(
                select(ModelConfiguration).where(ModelConfiguration.model_name == req.model_name)
            )
            config = result.scalar_one_or_none()

            if config:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é
                if req.display_name:
                    config.display_name = req.display_name
                if req.description:
                    config.description = req.description
                if req.default_parameters:
                    config.default_parameters = req.default_parameters
                if req.seo_optimized_params:
                    config.seo_optimized_params = req.seo_optimized_params
                if req.benchmark_params:
                    config.benchmark_params = req.benchmark_params

                config.updated_at = datetime.utcnow()
                action = "updated"
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
                config = ModelConfiguration(
                    model_name=req.model_name,
                    display_name=req.display_name or req.model_name,
                    description=req.description,
                    model_type="ollama",  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    default_parameters=req.default_parameters or {},
                    seo_optimized_params=req.seo_optimized_params or {},
                    benchmark_params=req.benchmark_params or {}
                )
                session.add(config)
                action = "created"

            await session.commit()

            return {
                "status": "success",
                "message": f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {req.model_name} {action}",
                "model_name": req.model_name
            }

    except Exception as e:
        return {
            "status": "error",
            "message": f"–û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π: {str(e)}"
        }


@app.post("/api/v1/benchmark/run")
async def run_benchmark(req: BenchmarkRequest) -> dict[str, object]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    try:
        if not req.models:
            raise HTTPException(status_code=400, detail="–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å –±–µ–Ω—á–º–∞—Ä–∫–∞
        from advanced_seo_benchmark import AdvancedSEOBenchmark

        benchmark = AdvancedSEOBenchmark()

        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å–∏ –≤ –ë–î –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        benchmark_runs = []

        async with AsyncSessionLocal() as session:
            for model_name in req.models:
                # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
                result = await session.execute(
                    select(ModelConfiguration).where(ModelConfiguration.model_name == model_name)
                )
                model_config = result.scalar_one_or_none()

                if not model_config:
                    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                    model_config = ModelConfiguration(
                        model_name=model_name,
                        display_name=model_name,
                        model_type="ollama"
                    )
                    session.add(model_config)
                    await session.flush()  # –ü–æ–ª—É—á–∞–µ–º ID

                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
                benchmark_run = BenchmarkRun(
                    name=req.name,
                    description=req.description,
                    benchmark_type=req.benchmark_type,
                    model_config_id=model_config.id,
                    iterations=req.iterations,
                    status="pending",
                    test_cases_config={
                        "benchmark_type": req.benchmark_type,
                        "iterations": req.iterations
                    }
                )
                session.add(benchmark_run)
                benchmark_runs.append(benchmark_run)

            await session.commit()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        results = {}

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º WebSocket —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ
        if req.client_id:
            await websocket_manager.send_progress(req.client_id, {
                "type": "benchmark_started",
                "models": req.models,
                "benchmark_type": req.benchmark_type
            })

        for idx, model_name in enumerate(req.models):
            if req.client_id:
                await websocket_manager.send_step(
                    req.client_id,
                    f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}",
                    idx + 1,
                    len(req.models),
                    f"–ó–∞–ø—É—Å–∫ {req.iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π..."
                )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–∏
            try:
                metrics = await benchmark.benchmark_model_advanced(model_name, req.iterations)
                results[model_name] = metrics

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ë–î
                async with AsyncSessionLocal() as session:
                    run = next((r for r in benchmark_runs if r.model_config.model_name == model_name), None)
                    if run:
                        result = await session.execute(
                            select(BenchmarkRun).where(BenchmarkRun.id == run.id)
                        )
                        db_run = result.scalar_one_or_none()
                        if db_run:
                            db_run.status = "completed"
                            db_run.completed_at = datetime.utcnow()
                            db_run.overall_score = metrics.overall_score
                            db_run.quality_score = metrics.language_quality_score
                            db_run.performance_score = metrics.tokens_per_second
                            db_run.results = {
                                "metrics": metrics.__dict__,
                                "model_name": model_name
                            }
                            db_run.duration_seconds = (db_run.completed_at - db_run.started_at).total_seconds()
                            await session.commit()

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                results[model_name] = {"error": str(e)}

                # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ failed –≤ –ë–î
                async with AsyncSessionLocal() as session:
                    run = next((r for r in benchmark_runs if r.model_config.model_name == model_name), None)
                    if run:
                        result = await session.execute(
                            select(BenchmarkRun).where(BenchmarkRun.id == run.id)
                        )
                        db_run = result.scalar_one_or_none()
                        if db_run:
                            db_run.status = "failed"
                            db_run.error_message = str(e)
                            await session.commit()

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        if req.client_id:
            await websocket_manager.send_progress(req.client_id, {
                "type": "benchmark_completed",
                "results": {k: v.__dict__ if hasattr(v, '__dict__') else v for k, v in results.items()}
            })

        return {
            "status": "success",
            "message": f"–ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {len(req.models)} –º–æ–¥–µ–ª–µ–π",
            "results": results,
            "benchmark_runs": [run.id for run in benchmark_runs]
        }

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}"
        print(error_msg)

        if req.client_id:
            await websocket_manager.send_error(req.client_id, "–û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞", str(e))

        raise HTTPException(status_code=500, detail=error_msg)


@app.get("/api/v1/benchmark/history")
async def get_benchmark_history() -> list[dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—É—Å–∫–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""
    try:
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(BenchmarkRun, ModelConfiguration)
                .join(ModelConfiguration)
                .order_by(BenchmarkRun.created_at.desc())
                .limit(50)  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50 –∑–∞–ø—É—Å–∫–æ–≤
            )
            runs = result.all()

            history = []
            for run, model_config in runs:
                history.append({
                    "id": run.id,
                    "name": run.name,
                    "description": run.description,
                    "benchmark_type": run.benchmark_type,
                    "model_name": model_config.model_name,
                    "model_display_name": model_config.display_name,
                    "status": run.status,
                    "iterations": run.iterations,
                    "overall_score": run.overall_score,
                    "quality_score": run.quality_score,
                    "performance_score": run.performance_score,
                    "duration_seconds": run.duration_seconds,
                    "started_at": run.started_at.isoformat(),
                    "completed_at": run.completed_at.isoformat() if run.completed_at else None,
                    "error_message": run.error_message
                })

            return history

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: {e}")
        return []


@app.get("/api/v1/benchmark/{run_id}")
async def get_benchmark_details(run_id: int) -> dict[str, object]:
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—É—Å–∫–µ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    try:
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(BenchmarkRun, ModelConfiguration)
                .join(ModelConfiguration)
                .where(BenchmarkRun.id == run_id)
            )
            run_data = result.first()

            if not run_data:
                raise HTTPException(status_code=404, detail="–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω")

            run, model_config = run_data

            return {
                "id": run.id,
                "name": run.name,
                "description": run.description,
                "benchmark_type": run.benchmark_type,
                "model_config": {
                    "id": model_config.id,
                    "name": model_config.model_name,
                    "display_name": model_config.display_name,
                    "model_type": model_config.model_type,
                    "context_size": model_config.context_size,
                    "max_tokens": model_config.max_tokens
                },
                "test_cases_config": run.test_cases_config,
                "results": run.results,
                "metrics": run.metrics,
                "status": run.status,
                "iterations": run.iterations,
                "overall_score": run.overall_score,
                "quality_score": run.quality_score,
                "performance_score": run.performance_score,
                "efficiency_score": run.efficiency_score,
                "duration_seconds": run.duration_seconds,
                "started_at": run.started_at.isoformat(),
                "completed_at": run.completed_at.isoformat() if run.completed_at else None,
                "error_message": run.error_message
            }

    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/benchmark/compare")
async def compare_benchmark_runs(run_ids: List[int]) -> dict[str, object]:
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""
    try:
        if len(run_ids) < 2:
            raise HTTPException(status_code=400, detail="–ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –∑–∞–ø—É—Å–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")

        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(BenchmarkRun, ModelConfiguration)
                .join(ModelConfiguration)
                .where(BenchmarkRun.id.in_(run_ids))
            )
            runs_data = result.all()

            if len(runs_data) != len(run_ids):
                raise HTTPException(status_code=404, detail="–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—É—Å–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            comparison_data = []
            for run, model_config in runs_data:
                comparison_data.append({
                    "run_id": run.id,
                    "model_name": model_config.model_name,
                    "overall_score": run.overall_score or 0,
                    "quality_score": run.quality_score or 0,
                    "performance_score": run.performance_score or 0,
                    "duration_seconds": run.duration_seconds or 0,
                    "results": run.results
                })

            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ
            best_overall = max(comparison_data, key=lambda x: x["overall_score"])
            best_quality = max(comparison_data, key=lambda x: x["quality_score"])
            best_performance = max(comparison_data, key=lambda x: x["performance_score"])
            fastest = min(comparison_data, key=lambda x: x["duration_seconds"])

            return {
                "comparison_data": comparison_data,
                "winners": {
                    "best_overall": {
                        "model": best_overall["model_name"],
                        "score": best_overall["overall_score"]
                    },
                    "best_quality": {
                        "model": best_quality["model_name"],
                        "score": best_quality["quality_score"]
                    },
                    "best_performance": {
                        "model": best_performance["model_name"],
                        "score": best_performance["performance_score"]
                    },
                    "fastest": {
                        "model": fastest["model_name"],
                        "duration": fastest["duration_seconds"]
                    }
                },
                "analysis": {
                    "total_runs": len(comparison_data),
                    "avg_overall_score": sum(x["overall_score"] for x in comparison_data) / len(comparison_data),
                    "avg_quality_score": sum(x["quality_score"] for x in comparison_data) / len(comparison_data),
                    "avg_performance_score": sum(x["performance_score"] for x in comparison_data) / len(comparison_data)
                }
            }

    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/ai_insights/semantic_network")
async def get_semantic_network_insights() -> dict[str, object]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏ –ò–ò."""
    try:
        network_insights = thought_generator.get_network_insights()
        thought_history = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –º—ã—Å–ª–µ–π
        for thought in thought_generator.thought_history[-10:]:
            thought_history.append({
                "thought_id": thought.thought_id,
                "stage": thought.stage,
                "content": thought.content,
                "confidence": thought.confidence,
                "semantic_weight": thought.semantic_weight,
                "related_concepts": thought.related_concepts,
                "reasoning_chain": thought.reasoning_chain,
                "timestamp": thought.timestamp.isoformat()
            })
        
        return {
            "semantic_network": network_insights,
            "recent_thoughts": thought_history,
            "thought_statistics": {
                "total_thoughts": len(thought_generator.thought_history),
                "avg_confidence": np.mean([t.confidence for t in thought_generator.thought_history]) if thought_generator.thought_history else 0.0,
                "avg_semantic_weight": np.mean([t.semantic_weight for t in thought_generator.thought_history]) if thought_generator.thought_history else 0.0,
                "stage_distribution": {
                    stage: len([t for t in thought_generator.thought_history if t.stage == stage])
                    for stage in ["analyzing", "connecting", "evaluating", "optimizing"]
                }
            },
            "ai_performance": {
                "concepts_processed": len(thought_generator.concept_embeddings),
                "reasoning_patterns": len(thought_generator.reasoning_patterns),
                "network_complexity": len(thought_generator.semantic_network)
            }
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞: {str(e)}")


@app.post("/api/v1/ai_insights/reset_network")
async def reset_semantic_network() -> dict[str, str]:
    """–°–±—Ä–æ—Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏ –ò–ò –¥–ª—è –Ω–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    try:
        thought_generator.thought_history.clear()
        thought_generator.concept_embeddings.clear()
        thought_generator.reasoning_patterns.clear()
        thought_generator.semantic_network.clear()
        
        return {
            "status": "success",
            "analytics": {
                "correlation_analysis": {
                    "confidence_weight_correlation": float(correlation),
                    "interpretation": "–≤—ã—Å–æ–∫–∞—è" if abs(correlation) > 0.7 else "—Å—Ä–µ–¥–Ω—è—è" if abs(correlation) > 0.3 else "–Ω–∏–∑–∫–∞—è"
                },
                "stage_performance": stage_analysis,
                "overall_metrics": {
                    "total_thoughts": len(thought_generator.thought_history),
                    "avg_confidence": float(np.mean(confidences)),
                    "confidence_stability": float(1.0 - np.std(confidences)),
                    "semantic_richness": float(np.mean(semantic_weights)),
                    "cognitive_diversity": len(set(t.stage for t in thought_generator.thought_history))
                }
            }
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞: {str(e)}")
