"""FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""

from __future__ import annotations

import asyncio
import os
import re
import json
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict

import httpx
import nltk
import numpy as np
import chromadb
import ollama
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from sqlalchemy import Integer, Text, JSON, select, DateTime, ARRAY, Float
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, sessionmaker
from datetime import datetime

# –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
RUSSIAN_STOP_WORDS = set(stopwords.words('russian'))

app = FastAPI()

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class Base(DeclarativeBase):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–µ–π."""


class Recommendation(Base):
    """–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""

    __tablename__ = "recommendations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    text: Mapped[str] = mapped_column(Text)
    links: Mapped[list[str]] = mapped_column(JSON)


class WordPressPost(Base):
    """–ú–æ–¥–µ–ª—å —Å—Ç–∞—Ç–µ–π WordPress —Å–∞–π—Ç–∞."""

    __tablename__ = "wordpress_posts"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain: Mapped[str] = mapped_column(Text)
    wp_post_id: Mapped[int] = mapped_column(Integer)
    title: Mapped[str] = mapped_column(Text)
    content: Mapped[str] = mapped_column(Text)
    excerpt: Mapped[str] = mapped_column(Text, nullable=True)
    link: Mapped[str] = mapped_column(Text)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)


class AnalysisHistory(Base):
    """–ú–æ–¥–µ–ª—å –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ WordPress —Å–∞–π—Ç–æ–≤."""

    __tablename__ = "analysis_history"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain: Mapped[str] = mapped_column(Text)
    posts_count: Mapped[int] = mapped_column(Integer)
    recommendations_count: Mapped[int] = mapped_column(Integer) 
    recommendations: Mapped[list[dict]] = mapped_column(JSON)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    summary: Mapped[str] = mapped_column(Text, nullable=True)


class ArticleEmbedding(Base):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å—Ç–∞—Ç–µ–π."""

    __tablename__ = "article_embeddings"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain: Mapped[str] = mapped_column(Text)
    wp_post_id: Mapped[int] = mapped_column(Integer)
    title: Mapped[str] = mapped_column(Text)
    content_snippet: Mapped[str] = mapped_column(Text)
    link: Mapped[str] = mapped_column(Text)
    embedding_vector: Mapped[str] = mapped_column(Text)  # JSON —Å—Ç—Ä–æ–∫–∞ —Å –≤–µ–∫—Ç–æ—Ä–æ–º
    themes: Mapped[list[str]] = mapped_column(JSON)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)


class RecommendRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""

    text: str


class WPRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ WordPress-—Å–∞–π—Ç–∞."""

    domain: str


OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3")

DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://seo_user:seo_pass@localhost/seo_db",
)

engine = create_async_engine(DATABASE_URL)
AsyncSessionLocal = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã
embedding_model = None
chroma_client = None

def initialize_rag_system():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î."""
    global embedding_model, chroma_client
    try:
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã...")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        chroma_client = chromadb.PersistentClient(path="./chroma_db")
        print("‚úÖ RAG-—Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
        embedding_model = None
        chroma_client = None


def clean_text_simple(text: str) -> str:
    """–ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
    clean_text = BeautifulSoup(text, 'html.parser').get_text()
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = word_tokenize(clean_text.lower(), language='russian')
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
    filtered_tokens = []
    for token in tokens:
        if token.isalpha() and token not in RUSSIAN_STOP_WORDS and len(token) > 2:
            filtered_tokens.append(token)
    
    return ' '.join(filtered_tokens)


def extract_key_themes(posts: List[Dict[str, str]]) -> Dict[str, List[str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –∏–∑ –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é TF-IDF."""
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    texts = []
    for post in posts:
        title = post.get('title', '')
        content = post.get('content', '')
        combined_text = f"{title} {content}"
        cleaned = clean_text_simple(combined_text)
        texts.append(cleaned)
    
    if not texts:
        return {}
    
    # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    vectorizer = TfidfVectorizer(
        max_features=50,
        ngram_range=(1, 2),
        min_df=1,
        max_df=0.8
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞
        themes = {}
        for i, post in enumerate(posts):
            post_vector = tfidf_matrix[i].toarray()[0]
            top_indices = post_vector.argsort()[-5:][::-1]
            top_keywords = [feature_names[idx] for idx in top_indices if post_vector[idx] > 0]
            themes[post['link']] = top_keywords
            
        return themes
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ TF-IDF –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return {}


def calculate_content_similarity(posts: List[Dict[str, str]]) -> Dict[tuple, float]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏."""
    if len(posts) < 2:
        return {}
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã
    texts = []
    for post in posts:
        title = post.get('title', '')
        content = post.get('content', '')
        combined_text = f"{title} {content}"
        cleaned = clean_text_simple(combined_text)
        texts.append(cleaned)
    
    try:
        # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞
        vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2), 
            min_df=1,
            max_df=0.9
        )
        
        tfidf_matrix = vectorizer.fit_transform(texts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–∏
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ (> 0.1)
        similarities = {}
        for i in range(len(posts)):
            for j in range(i + 1, len(posts)):
                sim_score = similarity_matrix[i][j]
                if sim_score > 0.1:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
                    similarities[(i, j)] = sim_score
                    
        return similarities
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
        return {}


def extract_semantic_clusters(posts: List[Dict[str, str]], themes: Dict[str, List[str]]) -> Dict[str, List[int]]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º."""
    clusters = {}
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
    keyword_to_posts = {}
    
    for i, post in enumerate(posts):
        post_themes = themes.get(post['link'], [])
        for theme in post_themes[:3]:  # –ë–µ—Ä–µ–º —Ç–æ–ø-3 —Ç–µ–º—ã
            if theme not in keyword_to_posts:
                keyword_to_posts[theme] = []
            keyword_to_posts[theme].append(i)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–∑ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è —Ç–µ–º
    for theme, post_indices in keyword_to_posts.items():
        if len(post_indices) >= 2:  # –ú–∏–Ω–∏–º—É–º 2 —Å—Ç–∞—Ç—å–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
            clusters[theme] = post_indices
    
    return clusters


class RAGDatabaseManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG-—Å–∏—Å—Ç–µ–º–æ–π –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î."""
    
    def __init__(self):
        self.collection_name = "articles"
    
    async def create_article_embeddings(self, domain: str, posts: List[Dict]) -> None:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å—Ç–∞—Ç–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î."""
        if not embedding_model or not chroma_client:
            print("‚ùå RAG-—Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return
            
        print(f"üîÆ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(posts)} —Å—Ç–∞—Ç–µ–π...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            collection = chroma_client.get_or_create_collection(
                name=f"{domain}_articles",
                metadata={"description": f"Articles from {domain}"}
            )
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–º–µ–Ω–∞
            existing_ids = collection.get()["ids"]
            if existing_ids:
                collection.delete(ids=existing_ids)
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π
            texts_to_embed = []
            metadatas = []
            ids = []
            
            async with AsyncSessionLocal() as session:
                for i, post in enumerate(posts):
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                    title = post.get('title', '')
                    content = post.get('content', '')[:500]  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
                    combined_text = f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n\n–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content}"
                    
                    texts_to_embed.append(combined_text)
                    metadatas.append({
                        "title": title,
                        "link": post.get('link', ''),
                        "content_snippet": content,
                        "domain": domain,
                        "post_id": i
                    })
                    ids.append(f"{domain}_{i}")
                
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
                embeddings = embedding_model.encode(texts_to_embed).tolist()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ ChromaDB
                collection.add(
                    embeddings=embeddings,
                    metadatas=metadatas,
                    documents=texts_to_embed,
                    ids=ids
                )
                
                # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ PostgreSQL –¥–ª—è –±—ç–∫–∞–ø–∞
                for i, (text, embedding, metadata) in enumerate(zip(texts_to_embed, embeddings, metadatas)):
                    article_embedding = ArticleEmbedding(
                        domain=domain,
                        wp_post_id=i,
                        title=metadata["title"],
                        content_snippet=metadata["content_snippet"],
                        link=metadata["link"],
                        embedding_vector=json.dumps(embedding),
                        themes=[]  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
                    )
                    session.add(article_embedding)
                
                await session.commit()
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(posts)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    def semantic_search(self, domain: str, query: str, n_results: int = 5) -> List[Dict]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å—Ç–∞—Ç—å—è–º."""
        if not embedding_model or not chroma_client:
            return []
        
        try:
            collection = chroma_client.get_collection(name=f"{domain}_articles")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = embedding_model.encode([query]).tolist()
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å—Ç–∞—Ç—å–∏
            results = collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                include=['metadatas', 'documents', 'distances']
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            articles = []
            for i in range(len(results['ids'][0])):
                articles.append({
                    'title': results['metadatas'][0][i]['title'],
                    'link': results['metadatas'][0][i]['link'],
                    'content': results['metadatas'][0][i]['content_snippet'],
                    'similarity': 1 - results['distances'][0][i]  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º distance –≤ similarity
                })
            
            return articles
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def get_all_articles(self, domain: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –¥–æ–º–µ–Ω–∞ –¥–ª—è –æ–±–∑–æ—Ä–∞."""
        if not chroma_client:
            return []
        
        try:
            collection = chroma_client.get_collection(name=f"{domain}_articles")
            results = collection.get(include=['metadatas'])
            
            articles = []
            for metadata in results['metadatas']:
                articles.append({
                    'title': metadata['title'],
                    'link': metadata['link'],
                    'content': metadata['content_snippet']
                })
            
            return articles
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π: {e}")
            return []


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG-–º–µ–Ω–µ–¥–∂–µ—Ä–∞
rag_manager = RAGDatabaseManager()


# –§—É–Ω–∫—Ü–∏–∏-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è Ollama
OLLAMA_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "search_articles",
            "description": "–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É —Å –∑–∞–ø—Ä–æ—Å–æ–º",
            "parameters": {
                "type": "object",
                "properties": {
                    "domain": {
                        "type": "string",
                        "description": "–î–æ–º–µ–Ω —Å–∞–π—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"
                    },
                    "query": {
                        "type": "string", 
                        "description": "–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"
                    },
                    "count": {
                        "type": "integer",
                        "description": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)"
                    }
                },
                "required": ["domain", "query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_all_articles_overview",
            "description": "–ü–æ–ª—É—á–∏—Ç—å –æ–±–∑–æ—Ä –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞",
            "parameters": {
                "type": "object",
                "properties": {
                    "domain": {
                        "type": "string",
                        "description": "–î–æ–º–µ–Ω —Å–∞–π—Ç–∞"
                    }
                },
                "required": ["domain"]
            }
        }
    }
]


async def execute_tool_call(tool_name: str, arguments: Dict) -> str:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ RAG-—Å–∏—Å—Ç–µ–º—ã."""
    try:
        if tool_name == "search_articles":
            domain = arguments.get("domain")
            query = arguments.get("query") 
            count = arguments.get("count", 5)
            
            articles = rag_manager.semantic_search(domain, query, count)
            
            if not articles:
                return f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–∞ –¥–æ–º–µ–Ω–µ {domain}"
            
            result = f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query}':\n\n"
            for i, article in enumerate(articles, 1):
                result += f"{i}. {article['title']}\n"
                result += f"   –°—Å—ã–ª–∫–∞: {article['link']}\n"
                result += f"   –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {article['content'][:100]}...\n"
                result += f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {article['similarity']:.3f}\n\n"
            
            return result
            
        elif tool_name == "get_all_articles_overview":
            domain = arguments.get("domain")
            articles = rag_manager.get_all_articles(domain)
            
            if not articles:
                return f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –Ω–∞ –¥–æ–º–µ–Ω–µ {domain}"
            
            result = f"–û–±–∑–æ—Ä –≤—Å–µ—Ö {len(articles)} —Å—Ç–∞—Ç–µ–π –Ω–∞ –¥–æ–º–µ–Ω–µ {domain}:\n\n"
            for i, article in enumerate(articles, 1):
                result += f"{i}. {article['title']}\n"
                result += f"   {article['content'][:80]}...\n\n"
            
            return result
        
        else:
            return f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {tool_name}"
            
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ {tool_name}: {e}"


async def generate_links(text: str) -> list[str]:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""
    prompt = (
        "–ü—Ä–µ–¥–ª–æ–∂–∏ –¥–æ –ø—è—Ç–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        "–ö–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É –≤—ã–≤–µ–¥–∏ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ /article/–Ω–∞–∑–≤–∞–Ω–∏–µ-—Å—Ç–∞—Ç—å–∏, "
        "–æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–ª–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è. "
        f"–¢–µ–∫—Å—Ç: {text}"
    )
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(
            OLLAMA_URL,
            json={"model": OLLAMA_MODEL, "prompt": prompt, "stream": False},
            timeout=60,
        )
    response.raise_for_status()
    data = response.json()
    lines = [line.strip("- \n") for line in data.get("response", "").splitlines()]
    links = [line for line in lines if line]
    return links[:5]


async def fetch_and_store_wp_posts(domain: str) -> list[dict[str, str]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ —Å WordPress —Å–∞–π—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î."""
    print(f"üåê –ó–∞–≥—Ä—É–∂–∞—é –ø–æ—Å—Ç—ã —Å —Å–∞–π—Ç–∞ {domain}")
    
    # –°–Ω–∞—á–∞–ª–∞ –æ—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ—Å—Ç—ã —ç—Ç–æ–≥–æ –¥–æ–º–µ–Ω–∞
    async with AsyncSessionLocal() as session:
        await session.execute(
            select(WordPressPost).where(WordPressPost.domain == domain)
        )
        await session.commit()
    
    url = f"https://{domain}/wp-json/wp/v2/posts?per_page=100"  # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ 100 —Å—Ç–∞—Ç–µ–π
    async with httpx.AsyncClient(timeout=120.0) as client:
        response = await client.get(url)
    if response.status_code >= 400:
        raise HTTPException(status_code=400, detail="–°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ WordPress")
    data = response.json()
    if not isinstance(data, list):
        raise HTTPException(status_code=400, detail="–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç WordPress")
    
    posts = []
    async with AsyncSessionLocal() as session:
        for item in data:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = item.get("content", {}).get("rendered", "")
                excerpt = item.get("excerpt", {}).get("rendered", "")
                
                # –û—á–∏—â–∞–µ–º HTML
                clean_content = BeautifulSoup(content, 'html.parser').get_text()
                clean_excerpt = BeautifulSoup(excerpt, 'html.parser').get_text() if excerpt else ""
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                wp_post = WordPressPost(
                    domain=domain,
                    wp_post_id=item["id"],
                    title=item["title"]["rendered"],
                    content=clean_content,
                    excerpt=clean_excerpt,
                    link=item["link"]
                )
                session.add(wp_post)
                
                # –î–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –±–µ—Ä–µ–º –∫—Ä–∞—Ç–∫—É—é –≤–µ—Ä—Å–∏—é
                short_content = clean_content[:500] if clean_content else clean_excerpt[:500]
                posts.append({
                    "title": item["title"]["rendered"], 
                    "link": item["link"],
                    "content": short_content.strip()
                })
                
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ø–æ—Å—Ç: {item['title']['rendered']}")
                
            except Exception as exc:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞ {item.get('id', 'unknown')}: {exc}")
                continue
        
        await session.commit()
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")
    
    return posts


async def generate_wp_recommendations_from_db(domain: str, progress_callback=None) -> list[dict[str, str]]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–µ —Å –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è."""
    print(f"üîç –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ –ë–î
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(WordPressPost).where(WordPressPost.domain == domain).order_by(WordPressPost.created_at.desc())
        )
        db_posts = result.scalars().all()
    
    if not db_posts:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ—Å—Ç—ã –≤ –ë–î –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return []
    
    print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(db_posts)} –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    limited_posts = db_posts[:min(len(db_posts), 30)]  # –§–æ–∫—É—Å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ, –∞ –Ω–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ
    print(f"üéØ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(limited_posts)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    posts_for_analysis = []
    for post in limited_posts:
        # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        content_preview = post.content[:1000] if post.content else post.excerpt[:1000] if post.excerpt else ""
        posts_for_analysis.append({
            "title": post.title,
            "link": post.link,
            "content": content_preview.strip(),
            "full_content": post.content  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        })
    
    # –≠—Ç–∞–ø 1: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print("üß† –≠—Ç–∞–ø 1: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    if progress_callback:
        progress_callback("–ê–Ω–∞–ª–∏–∑ —Ç–µ–º –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤", 1, 4)
    
    themes = extract_key_themes(posts_for_analysis)
    similarities = calculate_content_similarity(posts_for_analysis)
    clusters = extract_semantic_clusters(posts_for_analysis, themes)
    
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(clusters)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    print(f"üìä –í—ã—á–∏—Å–ª–µ–Ω–æ {len(similarities)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π")
    
    # –≠—Ç–∞–ø 2: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    print("ü§ñ –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
    if progress_callback:
        progress_callback("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —á–µ—Ä–µ–∑ –ò–ò", 2, 4)
    
    all_recommendations = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–µ–Ω—å—à–∏–º–∏ –ø–æ—Ä—Ü–∏—è–º–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    batch_size = 8  # –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    total_batches = min(len(limited_posts) // batch_size + 1, 5)  # –ú–∞–∫—Å–∏–º—É–º 5 –±–∞—Ç—á–µ–π
    
    for batch_num in range(total_batches):
        batch_start = batch_num * batch_size
        batch_end = min(batch_start + batch_size, len(limited_posts))
        batch_posts = limited_posts[batch_start:batch_end]
        
        if len(batch_posts) < 2:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 —Å—Ç–∞—Ç—å–∏ –¥–ª—è —Å–≤—è–∑–µ–π
            continue
        
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num + 1}/{total_batches}: {len(batch_posts)} —Å—Ç–∞—Ç–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –¥–ª—è –ò–ò
        detailed_posts = []
        for i, post in enumerate(batch_posts):
            global_idx = batch_start + i + 1
            post_themes = themes.get(post.link, [])[:4]  # –¢–æ–ø-4 —Ç–µ–º—ã
            themes_str = ", ".join(post_themes) if post_themes else "–æ–±—â–∏–µ —Ç–µ–º—ã"
            
            # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤)
            content_snippet = post.content[:150].replace('\n', ' ').strip()
            if len(post.content) > 150:
                content_snippet += "..."
            
            detailed_posts.append({
                'index': global_idx,
                'title': post.title,
                'themes': themes_str,
                'content_snippet': content_snippet,
                'link': post.link
            })
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø—Ä–æ–º–ø—Ç —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
        prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ SEO –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–µ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏ –∏ —Å–æ–∑–¥–∞–π –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ï —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å—Å—ã–ª–∫–∞–º.

–°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
"""
        
        for post_data in detailed_posts:
            prompt += f"""
{post_data['index']}. "{post_data['title']}"
–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: {post_data['themes']}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {post_data['content_snippet']}
"""
        
        prompt += f"""

–ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π 5-7 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å—Å—ã–ª–∫–∞–º –º–µ–∂–¥—É —ç—Ç–∏–º–∏ —Å—Ç–∞—Ç—å—è–º–∏.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –ö–ê–ß–ï–°–¢–í–£:
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–£–Æ –°–í–Ø–ó–¨ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏ —Å—Ç–∞—Ç–µ–π
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –õ–û–ì–ò–ß–ù–´–ú–ò –∏ –ü–û–õ–ï–ó–ù–´–ú–ò –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ê–Ω–∫–æ—Ä-—Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ï–°–¢–ï–°–¢–í–ï–ù–ù–´–ú –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –î–ï–¢–ê–õ–¨–ù–û–ï –û–ë–û–°–ù–û–í–ê–ù–ò–ï —Å–≤—è–∑–∏

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π):
N1->N2|–∞–Ω–∫–æ—Ä-—Ç–µ–∫—Å—Ç|–ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ—á–µ–º—É —ç—Ç–∞ —Å—Å—ã–ª–∫–∞ –ø–æ–ª–µ–∑–Ω–∞ –∏ –ª–æ–≥–∏—á–Ω–∞

–ü–†–ò–ú–ï–†–´ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô:
1->3|–∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–±—Ä–∞—Ç—å —Ä–∞–π–æ–Ω –¥–ª—è –ø–æ–∫—É–ø–∫–∏ –∫–≤–∞—Ä—Ç–∏—Ä—ã|–û–±–µ —Å—Ç–∞—Ç—å–∏ –æ—Å–≤–µ—â–∞—é—Ç –≤–æ–ø—Ä–æ—Å—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏: –ø–µ—Ä–≤–∞—è —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–æ–∫—É–ø–∫–∏, –∞ —Ç—Ä–µ—Ç—å—è –¥–µ—Ç–∞–ª—å–Ω–æ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –ª–æ–∫–∞—Ü–∏–∏, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ª–æ–≥–∏—á–Ω—ã–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è, –ø—Ä–∏–Ω–∏–º–∞—é—â–µ–≥–æ —Ä–µ—à–µ–Ω–∏–µ –æ –ø–æ–∫—É–ø–∫–µ
2->4|–º–µ—Ç–æ–¥—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞|–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å —á–µ—Ä–µ–∑ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: –≤—Ç–æ—Ä–∞—è —Å—Ç–∞—Ç—å—è –æ —Å–∞–º–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–µ–¥–µ—Ç –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –º–µ—Ç–æ–¥–∏–∫–∞–º –∏–∑—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è —á–∏—Ç–∞—Ç–µ–ª—é –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–ª–∞–Ω–æ–≤ –ø–æ —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—é

–í–ê–ñ–ù–û: 
- –ù–ï —Å–æ–∑–¥–∞–≤–∞–π –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ —Å–≤—è–∑–∏ —Ç–∏–ø–∞ "—Å—Ö–æ–∂–∏–µ —Ç–µ–º—ã"
- –ö–∞–∂–¥–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ö–û–ù–ö–†–ï–¢–ù–´–ú –∏ –†–ê–ó–í–ï–†–ù–£–¢–´–ú
- –ê–Ω–∫–æ—Ä –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç–∞—Ç—å–∏
- –°—Å—ã–ª–∫–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–∏–Ω–æ—Å–∏—Ç—å –†–ï–ê–õ–¨–ù–£–Æ –ü–û–õ–¨–ó–£ —á–∏—Ç–∞—Ç–µ–ª—é

–ê–ù–ê–õ–ò–ó–ò–†–£–ô –ò –°–û–ó–î–ê–í–ê–ô –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:"""

        try:
            print(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –¥–µ—Ç–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ Ollama (–±–∞—Ç—á {batch_num + 1})...")
            
            async with httpx.AsyncClient(timeout=120.0) as client:  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Ç–∞–π–º–∞—É—Ç
                resp = await client.post(
                    OLLAMA_URL,
                    json={
                        "model": OLLAMA_MODEL, 
                        "prompt": prompt, 
                        "stream": False,
                        "options": {
                            "num_ctx": 4096,        # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                            "temperature": 0.4,     # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é
                            "top_p": 0.7,          # –ë–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
                            "repeat_penalty": 1.1,  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                            "num_predict": 800      # –ë–æ–ª—å—à–µ –º–µ—Å—Ç–∞ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π
                        }
                    },
                    timeout=120,
                )
            
            if resp.status_code != 200:
                print(f"‚ùå Ollama –≤–µ—Ä–Ω—É–ª–∞ –∫–æ–¥ {resp.status_code}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞—Ç—á")
                continue
                
            content = resp.json().get("response", "")
            print(f"ü§ñ –ü–æ–ª—É—á–µ–Ω —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            batch_recommendations = 0
            for line in content.splitlines():
                line = line.strip()
                if not line or "->" not in line or "|" not in line:
                    continue
                    
                try:
                    # –ü–∞—Ä—Å–∏–Ω–≥: N1->N2|–∞–Ω–∫–æ—Ä|–¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
                    parts = line.split("|", 2)
                    if len(parts) < 3:
                        continue
                        
                    link_part = parts[0].strip()
                    anchor = parts[1].strip()
                    detailed_comment = parts[2].strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                    if len(detailed_comment) < 30:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é —Å –∫–æ—Ä–æ—Ç–∫–∏–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º: {line[:50]}...")
                        continue
                    
                    if "->" in link_part:
                        src_part, dst_part = link_part.split("->", 1)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
                        src_match = re.search(r'\d+', src_part)
                        dst_match = re.search(r'\d+', dst_part)
                        
                        if src_match and dst_match:
                            src_idx = int(src_match.group())
                            dst_idx = int(dst_match.group())
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
                            src_global = src_idx - 1
                            dst_global = dst_idx - 1
                            
                            if (0 <= src_global < len(limited_posts) and 
                                0 <= dst_global < len(limited_posts) and 
                                src_global != dst_global):
                                
                                # –£–ª—É—á—à–∞–µ–º –∞–Ω–∫–æ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                                if len(anchor) < 10:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∞–Ω–∫–æ—Ä
                                    anchor = f"–ø–æ–¥—Ä–æ–±–Ω–µ–µ –æ {limited_posts[dst_global].title.split()[0]}"
                                
                                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                                recommendation = {
                                    "from": limited_posts[src_global].link, 
                                    "to": limited_posts[dst_global].link, 
                                    "anchor": anchor,
                                    "comment": detailed_comment
                                }
                                
                                all_recommendations.append(recommendation)
                                batch_recommendations += 1
                                print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è {len(all_recommendations)}: {src_idx}->{dst_idx}")
                                print(f"   –ê–Ω–∫–æ—Ä: {anchor[:50]}...")
                                print(f"   –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {detailed_comment[:100]}...")
                        
                except Exception as parse_error:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {parse_error}")
                    continue
            
            print(f"üìä –ë–∞—Ç—á {batch_num + 1} –∑–∞–≤–µ—Ä—à–µ–Ω: –¥–æ–±–∞–≤–ª–µ–Ω–æ {batch_recommendations} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
            
            # –ü–∞—É–∑–∞ –¥–ª—è —Ä–∞–∑–≥—Ä—É–∑–∫–∏ Ollama
            if batch_num < total_batches - 1:
                print("‚è≥ –ü–∞—É–∑–∞ 4 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
                await asyncio.sleep(4)
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_num + 1}: {e}")
            continue
    
    # –≠—Ç–∞–ø 3: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    print("üîß –≠—Ç–∞–ø 3: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è...")
    if progress_callback:
        progress_callback("–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", 3, 4)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    unique_recommendations = []
    seen_pairs = set()
    
    for rec in all_recommendations:
        pair_key = (rec["from"], rec["to"])
        if pair_key not in seen_pairs:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            if (len(rec["anchor"]) >= 10 and 
                len(rec["comment"]) >= 30 and
                "—Å—Ö–æ–∂–∏–µ —Ç–µ–º—ã" not in rec["comment"].lower() and
                "–æ–±—â–∏–µ —Ç–µ–º—ã" not in rec["comment"].lower()):
                
                unique_recommendations.append(rec)
                seen_pairs.add(pair_key)
    
    # –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
    print("üéØ –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    if progress_callback:
        progress_callback("–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 4, 4)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
    final_recommendations = unique_recommendations[:50] 
    
    print(f"üéâ –°–æ–∑–¥–∞–Ω–æ {len(final_recommendations)} –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    print(f"üìä –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è: {sum(len(r['comment']) for r in final_recommendations) // len(final_recommendations) if final_recommendations else 0} —Å–∏–º–≤–æ–ª–æ–≤")
    
    return final_recommendations


async def generate_wp_recommendations_with_rag(domain: str, progress_callback=None) -> list[dict[str, str]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è RAG-—Å–∏—Å—Ç–µ–º—É —Å –ø—Ä—è–º—ã–º –¥–æ—Å—Ç—É–ø–æ–º Ollama –∫ –ë–î."""
    print(f"ü§ñ –ó–∞–ø—É—Å–∫ RAG-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}")
    
    if progress_callback:
        progress_callback("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã", 1, 6)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ –ë–î
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(WordPressPost).where(WordPressPost.domain == domain).order_by(WordPressPost.created_at.desc())
        )
        db_posts = result.scalars().all()
    
    if not db_posts:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ—Å—Ç—ã –≤ –ë–î –¥–ª—è RAG-–∞–Ω–∞–ª–∏–∑–∞")
        return []
    
    print(f"üìö –ù–∞–π–¥–µ–Ω–æ {len(db_posts)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è RAG-–∞–Ω–∞–ª–∏–∑–∞")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    posts_for_rag = []
    for post in db_posts:
        posts_for_rag.append({
            "title": post.title,
            "link": post.link,
            "content": post.content[:1000]  # –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è RAG
        })
    
    # –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    if progress_callback:
        progress_callback("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤", 2, 6)
    
    await rag_manager.create_article_embeddings(domain, posts_for_rag)
    
    # –≠—Ç–∞–ø 2: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG
    if progress_callback:
        progress_callback("–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ RAG-—Å–∏—Å—Ç–µ–º—É", 3, 6)
    
    print("üß† –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å RAG...")
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Ç–µ—Ä-–ø—Ä–æ–º–ø—Ç –¥–ª—è Ollama —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º
    master_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ SEO –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–µ —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞ {domain}.

–£ —Ç–µ–±—è –µ—Å—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö:
1. search_articles - –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
2. get_all_articles_overview - –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–∞ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π

–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ —Å—Ç–∞—Ç—å–∏ —Å–∞–π—Ç–∞ {domain} –∏ —Å–æ–∑–¥–∞–π –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å—Å—ã–ª–∫–∞–º.

–ü–õ–ê–ù –î–ï–ô–°–¢–í–ò–ô:
1. –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∏ –æ–±–∑–æ—Ä –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞
2. –û–ø—Ä–µ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–∞–π–¥–∏ —Å—Ç–∞—Ç—å–∏, –∫–æ—Ç–æ—Ä—ã–µ –ª–æ–≥–∏—á–Ω–æ —Å–≤—è–∑–∞—Ç—å –º–µ–∂–¥—É —Å–æ–±–æ–π
4. –°–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è–º–∏

–ù–∞—á–Ω–∏ —Å –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–∑–æ—Ä–∞ –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π —Å–∞–π—Ç–∞ {domain}."""

    try:
        if progress_callback:
            progress_callback("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å Ollama", 4, 6)
        
        print("ü§ñ –ó–∞–ø—É—Å–∫ RAG-–¥–∏–∞–ª–æ–≥–∞ —Å Ollama...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama Python –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
        messages = [
            {
                "role": "system",
                "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ SEO —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π."
            },
            {
                "role": "user", 
                "content": master_prompt
            }
        ]
        
        all_recommendations = []
        conversation_steps = 0
        max_steps = 5  # –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–∏–∞–ª–æ–≥–∞
        
        while conversation_steps < max_steps:
            if progress_callback:
                progress_callback(f"RAG-–¥–∏–∞–ª–æ–≥: —à–∞–≥ {conversation_steps + 1}", 4 + conversation_steps, 6)
            
            print(f"üîÑ RAG-–¥–∏–∞–ª–æ–≥: —à–∞–≥ {conversation_steps + 1}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama
            response = ollama.chat(
                model=OLLAMA_MODEL,
                messages=messages,
                tools=OLLAMA_TOOLS if conversation_steps == 0 else None,  # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–º —à–∞–≥–µ
                options={
                    "temperature": 0.4,
                    "num_ctx": 8192,  # –ë–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è RAG
                    "num_predict": 1000
                }
            )
            
            assistant_message = response['message']
            messages.append(assistant_message)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            if 'tool_calls' in assistant_message:
                print(f"üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {len(assistant_message['tool_calls'])}")
                
                for tool_call in assistant_message['tool_calls']:
                    tool_name = tool_call['function']['name']
                    arguments = tool_call['function']['arguments']
                    
                    print(f"üìû –í—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {tool_name} —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏: {arguments}")
                    
                    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
                    tool_result = await execute_tool_call(tool_name, arguments)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –¥–∏–∞–ª–æ–≥
                    messages.append({
                        "role": "tool",
                        "content": tool_result,
                        "tool_call_id": tool_call.get('id', 'tool_1')
                    })
                
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–∏–∞–ª–æ–≥ –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
                if conversation_steps == 0:
                    messages.append({
                        "role": "user",
                        "content": """–û—Ç–ª–∏—á–Ω–æ! –¢–µ–ø–µ—Ä—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç–∞—Ç—å–∏ –∏ —Å–æ–∑–¥–∞–π 15-20 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Å—Å—ã–ª–∫–∞–º.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
–°–°–´–õ–ö–ê_–ò–°–¢–û–ß–ù–ò–ö -> –°–°–´–õ–ö–ê_–¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä-—Ç–µ–∫—Å—Ç | –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –ê–Ω–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
- –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –æ–±—ä—è—Å–Ω—è—Ç—å, –ø–æ—á–µ–º—É —ç—Ç–∞ —Å–≤—è–∑—å –ª–æ–≥–∏—á–Ω–∞ –∏ –ø–æ–ª–µ–∑–Ω–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ú–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏
- –ù–∏–∫–∞–∫–∏—Ö –æ–±—â–∏—Ö —Ñ—Ä–∞–∑ —Ç–∏–ø–∞ "—Å—Ö–æ–∂–∏–µ —Ç–µ–º—ã" –∏–ª–∏ "—Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏\""""
                    })
            
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                content = assistant_message.get('content', '')
                print(f"üìù –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞
                recommendations = parse_rag_recommendations(content, posts_for_rag)
                all_recommendations.extend(recommendations)
                
                if recommendations:
                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏–∑ RAG-–æ—Ç–≤–µ—Ç–∞")
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –º–æ–∂–µ–º –∑–∞–≤–µ—Ä—à–∏—Ç—å
                if len(all_recommendations) >= 10:
                    break
            
            conversation_steps += 1
        
        # –≠—Ç–∞–ø 3: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ RAG-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        if progress_callback:
            progress_callback("–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ RAG-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", 6, 6)
        
        print("üîß –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ RAG-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        unique_recommendations = []
        seen_pairs = set()
        
        for rec in all_recommendations:
            pair_key = (rec["from"], rec["to"])
            if pair_key not in seen_pairs and len(rec["comment"]) >= 30:
                unique_recommendations.append(rec)
                seen_pairs.add(pair_key)
        
        print(f"üéâ RAG-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(unique_recommendations)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
        return unique_recommendations[:30]  # –¢–æ–ø-30 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ RAG-–∞–Ω–∞–ª–∏–∑–∞: {e}")
        # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –∫ –æ–±—ã—á–Ω–æ–º—É –º–µ—Ç–æ–¥—É
        return await generate_wp_recommendations_from_db(domain, progress_callback)


def parse_rag_recommendations(text: str, posts: List[Dict]) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ RAG-–æ—Ç–≤–µ—Ç–∞ Ollama."""
    recommendations = []
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
    title_to_link = {}
    for post in posts:
        title_to_link[post['title'].lower()] = post['link']
    
    lines = text.splitlines()
    for line in lines:
        line = line.strip()
        
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        if '->' in line and '|' in line:
            try:
                # –ü–∞—Ä—Å–∏–º —Ñ–æ—Ä–º–∞—Ç: URL1 -> URL2 | –∞–Ω–∫–æ—Ä | –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
                parts = line.split('|', 2)
                if len(parts) >= 3:
                    link_part = parts[0].strip()
                    anchor = parts[1].strip()
                    comment = parts[2].strip()
                    
                    if '->' in link_part:
                        source_part, target_part = link_part.split('->', 1)
                        source = source_part.strip()
                        target = target_part.strip()
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ–ª–Ω—ã–µ URL –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                        source_url = find_article_url(source, title_to_link, posts)
                        target_url = find_article_url(target, title_to_link, posts)
                        
                        if source_url and target_url and source_url != target_url:
                            recommendations.append({
                                "from": source_url,
                                "to": target_url,
                                "anchor": anchor,
                                "comment": comment
                            })
                            
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RAG-—Å—Ç—Ä–æ–∫–∏ '{line}': {e}")
                continue
    
    return recommendations


def find_article_url(text: str, title_to_link: Dict, posts: List[Dict]) -> Optional[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç URL —Å—Ç–∞—Ç—å–∏ –ø–æ —Ç–µ–∫—Å—Ç—É (–∑–∞–≥–æ–ª–æ–≤–∫—É –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é)."""
    text_lower = text.lower().strip()
    
    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ URL
    if text.startswith('http'):
        return text
    
    # –ü–æ–∏—Å–∫ –ø–æ —Ç–æ—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –∑–∞–≥–æ–ª–æ–≤–∫–∞
    if text_lower in title_to_link:
        return title_to_link[text_lower]
    
    # –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
    for title, link in title_to_link.items():
        if text_lower in title or title in text_lower:
            return link
    
    # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    text_words = text_lower.split()
    for post in posts:
        title_words = post['title'].lower().split()
        if len(set(text_words) & set(title_words)) >= 2:  # –ú–∏–Ω–∏–º—É–º 2 –æ–±—â–∏—Ö —Å–ª–æ–≤–∞
            return post['link']
    
    return None


@app.on_event("startup")
async def on_startup() -> None:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ."""
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG-—Å–∏—Å—Ç–µ–º—É
    initialize_rag_system()


@app.post("/api/v1/test")
async def test(req: RecommendRequest) -> dict[str, str]:
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint."""
    return {"message": f"–ü–æ–ª—É—á–µ–Ω —Ç–µ–∫—Å—Ç: {req.text[:50]}..."}


@app.post("/api/v1/recommend")
async def recommend(req: RecommendRequest) -> dict[str, list[str]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫ –æ—Ç Ollama –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö."""
    links = await generate_links(req.text)

    async with AsyncSessionLocal() as session:
        rec = Recommendation(text=req.text, links=links)
        session.add(rec)
        await session.commit()
    return {"links": links}


@app.post("/api/v1/wp_recommend")
async def wp_recommend(req: WPRequest) -> dict[str, list[dict[str, str]]]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç WordPress-—Å–∞–π—Ç –∏ –≤—ã–¥–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–µ."""
    # –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å—Ç—ã –≤ –ë–î
    posts = await fetch_and_store_wp_posts(req.domain)
    
    # –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
    recs = await generate_wp_recommendations_from_db(req.domain)
    
    # –≠—Ç–∞–ø 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–∞
    summary = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å–∞–π—Ç {req.domain}: –Ω–∞–π–¥–µ–Ω–æ {len(posts)} —Å—Ç–∞—Ç–µ–π, —Å–æ–∑–¥–∞–Ω–æ {len(recs)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
    
    async with AsyncSessionLocal() as session:
        analysis = AnalysisHistory(
            domain=req.domain,
            posts_count=len(posts),
            recommendations_count=len(recs),
            recommendations=recs,
            summary=summary
        )
        session.add(analysis)
        await session.commit()
    
    return {"recommendations": recs}


@app.post("/api/v1/wp_recommend_rag")
async def wp_recommend_rag(req: WPRequest) -> dict[str, list[dict[str, str]]]:
    """RAG-–∞–Ω–∞–ª–∏–∑ WordPress-—Å–∞–π—Ç–∞ —Å –ø—Ä—è–º—ã–º –¥–æ—Å—Ç—É–ø–æ–º Ollama –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î."""
    # –≠—Ç–∞–ø 1: –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å—Ç—ã –≤ –ë–î  
    posts = await fetch_and_store_wp_posts(req.domain)
    
    # –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ RAG-—Å–∏—Å—Ç–µ–º—É
    recs = await generate_wp_recommendations_with_rag(req.domain)
    
    # –≠—Ç–∞–ø 3: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é RAG-–∞–Ω–∞–ª–∏–∑–∞
    summary = f"RAG-–∞–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞ {req.domain}: –Ω–∞–π–¥–µ–Ω–æ {len(posts)} —Å—Ç–∞—Ç–µ–π, —Å–æ–∑–¥–∞–Ω–æ {len(recs)} RAG-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
    
    async with AsyncSessionLocal() as session:
        analysis = AnalysisHistory(
            domain=req.domain,
            posts_count=len(posts),
            recommendations_count=len(recs),
            recommendations=recs,
            summary=summary
        )
        session.add(analysis)
        await session.commit()
    
    return {"recommendations": recs}


@app.get("/api/v1/health")
async def health() -> dict[str, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏."""
    return {"status": "ok"}


@app.get("/api/v1/recommendations")
async def list_recommendations() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Recommendation).order_by(Recommendation.id.desc())
        )
        items = [
            {"id": rec.id, "text": rec.text, "links": rec.links}
            for rec in result.scalars()
        ]
    return items


@app.get("/api/v1/analysis_history")
async def list_analysis_history() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤ WordPress —Å–∞–π—Ç–æ–≤."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(AnalysisHistory).order_by(AnalysisHistory.created_at.desc())
        )
        items = [
            {
                "id": analysis.id,
                "domain": analysis.domain,
                "posts_count": analysis.posts_count,
                "recommendations_count": analysis.recommendations_count,
                "summary": analysis.summary,
                "created_at": analysis.created_at.isoformat(),
                "recommendations": analysis.recommendations
            }
            for analysis in result.scalars()
        ]
    return items


@app.get("/api/v1/analysis_history/{analysis_id}")
async def get_analysis_details(analysis_id: int) -> dict[str, object]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(AnalysisHistory).where(AnalysisHistory.id == analysis_id)
        )
        analysis = result.scalar_one_or_none()
        if not analysis:
            raise HTTPException(status_code=404, detail="–ê–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        return {
            "id": analysis.id,
            "domain": analysis.domain,
            "posts_count": analysis.posts_count,
            "recommendations_count": analysis.recommendations_count,
            "summary": analysis.summary,
            "created_at": analysis.created_at.isoformat(),
            "recommendations": analysis.recommendations
        }


@app.get("/")
async def root():
    """–†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥."""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="http://localhost:3000")


# –£–±–∏—Ä–∞–µ–º StaticFiles mount —Ç–∞–∫ –∫–∞–∫ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ —Ç–µ–ø–µ—Ä—å –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ä—Ç—É
