#!/usr/bin/env python3
"""
üß† –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Docker –∫–µ—à–µ–º

–ê–Ω–∞–ª–æ–≥ Git –¥–ª—è Docker —Å–ª–æ–µ–≤ - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥–µ
–∏ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã–µ —Å–ª–æ–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–µ—à –¥–ª—è –Ω–µ–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π.
"""

import os
import sys
import json
import hashlib
import subprocess
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import docker
from docker.errors import DockerException
import yaml

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class LayerInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ Docker —Å–ª–æ–µ"""
    layer_id: str
    files_hash: str
    dependencies: List[str] = field(default_factory=list)
    build_time: Optional[datetime] = None
    size: Optional[int] = None
    is_valid: bool = True

@dataclass
class BuildContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–±–æ—Ä–∫–∏"""
    service_name: str
    dockerfile_path: str
    context_path: str
    build_args: Dict[str, str] = field(default_factory=dict)
    target: Optional[str] = None
    cache_from: List[str] = field(default_factory=list)
    cache_to: Optional[str] = None

class SmartDockerCache:
    """–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Docker –∫–µ—à–µ–º"""
    
    def __init__(self, cache_dir: str = ".docker_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.layer_cache: Dict[str, LayerInfo] = {}
        self.docker_client = docker.from_env()
        self.load_cache_metadata()
    
    def load_cache_metadata(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–µ—à–∞"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r') as f:
                    data = json.load(f)
                    self.layer_cache = {
                        layer_id: LayerInfo(**layer_data)
                        for layer_id, layer_data in data.items()
                    }
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–µ—à–∞: {e}")
    
    def save_cache_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–µ—à–∞"""
        try:
            data = {
                layer_id: {
                    'layer_id': layer.layer_id,
                    'files_hash': layer.files_hash,
                    'dependencies': layer.dependencies,
                    'build_time': layer.build_time.isoformat() if layer.build_time else None,
                    'size': layer.size,
                    'is_valid': layer.is_valid
                }
                for layer_id, layer in self.layer_cache.items()
            }
            with open(self.metadata_file, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–µ—à–∞: {e}")
    
    def calculate_files_hash(self, context_path: str, dockerfile_path: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–±–æ—Ä–∫–∏"""
        context_path = Path(context_path)
        dockerfile_path = Path(dockerfile_path)
        
        # –ß–∏—Ç–∞–µ–º Dockerfile –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        dockerfile_content = dockerfile_path.read_text()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º COPY –∏ ADD –∫–æ–º–∞–Ω–¥—ã
        files_to_hash = set()
        
        for line in dockerfile_content.split('\n'):
            line = line.strip()
            if line.startswith(('COPY', 'ADD')):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ –∫–æ–º–∞–Ω–¥—ã
                parts = line.split()
                if len(parts) >= 3:
                    source = parts[1]
                    if not source.startswith('--'):
                        files_to_hash.add(source)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º Dockerfile
        files_to_hash.add(str(dockerfile_path.relative_to(context_path)))
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        hasher = hashlib.sha256()
        
        for file_pattern in files_to_hash:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º wildcards
            if '*' in file_pattern or '?' in file_pattern:
                import glob
                matching_files = glob.glob(str(context_path / file_pattern), recursive=True)
                for file_path in matching_files:
                    if Path(file_path).is_file():
                        self._add_file_to_hash(hasher, file_path)
            else:
                file_path = context_path / file_pattern
                if file_path.is_file():
                    self._add_file_to_hash(hasher, file_path)
                elif file_path.is_dir():
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    for subfile in file_path.rglob('*'):
                        if subfile.is_file():
                            self._add_file_to_hash(hasher, str(subfile))
        
        return hasher.hexdigest()
    
    def _add_file_to_hash(self, hasher, file_path: str):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∫ —Ö–µ—à—É"""
        try:
            with open(file_path, 'rb') as f:
                hasher.update(f.read())
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")
    
    def get_layer_dependencies(self, dockerfile_path: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–ª–æ—è –∏–∑ Dockerfile"""
        dockerfile_content = Path(dockerfile_path).read_text()
        dependencies = []
        
        for line in dockerfile_content.split('\n'):
            line = line.strip()
            if line.startswith('FROM'):
                parts = line.split()
                if len(parts) >= 2:
                    base_image = parts[1]
                    dependencies.append(base_image)
        
        return dependencies
    
    def is_layer_valid(self, layer_id: str, context_path: str, dockerfile_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Å–ª–æ—è"""
        if layer_id not in self.layer_cache:
            return False
        
        layer = self.layer_cache[layer_id]
        if not layer.is_valid:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–æ–≤
        current_hash = self.calculate_files_hash(context_path, dockerfile_path)
        if layer.files_hash != current_hash:
            logger.info(f"–•–µ—à —Ñ–∞–π–ª–æ–≤ –∏–∑–º–µ–Ω–∏–ª—Å—è –¥–ª—è —Å–ª–æ—è {layer_id}")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        current_deps = self.get_layer_dependencies(dockerfile_path)
        if layer.dependencies != current_deps:
            logger.info(f"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –¥–ª—è —Å–ª–æ—è {layer_id}")
            return False
        
        return True
    
    def invalidate_layer(self, layer_id: str):
        """–ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–ª–æ—è"""
        if layer_id in self.layer_cache:
            self.layer_cache[layer_id].is_valid = False
            logger.info(f"–°–ª–æ–π {layer_id} –∏–Ω–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω")
    
    def invalidate_dependent_layers(self, layer_id: str):
        """–ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º—ã—Ö —Å–ª–æ–µ–≤"""
        invalidated = set()
        to_invalidate = {layer_id}
        
        while to_invalidate:
            current = to_invalidate.pop()
            if current in invalidated:
                continue
            
            invalidated.add(current)
            self.invalidate_layer(current)
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–ª–æ–∏, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ
            for layer_id, layer in self.layer_cache.items():
                if current in layer.dependencies and layer_id not in invalidated:
                    to_invalidate.add(layer_id)
    
    def build_with_smart_cache(self, context: BuildContext, force_rebuild: bool = False) -> bool:
        """–£–º–Ω–∞—è —Å–±–æ—Ä–∫–∞ —Å –∫–µ—à–µ–º"""
        logger.info(f"üî® –£–º–Ω–∞—è —Å–±–æ—Ä–∫–∞ {context.service_name}")
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–æ–≤
        files_hash = self.calculate_files_hash(context.context_path, context.dockerfile_path)
        
        # –°–æ–∑–¥–∞–µ–º ID —Å–ª–æ—è
        layer_id = f"{context.service_name}_{files_hash[:8]}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫–µ—à–∞
        if not force_rebuild and self.is_layer_valid(layer_id, context.context_path, context.dockerfile_path):
            logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à –¥–ª—è {context.service_name}")
            return True
        
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º—ã–µ —Å–ª–æ–∏
        if layer_id in self.layer_cache:
            self.invalidate_dependent_layers(layer_id)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–±–æ—Ä–∫—É
        logger.info(f"üèóÔ∏è –ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ {context.service_name}")
        
        try:
            build_args = {
                'DOCKER_BUILDKIT': '1',
                'COMPOSE_DOCKER_CLI_BUILD': '1'
            }
            build_args.update(context.build_args)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–º–∞–Ω–¥—É —Å–±–æ—Ä–∫–∏
            cmd = [
                'docker', 'build',
                '-f', context.dockerfile_path,
                '--build-arg', f'DOCKER_BUILDKIT=1'
            ]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–µ—à
            if context.cache_from:
                for cache_from in context.cache_from:
                    cmd.extend(['--cache-from', cache_from])
            
            if context.cache_to:
                cmd.extend(['--cache-to', context.cache_to])
            
            # –î–æ–±–∞–≤–ª—è–µ–º target –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if context.target:
                cmd.extend(['--target', context.target])
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥
            cmd.extend(['-t', f"{context.service_name}:latest"])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            cmd.append(context.context_path)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–±–æ—Ä–∫—É
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–µ—à–∞
                self.layer_cache[layer_id] = LayerInfo(
                    layer_id=layer_id,
                    files_hash=files_hash,
                    dependencies=self.get_layer_dependencies(context.dockerfile_path),
                    build_time=datetime.now(),
                    is_valid=True
                )
                self.save_cache_metadata()
                
                logger.info(f"‚úÖ –°–±–æ—Ä–∫–∞ {context.service_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏ {context.service_name}: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏ {context.service_name}: {e}")
            return False
    
    def clean_invalid_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ –∫–µ—à–∞"""
        invalid_layers = [
            layer_id for layer_id, layer in self.layer_cache.items()
            if not layer.is_valid
        ]
        
        for layer_id in invalid_layers:
            del self.layer_cache[layer_id]
        
        if invalid_layers:
            self.save_cache_metadata()
            logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {len(invalid_layers)} –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Å–ª–æ–µ–≤")
    
    def get_cache_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∞"""
        total_layers = len(self.layer_cache)
        valid_layers = sum(1 for layer in self.layer_cache.values() if layer.is_valid)
        total_size = sum(layer.size or 0 for layer in self.layer_cache.values())
        
        return {
            'total_layers': total_layers,
            'valid_layers': valid_layers,
            'invalid_layers': total_layers - valid_layers,
            'total_size_mb': total_size / (1024 * 1024) if total_size else 0,
            'cache_dir': str(self.cache_dir)
        }

def parse_docker_compose(compose_file: str) -> List[BuildContext]:
    """–ü–∞—Ä—Å–∏–Ω–≥ docker-compose.yml –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å–±–æ—Ä–∫–∏"""
    with open(compose_file, 'r') as f:
        compose_data = yaml.safe_load(f)
    
    contexts = []
    
    for service_name, service_config in compose_data.get('services', {}).items():
        if 'build' in service_config:
            build_config = service_config['build']
            
            if isinstance(build_config, dict):
                context_path = build_config.get('context', '.')
                dockerfile = build_config.get('dockerfile', 'Dockerfile')
                dockerfile_path = os.path.join(context_path, dockerfile)
                target = build_config.get('target')
                
                contexts.append(BuildContext(
                    service_name=service_name,
                    dockerfile_path=dockerfile_path,
                    context_path=context_path,
                    target=target
                ))
    
    return contexts

def main():
    parser = argparse.ArgumentParser(description='–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Docker –∫–µ—à–µ–º')
    parser.add_argument('--compose-file', default='docker-compose.yml', help='–ü—É—Ç—å –∫ docker-compose.yml')
    parser.add_argument('--service', help='–°–±–æ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞')
    parser.add_argument('--force', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞')
    parser.add_argument('--clean', action='store_true', help='–û—á–∏—Å—Ç–∫–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ –∫–µ—à–∞')
    parser.add_argument('--stats', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–µ—à–∞')
    parser.add_argument('--cache-dir', default='.docker_cache', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞')
    
    args = parser.parse_args()
    
    cache_manager = SmartDockerCache(args.cache_dir)
    
    if args.stats:
        stats = cache_manager.get_cache_stats()
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞:")
        print(f"   –í—Å–µ–≥–æ —Å–ª–æ–µ–≤: {stats['total_layers']}")
        print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö: {stats['valid_layers']}")
        print(f"   –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö: {stats['invalid_layers']}")
        print(f"   –†–∞–∑–º–µ—Ä: {stats['total_size_mb']:.2f} MB")
        print(f"   –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {stats['cache_dir']}")
        return
    
    if args.clean:
        cache_manager.clean_invalid_cache()
        return
    
    # –ü–∞—Ä—Å–∏–º docker-compose.yml
    if not os.path.exists(args.compose_file):
        logger.error(f"–§–∞–π–ª {args.compose_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    contexts = parse_docker_compose(args.compose_file)
    
    if args.service:
        contexts = [ctx for ctx in contexts if ctx.service_name == args.service]
        if not contexts:
            logger.error(f"–°–µ—Ä–≤–∏—Å {args.service} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–±–æ—Ä–∫—É
    success_count = 0
    for context in contexts:
        if cache_manager.build_with_smart_cache(context, args.force):
            success_count += 1
    
    logger.info(f"‚úÖ –°–±–æ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {success_count}/{len(contexts)} —Å–µ—Ä–≤–∏—Å–æ–≤")

if __name__ == '__main__':
    main() 