"""
üß† –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è LLM Tuning –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
"""

import asyncio
import aiohttp
import json
import hashlib
import time
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
from functools import wraps, lru_cache
from dataclasses import dataclass
from datetime import datetime, timedelta
import redis.asyncio as redis
from pydantic import BaseModel, validator
import numpy as np
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

logger = logging.getLogger(__name__)


@dataclass
class OllamaConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
    base_url: str = "http://localhost:11434"
    timeout: int = 300
    max_retries: int = 3
    retry_delay: float = 1.0
    
    # Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    metal_enabled: bool = True
    flash_attention: bool = True
    kv_cache_type: str = "q8_0"
    context_length: int = 4096
    batch_size: int = 512
    num_parallel: int = 2
    mem_fraction: float = 0.9


class OllamaClient:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama API"""
    
    def __init__(self, config: OllamaConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self._models_cache: Dict[str, Dict] = {}
        self._cache_ttl = 300  # 5 –º–∏–Ω—É—Ç
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        self.session = aiohttp.ClientSession(
            base_url=self.config.base_url,
            timeout=timeout,
            headers={"Content-Type": "application/json"}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()
    
    async def _make_request(self, method: str, endpoint: str, data: Dict = None) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(self.config.max_retries):
            try:
                async with self.session.request(method, endpoint, json=data) as response:
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 404:
                        raise ValueError(f"Endpoint not found: {endpoint}")
                    else:
                        error_text = await response.text()
                        raise Exception(f"HTTP {response.status}: {error_text}")
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    raise
                logger.warning(f"Attempt {attempt + 1} failed: {e}")
                await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
    
    async def list_models(self) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        cache_key = f"models_list_{int(time.time() // self._cache_ttl)}"
        
        if cache_key in self._models_cache:
            return self._models_cache[cache_key]
        
        try:
            result = await self._make_request("GET", "/api/tags")
            models = result.get("models", [])
            self._models_cache[cache_key] = models
            return models
        except Exception as e:
            logger.error(f"Error listing models: {e}")
            return []
    
    async def generate(self, model: str, prompt: str, **kwargs) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏"""
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            **kwargs
        }
        
        # Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.config.metal_enabled:
            data["options"] = {
                "num_gpu": 1,
                "num_thread": self.config.num_parallel,
                "num_ctx": self.config.context_length,
                "batch_size": self.config.batch_size,
                "rope_freq_base": 10000,
                "rope_freq_scale": 0.5
            }
        
        return await self._make_request("POST", "/api/generate", data)
    
    async def chat(self, model: str, messages: List[Dict], **kwargs) -> Dict:
        """–ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é"""
        data = {
            "model": model,
            "messages": messages,
            "stream": False,
            **kwargs
        }
        
        # Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if self.config.metal_enabled:
            data["options"] = {
                "num_gpu": 1,
                "num_thread": self.config.num_parallel,
                "num_ctx": self.config.context_length,
                "batch_size": self.config.batch_size
            }
        
        return await self._make_request("POST", "/api/chat", data)
    
    async def create_model(self, name: str, modelfile: str) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        data = {
            "name": name,
            "modelfile": modelfile
        }
        return await self._make_request("POST", "/api/create", data)
    
    async def pull_model(self, name: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–µ—Å—Ç—Ä–∞"""
        data = {"name": name}
        return await self._make_request("POST", "/api/pull", data)
    
    async def delete_model(self, name: str) -> Dict:
        """–£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        data = {"name": name}
        return await self._make_request("DELETE", "/api/delete", data)
    
    async def get_model_info(self, name: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return await self._make_request("GET", f"/api/show", {"name": name})


class CacheManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å Redis"""
    
    def __init__(self, redis_url: str, ttl: int = 3600):
        self.redis_url = redis_url
        self.ttl = ttl
        self.redis: Optional[redis.Redis] = None
    
    async def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis"""
        if not self.redis:
            self.redis = redis.from_url(self.redis_url, decode_responses=True)
    
    async def disconnect(self):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç Redis"""
        if self.redis:
            await self.redis.close()
    
    def _generate_key(self, prefix: str, *args) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        key_parts = [prefix] + [str(arg) for arg in args]
        return ":".join(key_parts)
    
    async def get(self, prefix: str, *args) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞"""
        await self.connect()
        key = self._generate_key(prefix, *args)
        try:
            value = await self.redis.get(key)
            return json.loads(value) if value else None
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None
    
    async def set(self, prefix: str, value: Any, ttl: int = None, *args):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫—ç—à"""
        await self.connect()
        key = self._generate_key(prefix, *args)
        try:
            await self.redis.setex(
                key, 
                ttl or self.ttl, 
                json.dumps(value, default=str)
            )
        except Exception as e:
            logger.error(f"Cache set error: {e}")
    
    async def delete(self, prefix: str, *args):
        """–£–¥–∞–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞"""
        await self.connect()
        key = self._generate_key(prefix, *args)
        try:
            await self.redis.delete(key)
        except Exception as e:
            logger.error(f"Cache delete error: {e}")
    
    async def clear_pattern(self, pattern: str):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        await self.connect()
        try:
            keys = await self.redis.keys(pattern)
            if keys:
                await self.redis.delete(*keys)
        except Exception as e:
            logger.error(f"Cache clear pattern error: {e}")


class SecurityUtils:
    """–£—Ç–∏–ª–∏—Ç—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
    
    @staticmethod
    def hash_password(password: str) -> str:
        """–•–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–æ–ª—è"""
        return hashlib.sha256(password.encode()).hexdigest()
    
    @staticmethod
    def verify_password(password: str, hashed: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–æ–ª—è"""
        return SecurityUtils.hash_password(password) == hashed
    
    @staticmethod
    def generate_api_key() -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è API –∫–ª—é—á–∞"""
        import secrets
        return secrets.token_urlsafe(32)
    
    @staticmethod
    def sanitize_input(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        import re
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        sanitized = re.sub(r'[<>"\']', '', text)
        return sanitized.strip()
    
    @staticmethod
    def validate_rate_limit(user_id: str, limit: int, window: int) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è rate limit"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis
        return True


class ValidationUtils:
    """–£—Ç–∏–ª–∏—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    @staticmethod
    def validate_model_name(name: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏"""
        import re
        pattern = r'^[a-zA-Z0-9_-]+$'
        return bool(re.match(pattern, name)) and len(name) <= 100
    
    @staticmethod
    def validate_prompt(prompt: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞"""
        return len(prompt.strip()) > 0 and len(prompt) <= 10000
    
    @staticmethod
    def validate_temperature(temp: float) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã"""
        return 0.0 <= temp <= 2.0
    
    @staticmethod
    def validate_max_tokens(tokens: int) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        return 1 <= tokens <= 8192
    
    @staticmethod
    def validate_context_length(length: int) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        return 512 <= length <= 32768


class MetricsCollector:
    """–°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.metrics: Dict[str, List[float]] = {
            "response_times": [],
            "token_counts": [],
            "error_rates": [],
            "cache_hits": [],
            "cache_misses": []
        }
    
    def record_response_time(self, time_ms: float):
        """–ó–∞–ø–∏—Å—å –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞"""
        self.metrics["response_times"].append(time_ms)
        if len(self.metrics["response_times"]) > 1000:
            self.metrics["response_times"] = self.metrics["response_times"][-1000:]
    
    def record_token_count(self, count: int):
        """–ó–∞–ø–∏—Å—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        self.metrics["token_counts"].append(count)
        if len(self.metrics["token_counts"]) > 1000:
            self.metrics["token_counts"] = self.metrics["token_counts"][-1000:]
    
    def record_error(self):
        """–ó–∞–ø–∏—Å—å –æ—à–∏–±–∫–∏"""
        self.metrics["error_rates"].append(1.0)
        if len(self.metrics["error_rates"]) > 1000:
            self.metrics["error_rates"] = self.metrics["error_rates"][-1000:]
    
    def record_success(self):
        """–ó–∞–ø–∏—Å—å —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        self.metrics["error_rates"].append(0.0)
        if len(self.metrics["error_rates"]) > 1000:
            self.metrics["error_rates"] = self.metrics["error_rates"][-1000:]
    
    def record_cache_hit(self):
        """–ó–∞–ø–∏—Å—å –ø–æ–ø–∞–¥–∞–Ω–∏—è –≤ –∫—ç—à"""
        self.metrics["cache_hits"].append(1.0)
        if len(self.metrics["cache_hits"]) > 1000:
            self.metrics["cache_hits"] = self.metrics["cache_hits"][-1000:]
    
    def record_cache_miss(self):
        """–ó–∞–ø–∏—Å—å –ø—Ä–æ–º–∞—Ö–∞ –∫—ç—à–∞"""
        self.metrics["cache_misses"].append(1.0)
        if len(self.metrics["cache_misses"]) > 1000:
            self.metrics["cache_misses"] = self.metrics["cache_misses"][-1000:]
    
    def get_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –º–µ—Ç—Ä–∏–∫"""
        summary = {}
        
        for metric_name, values in self.metrics.items():
            if values:
                summary[metric_name] = {
                    "count": len(values),
                    "mean": np.mean(values),
                    "median": np.median(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values)
                }
            else:
                summary[metric_name] = {
                    "count": 0,
                    "mean": 0,
                    "median": 0,
                    "std": 0,
                    "min": 0,
                    "max": 0
                }
        
        return summary
    
    def reset(self):
        """–°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫"""
        for key in self.metrics:
            self.metrics[key] = []


class EmbeddingManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è RAG"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model: Optional[SentenceTransformer] = None
        self.chroma_client: Optional[chromadb.Client] = None
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            self.model = SentenceTransformer(self.model_name)
            self.chroma_client = chromadb.Client(Settings(
                chroma_api_impl="rest",
                chroma_server_host="localhost",
                chroma_server_http_port=8000
            ))
            logger.info(f"Embedding model initialized: {self.model_name}")
        except Exception as e:
            logger.error(f"Failed to initialize embedding model: {e}")
            raise
    
    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤"""
        if not self.model:
            raise ValueError("Embedding model not initialized")
        
        try:
            embeddings = self.model.encode(texts, convert_to_tensor=False)
            return embeddings.tolist()
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            raise
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        try:
            vec1_array = np.array(vec1)
            vec2_array = np.array(vec2)
            
            dot_product = np.dot(vec1_array, vec2_array)
            norm1 = np.linalg.norm(vec1_array)
            norm2 = np.linalg.norm(vec2_array)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return dot_product / (norm1 * norm2)
        except Exception as e:
            logger.error(f"Error computing cosine similarity: {e}")
            return 0.0
    
    async def add_documents(self, collection_name: str, documents: List[str], 
                          metadatas: List[Dict] = None, ids: List[str] = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        if not self.chroma_client:
            raise ValueError("ChromaDB client not initialized")
        
        try:
            collection = self.chroma_client.get_or_create_collection(collection_name)
            
            if not ids:
                ids = [f"doc_{i}" for i in range(len(documents))]
            
            if not metadatas:
                metadatas = [{"source": "unknown"} for _ in documents]
            
            embeddings = self.generate_embeddings(documents)
            
            collection.add(
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"Added {len(documents)} documents to collection {collection_name}")
        except Exception as e:
            logger.error(f"Error adding documents: {e}")
            raise
    
    async def search_similar(self, collection_name: str, query: str, 
                           n_results: int = 5, threshold: float = 0.7) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not self.chroma_client:
            raise ValueError("ChromaDB client not initialized")
        
        try:
            collection = self.chroma_client.get_collection(collection_name)
            query_embedding = self.generate_embeddings([query])[0]
            
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞
            filtered_results = []
            for i, distance in enumerate(results['distances'][0]):
                similarity = 1 - distance  # ChromaDB –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
                if similarity >= threshold:
                    filtered_results.append({
                        'document': results['documents'][0][i],
                        'metadata': results['metadatas'][0][i],
                        'similarity': similarity,
                        'id': results['ids'][0][i]
                    })
            
            return filtered_results
        except Exception as e:
            logger.error(f"Error searching similar documents: {e}")
            return []


# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
def cache_result(prefix: str, ttl: int = 3600):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞
            cache_key = f"{prefix}:{hash(str(args) + str(sorted(kwargs.items())))}"
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
            cache_manager = CacheManager("redis://localhost:6379")
            cached_result = await cache_manager.get(prefix, cache_key)
            
            if cached_result is not None:
                logger.info(f"Cache hit for {func.__name__}")
                return cached_result
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
            result = await func(*args, **kwargs)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            await cache_manager.set(prefix, result, ttl, cache_key)
            logger.info(f"Cache miss for {func.__name__}, stored result")
            
            return result
        return wrapper
    return decorator


def monitor_performance(metrics_collector: MetricsCollector):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                metrics_collector.record_success()
                return result
            except Exception as e:
                metrics_collector.record_error()
                raise
            finally:
                end_time = time.time()
                response_time = (end_time - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
                metrics_collector.record_response_time(response_time)
        
        return wrapper
    return decorator


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã
ollama_client = None
cache_manager = None
metrics_collector = MetricsCollector()
embedding_manager = None


async def initialize_utils(ollama_config: OllamaConfig, redis_url: str):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —É—Ç–∏–ª–∏—Ç"""
    global ollama_client, cache_manager, embedding_manager
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞
    ollama_client = OllamaClient(ollama_config)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    cache_manager = CacheManager(redis_url)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embedding_manager = EmbeddingManager()
    await embedding_manager.initialize()
    
    logger.info("Utils initialized successfully")


async def cleanup_utils():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ —É—Ç–∏–ª–∏—Ç"""
    global ollama_client, cache_manager
    
    if cache_manager:
        await cache_manager.disconnect()
    
    logger.info("Utils cleaned up successfully") 