"""
üß† RAG (Retrieval-Augmented Generation) —Å–µ—Ä–≤–∏—Å
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ LLM
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import json
import re
from dataclasses import dataclass

from .utils import EmbeddingManager, OllamaClient, CacheManager
from .models import RAGDocument, RAGQuery, RAGResponse
from .config import settings

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –≤ RAG"""
    document: str
    doc_metadata: Dict[str, Any]
    similarity: float
    source: str
    chunk_id: str


class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG (Retrieval-Augmented Generation)"""
    
    def __init__(self):
        self.embedding_manager: Optional[EmbeddingManager] = None
        self.ollama_client: Optional[OllamaClient] = None
        self.cache_manager: Optional[CacheManager] = None
        self.collection_name: str = "relink_documents"
        self.chunk_size: int = 1000
        self.chunk_overlap: int = 200
        self.max_context_length: int = 4000
        self.top_k_results: int = 5
        self.similarity_threshold: float = 0.7
    
    async def initialize(self, embedding_manager: EmbeddingManager, 
                        ollama_client: OllamaClient, cache_manager: CacheManager):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–µ—Ä–≤–∏—Å–∞"""
        self.embedding_manager = embedding_manager
        self.ollama_client = ollama_client
        self.cache_manager = cache_manager
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ ChromaDB
        try:
            if self.embedding_manager.chroma_client:
                self.embedding_manager.chroma_client.get_or_create_collection(
                    self.collection_name,
                    metadata={"description": "reLink documents for RAG"}
                )
                logger.info(f"RAG collection '{self.collection_name}' initialized")
        except Exception as e:
            logger.error(f"Failed to initialize RAG collection: {e}")
            raise
    
    def chunk_text(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –∏—â–µ–º —Ö–æ—Ä–æ—à—É—é —Ç–æ—á–∫—É —Ä–∞–∑—Ä—ã–≤–∞
            if end < len(text):
                # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –∞–±–∑–∞—Ü–∞
                for i in range(end, max(start + self.chunk_size - 100, start), -1):
                    if text[i] in '.!?\n':
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - self.chunk_overlap
            if start >= len(text):
                break
        
        return chunks
    
    def extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
        words = re.findall(r'\b\w+\b', text.lower())
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
            'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those'
        }
        
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∏ –≤—ã–±–æ—Ä —Ç–æ–ø-10
        from collections import Counter
        keyword_counts = Counter(keywords)
        return [word for word, count in keyword_counts.most_common(10)]
    
    async def add_document(self, title: str, content: str, source: str = None,
                          document_type: str = None, tags: List[str] = None) -> str:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG —Å–∏—Å—Ç–µ–º—É"""
        try:
            # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.chunk_text(content)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keywords = self.extract_keywords(content)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            doc_metadata_list = []
            for i, chunk in enumerate(chunks):
                doc_metadata = {
                    "title": title,
                    "source": source or "unknown",
                    "document_type": document_type or "text",
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "keywords": keywords,
                    "tags": tags or [],
                    "created_at": datetime.utcnow().isoformat()
                }
                doc_metadata_list.append(doc_metadata)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID –¥–ª—è —á–∞–Ω–∫–æ–≤
            chunk_ids = [f"{title}_{i}_{hash(chunk) % 10000}" for i, chunk in enumerate(chunks)]
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            await self.embedding_manager.add_documents(
                self.collection_name,
                chunks,
                doc_metadata_list,
                chunk_ids
            )
            
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if self.cache_manager:
                doc_key = f"doc:{hash(title + content) % 10000}"
                await self.cache_manager.set(
                    "documents",
                    {
                        "title": title,
                        "content": content,
                        "source": source,
                        "document_type": document_type,
                        "tags": tags,
                        "chunks_count": len(chunks),
                        "keywords": keywords
                    },
                    3600,  # 1 —á–∞—Å
                    doc_key
                )
            
            logger.info(f"Added document '{title}' with {len(chunks)} chunks")
            return f"Document added with {len(chunks)} chunks"
            
        except Exception as e:
            logger.error(f"Error adding document: {e}")
            raise
    
    async def search_documents(self, query: str, top_k: int = None) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            top_k = top_k or self.top_k_results
            
            # –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
            results = await self.embedding_manager.search_similar(
                self.collection_name,
                query,
                top_k,
                self.similarity_threshold
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ SearchResult
            search_results = []
            for result in results:
                search_result = SearchResult(
                    document=result['document'],
                    doc_metadata=result['metadata'],
                    similarity=result['similarity'],
                    source=result['metadata'].get('source', 'unknown'),
                    chunk_id=result['id']
                )
                search_results.append(search_result)
            
            return search_results
            
        except Exception as e:
            logger.error(f"Error searching documents: {e}")
            return []
    
    def build_context(self, search_results: List[SearchResult], max_length: int = None) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        max_length = max_length or self.max_context_length
        
        context_parts = []
        current_length = 0
        
        for result in search_results:
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_text = f"[Source: {result.source}, Similarity: {result.similarity:.3f}]"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞
            chunk_text = f"{metadata_text}\n{result.document}\n\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏ –ª–∏–º–∏—Ç
            if current_length + len(chunk_text) > max_length:
                break
            
            context_parts.append(chunk_text)
            current_length += len(chunk_text)
        
        return "".join(context_parts).strip()
    
    async def generate_response(self, query: str, context: str, 
                              model: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            model = model or settings.ollama.default_model
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            system_prompt = settings.rag.system_prompt
            query_template = settings.rag.query_template
            
            prompt = query_template.format(
                context=context,
                question=query
            )
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama
            response = await self.ollama_client.generate(
                model=model,
                prompt=prompt,
                temperature=0.7,
                max_tokens=1000
            )
            
            return response.get('response', 'No response generated')
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"Error generating response: {str(e)}"
    
    async def query(self, query: str, model: str = None, 
                   top_k: int = None, include_sources: bool = True) -> RAGResponse:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è RAG –∑–∞–ø—Ä–æ—Å–∞"""
        start_time = asyncio.get_event_loop().time()
        
        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            search_results = await self.search_documents(query, top_k)
            
            if not search_results:
                return RAGResponse(
                    query=query,
                    answer="No relevant documents found for your query.",
                    sources=[],
                    context="",
                    processing_time=asyncio.get_event_loop().time() - start_time
                )
            
            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = self.build_context(search_results)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            answer = await self.generate_response(query, context, model)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources = []
            if include_sources:
                for result in search_results:
                    source = {
                        "title": result.doc_metadata.get("title", "Unknown"),
                        "source": result.source,
                        "similarity": result.similarity,
                        "chunk_id": result.chunk_id,
                        "document_type": result.doc_metadata.get("document_type", "text")
                    }
                    sources.append(source)
            
            processing_time = asyncio.get_event_loop().time() - start_time
            
            return RAGResponse(
                query=query,
                answer=answer,
                sources=sources,
                context=context,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Error in RAG query: {e}")
            return RAGResponse(
                query=query,
                answer=f"Error processing query: {str(e)}",
                sources=[],
                context="",
                processing_time=asyncio.get_event_loop().time() - start_time
            )
    
    async def batch_query(self, queries: List[str], model: str = None) -> List[RAGResponse]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        tasks = [self.query(query, model) for query in queries]
        return await asyncio.gather(*tasks, return_exceptions=True)
    
    async def get_document_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            if not self.embedding_manager.chroma_client:
                return {"error": "ChromaDB not initialized"}
            
            collection = self.embedding_manager.chroma_client.get_collection(self.collection_name)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            results = collection.get()
            
            if not results['documents']:
                return {
                    "total_documents": 0,
                    "total_chunks": 0,
                    "sources": [],
                    "document_types": []
                }
            
            # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            sources = set()
            document_types = set()
            titles = set()
            
            # ChromaDB –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –ø–æ–ª–µ 'metadatas'
            if 'metadatas' in results and results['metadatas']:
                for metadata_list in results['metadatas']:
                    for metadata in metadata_list:
                        if metadata:
                            sources.add(metadata.get('source', 'unknown'))
                            document_types.add(metadata.get('document_type', 'text'))
                            titles.add(metadata.get('title', 'unknown'))
            
            return {
                "total_documents": len(titles),
                "total_chunks": len(results['documents']),
                "sources": list(sources),
                "document_types": list(document_types),
                "titles": list(titles)[:10]  # –ü–µ—Ä–≤—ã–µ 10 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            }
            
        except Exception as e:
            logger.error(f"Error getting document stats: {e}")
            return {"error": str(e)}
    
    async def delete_document(self, title: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ RAG —Å–∏—Å—Ç–µ–º—ã"""
        try:
            if not self.embedding_manager.chroma_client:
                return False
            
            collection = self.embedding_manager.chroma_client.get_collection(self.collection_name)
            
            # –ü–æ–∏—Å–∫ —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            results = collection.get(
                where={"title": title}
            )
            
            if results['ids']:
                # –£–¥–∞–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
                collection.delete(ids=results['ids'])
                logger.info(f"Deleted document '{title}' with {len(results['ids'])} chunks")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error deleting document: {e}")
            return False
    
    async def update_document(self, title: str, new_content: str, 
                            source: str = None, document_type: str = None,
                            tags: List[str] = None) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG —Å–∏—Å—Ç–µ–º–µ"""
        try:
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            await self.delete_document(title)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            await self.add_document(title, new_content, source, document_type, tags)
            
            logger.info(f"Updated document '{title}'")
            return True
            
        except Exception as e:
            logger.error(f"Error updating document: {e}")
            return False
    
    async def search_by_keywords(self, keywords: List[str], top_k: int = 10) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            query = " ".join(keywords)
            
            # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            search_results = await self.search_documents(query, top_k)
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            filtered_results = []
            for result in search_results:
                result_keywords = result.doc_metadata.get('keywords', [])
                if any(keyword.lower() in [kw.lower() for kw in result_keywords] 
                      for keyword in keywords):
                    filtered_results.append(result)
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"Error searching by keywords: {e}")
            return []
    
    async def get_similar_documents(self, document_id: str, top_k: int = 5) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            if not self.embedding_manager.chroma_client:
                return []
            
            collection = self.embedding_manager.chroma_client.get_collection(self.collection_name)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            results = collection.get(ids=[document_id])
            
            if not results['embeddings']:
                return []
            
            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            similar_results = collection.query(
                query_embeddings=results['embeddings'],
                n_results=top_k + 1  # +1 —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
            )
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            search_results = []
            for i, distance in enumerate(similar_results['distances'][0]):
                if similar_results['ids'][0][i] != document_id:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º –¥–æ–∫—É–º–µ–Ω—Ç
                    similarity = 1 - distance
                    search_result = SearchResult(
                        document=similar_results['documents'][0][i],
                        doc_metadata=similar_results['metadatas'][0][i],
                        similarity=similarity,
                        source=similar_results['metadatas'][0][i].get('source', 'unknown'),
                        chunk_id=similar_results['ids'][0][i]
                    )
                    search_results.append(search_result)
            
            return search_results
            
        except Exception as e:
            logger.error(f"Error getting similar documents: {e}")
            return [] 