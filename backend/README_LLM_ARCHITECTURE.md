# üß† –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è LLM –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ reLink

## üìã –û–±–∑–æ—Ä

–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ reLink –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ Ollama –≤—Å–µ–º–∏ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è Apple M4 –∏ –¥—Ä—É–≥–∏—Ö —Å–∏—Å—Ç–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

1. **CentralizedLLMArchitecture** - –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä
2. **ConcurrentOllamaManager** - –ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Ollama
3. **DistributedRAGCache** - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –∫—ç—à —Å Redis
4. **RequestPrioritizer** - –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
5. **RAGMonitor** - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏

### –°—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã

```
–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã ‚Üí LLMIntegrationService ‚Üí CentralizedLLMArchitecture
                                           ‚Üì
                                    RequestPrioritizer
                                           ‚Üì
                                    ConcurrentOllamaManager (—Å–µ–º–∞—Ñ–æ—Ä=2)
                                           ‚Üì
                                    Ollama (qwen2.5:7b-instruct-turbo)
                                           ‚Üì
                                    DistributedRAGCache
                                           ‚Üì
                                    RAGMonitor
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -r requirements_llm.txt
```

### –ó–∞–ø—É—Å–∫ Redis

```bash
docker run -d -p 6379:6379 redis:alpine
```

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from app.llm_integration import get_llm_integration_service

async def main():
    # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    llm_service = await get_llm_integration_service()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = await llm_service.generate_response(
        prompt="–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
        max_tokens=100
    )
    
    print(response)

# –ó–∞–ø—É—Å–∫
asyncio.run(main())
```

## üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Apple M4

```python
from app.llm.concurrent_manager import OllamaConfig

config = OllamaConfig(
    max_concurrent_requests=2,  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è M4
    request_timeout=300.0,
    keep_alive="2h",
    context_length=4096,
    batch_size=512,
    num_parallel=2
)
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤

```python
from app.llm.request_prioritizer import RequestPrioritizer

prioritizer = RequestPrioritizer()

# –£—Ä–æ–≤–Ω–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
priorities = {
    "critical": 100,    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ
    "high": 80,         # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    "normal": 50,       # –û–±—ã—á–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    "low": 20,          # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    "background": 10    # –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
}
```

## üîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º–∏

### –°–µ—Ä–≤–∏—Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

```python
from app.llm_integration import LLMIntegrationFactory

async def test_integration():
    llm_service = await get_llm_integration_service()
    factory = LLMIntegrationFactory(llm_service)
    
    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    testing = factory.get_testing_integration()
    test_case = await testing.generate_test_case(
        "–§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ email",
        "unit"
    )
    
    return test_case
```

### –°–µ—Ä–≤–∏—Å –¥–∏–∞–≥—Ä–∞–º–º

```python
async def diagram_integration():
    llm_service = await get_llm_integration_service()
    factory = LLMIntegrationFactory(llm_service)
    
    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥–∏–∞–≥—Ä–∞–º–º–∞–º–∏
    diagram = factory.get_diagram_integration()
    description = await diagram.generate_diagram_description(
        {"nodes": ["A", "B"], "edges": [("A", "B")]},
        "flowchart"
    )
    
    return description
```

### –°–µ—Ä–≤–∏—Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

```python
async def monitoring_integration():
    llm_service = await get_llm_integration_service()
    factory = LLMIntegrationFactory(llm_service)
    
    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
    monitoring = factory.get_monitoring_integration()
    analysis = await monitoring.analyze_performance_data({
        "response_time": 2.5,
        "error_rate": 0.01,
        "throughput": 100
    })
    
    return analysis
```

## üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏

### –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫

```python
async def get_metrics():
    llm_service = await get_llm_integration_service()
    metrics = await llm_service.get_metrics()
    
    print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {metrics['total_requests']}")
    print(f"–ö—ç—à-—Ö–∏—Ç—ã: {metrics['cache_hits']}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {metrics['avg_response_time']:.2f}s")
    print(f"–û—à–∏–±–∫–∏: {metrics['errors']}")
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è

```python
async def health_check():
    llm_service = await get_llm_integration_service()
    health = await llm_service.health_check()
    
    if health["status"] == "healthy":
        print("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
    else:
        print(f"‚ùå –ü—Ä–æ–±–ª–µ–º—ã: {health}")
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

```bash
# –í—Å–µ —Ç–µ—Å—Ç—ã
pytest test_llm_architecture.py -v

# –¢–æ–ª—å–∫–æ —Ç–µ—Å—Ç—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
pytest test_llm_architecture.py::TestCentralizedLLMArchitecture -v

# –¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
pytest test_llm_architecture.py::TestPerformance -v
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
import asyncio
import time
from app.llm_integration import get_llm_integration_service

async def performance_test():
    llm_service = await get_llm_integration_service()
    
    # –¢–µ—Å—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    start_time = time.time()
    
    tasks = []
    for i in range(10):
        task = llm_service.generate_response(f"–ó–∞–ø—Ä–æ—Å {i}")
        tasks.append(task)
    
    responses = await asyncio.gather(*tasks)
    total_time = time.time() - start_time
    
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(responses)} –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ {total_time:.2f}s")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {total_time/len(responses):.2f}s –Ω–∞ –∑–∞–ø—Ä–æ—Å")
```

## üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

### Rate Limiting

```python
from app.llm.request_prioritizer import RequestPrioritizer

prioritizer = RequestPrioritizer()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
can_process = prioritizer.can_process_request(
    user_id=123,
    user_tier="standard"  # premium, standard, basic, free
)

if not can_process:
    raise Exception("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤")
```

### –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è

```python
# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ø–µ—Ä–≤—ã–º–∏
critical_request = LLMRequest(
    id="critical-1",
    prompt="–°—Ä–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
    priority="critical"
)

# –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏
background_request = LLMRequest(
    id="bg-1", 
    prompt="–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞",
    priority="background"
)
```

## üö® –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ Ollama

```python
async def troubleshoot_ollama():
    llm_service = await get_llm_integration_service()
    health = await llm_service.health_check()
    
    if health["ollama_status"]["status"] != "healthy":
        print("‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å Ollama:")
        print(f"   - {health['ollama_status']['error']}")
        print("üîß –†–µ—à–µ–Ω–∏—è:")
        print("   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω")
        print("   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –º–æ–¥–µ–ª—å qwen2.5:7b-instruct-turbo")
        print("   3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Ä—Ç 11434")
```

### –ü—Ä–æ–±–ª–µ–º—ã —Å Redis

```python
async def troubleshoot_redis():
    from app.llm.distributed_cache import DistributedRAGCache
    
    cache = DistributedRAGCache()
    health = await cache.health_check()
    
    if health["status"] != "healthy":
        print("‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å Redis:")
        print(f"   - {health['error']}")
        print("üîß –†–µ—à–µ–Ω–∏—è:")
        print("   1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Redis: docker run -d -p 6379:6379 redis:alpine")
        print("   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ localhost:6379")
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
async def optimize_performance():
    llm_service = await get_llm_integration_service()
    metrics = await llm_service.get_metrics()
    
    # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
    if metrics["avg_response_time"] > 5.0:
        print("‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã")
        print("üîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("   - –£–º–µ–Ω—å—à–∏—Ç–µ max_tokens")
        print("   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ")
        print("   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–∏—Å—Ç–µ–º—É")
    
    if metrics["cache_hit_rate"] < 0.5:
        print("‚ö†Ô∏è –ù–∏–∑–∫–∏–π hit rate –∫—ç—à–∞")
        print("üîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("   - –£–≤–µ–ª–∏—á—å—Ç–µ TTL –∫—ç—à–∞")
        print("   - –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
```

## üìö API Reference

### CentralizedLLMArchitecture

```python
class CentralizedLLMArchitecture:
    async def start() -> None
    async def stop() -> None
    async def submit_request(request: LLMRequest) -> str
    async def get_response(request_id: str) -> Optional[LLMResponse]
    def get_metrics() -> Dict[str, Any]
    async def health_check() -> Dict[str, Any]
```

### LLMIntegrationService

```python
class LLMIntegrationService:
    async def initialize(redis_url: str = "redis://localhost:6379") -> None
    async def shutdown() -> None
    async def generate_response(prompt: str, **kwargs) -> str
    async def get_embedding(text: str) -> List[float]
    async def search_knowledge_base(query: str, limit: int = 5) -> List[str]
    async def get_metrics() -> Dict[str, Any]
    async def health_check() -> Dict[str, Any]
```

### LLMRequest

```python
@dataclass
class LLMRequest:
    id: str
    prompt: str
    model_name: str = "qwen2.5:7b-instruct-turbo"
    priority: str = "normal"
    max_tokens: int = 100
    temperature: float = 0.7
    use_rag: bool = True
    user_id: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
```

## üîÑ –ú–∏–≥—Ä–∞—Ü–∏—è —Å —Å—Ç–∞—Ä–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞

```python
# –°—Ç–∞—Ä—ã–π –∫–æ–¥
from app.llm_router import generate_seo_recommendations

# –ù–æ–≤—ã–π –∫–æ–¥
from app.llm_integration import get_llm_integration_service

async def new_approach():
    llm_service = await get_llm_integration_service()
    response = await llm_service.generate_response(
        prompt="SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
        priority="high"
    )
    return response
```

### –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

–°—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (`generate_seo_recommendations`, `generate_diagram`, etc.) –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–æ —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–æ–≤—É—é —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.

## üìà –ü–ª–∞–Ω—ã —Ä–∞–∑–≤–∏—Ç–∏—è

### –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ (1-2 –Ω–µ–¥–µ–ª–∏)
- [x] –ë–∞–∑–æ–≤–∞—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- [x] –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ Ollama
- [x] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
- [x] –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
- [ ] –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ (1-2 –º–µ—Å—è—Ü–∞)
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- [ ] A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
- [ ] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ LLM API

### –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ (3-6 –º–µ—Å—è—Ü–µ–≤)
- [ ] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- [ ] –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- [ ] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Kubernetes

## ü§ù –í–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç

1. –§–æ—Ä–∫–Ω–∏—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
2. –°–æ–∑–¥–∞–π—Ç–µ –≤–µ—Ç–∫—É –¥–ª—è –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
3. –í–Ω–µ—Å–∏—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
4. –î–æ–±–∞–≤—å—Ç–µ —Ç–µ—Å—Ç—ã
5. –°–æ–∑–¥–∞–π—Ç–µ Pull Request

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: [README_LLM_ARCHITECTURE.md](README_LLM_ARCHITECTURE.md)
- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-repo/discussions)

---

**reLink Centralized LLM Architecture** - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Ollama –¥–ª—è –≤—Å–µ—Ö –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã üöÄ 