#!/usr/bin/env python3
"""
üöÄ –ü–†–û–î–í–ò–ù–£–¢–´–ô SEO-–ë–ï–ù–ß–ú–ê–†–ö –°–ò–°–¢–ï–ú–ê
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ LLM –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π SEO-–ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏

–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏:
üéØ –ö–∞—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–∞ –∏ –∫–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥–∞
üîó SEO-—ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏  
‚ö° –°–∫–æ—Ä–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
üß† –ê–Ω–∞–ª–∏–∑ —Å–º—ã—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
üìö RAG-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ —Ä–∞–±–æ—Ç–∞ —Å –ë–î
üé® –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
"""

import asyncio
import json
import time
import httpx
import re
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
import statistics
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class AdvancedSEOMetrics:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è SEO-–±–µ–Ω—á–º–∞—Ä–∫–∞."""
    model_name: str
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    avg_response_time: float
    tokens_per_second: float
    context_utilization: float
    
    # –ö–∞—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–∞ –∏ –∫–æ–ø–∏—Ä–∞–π—Ç–∏–Ω–≥–∞
    language_quality_score: float      # –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞, —Å—Ç–∏–ª—å, —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å
    copywriting_expertise: float       # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤
    linguistic_diversity: float        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
    
    # SEO-—ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å
    seo_understanding: float           # –ü–æ–Ω–∏–º–∞–Ω–∏–µ SEO-–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤
    anchor_optimization: float         # –ö–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∫–æ—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    semantic_relevance: float          # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    internal_linking_strategy: float   # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏
    
    # –°–º—ã—Å–ª–æ–≤–æ–π –∞–Ω–∞–ª–∏–∑ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    semantic_analysis_depth: float     # –ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–º—ã—Å–ª–∞
    context_retention: float           # –£–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    conceptual_understanding: float    # –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
    topical_coherence: float           # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    
    # RAG –∏ –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
    rag_integration_quality: float     # –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã —Å RAG
    database_utilization: float        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î
    learning_from_history: float       # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    recommendation_evolution: float    # –≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    
    # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å
    domain_expertise: float            # –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏
    strategic_thinking: float          # –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
    optimization_insights: float       # –ò–Ω—Å–∞–π—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    
    # –û–±—â–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
    overall_score: float
    reliability_rate: float
    hallucination_rate: float
    timestamp: str

@dataclass
class SEOTestCase:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∫–µ–π—Å –¥–ª—è SEO-–æ—Ü–µ–Ω–∫–∏."""
    name: str
    domain: str
    test_text: str
    expected_links_count: int
    difficulty_level: str
    category: str
    seo_complexity: str                # low, medium, high, expert
    target_keywords: List[str]
    contextual_depth: int              # –≥–ª—É–±–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 1-10
    requires_domain_knowledge: bool
    rag_context_needed: bool
    expected_anchor_types: List[str]   # exact, partial, branded, generic

class AdvancedSEOBenchmark:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ –¥–ª—è SEO."""
    
    def __init__(self, backend_url: str = "http://localhost:8000"):
        self.backend_url = backend_url
        self.test_cases = self._create_advanced_test_cases()
        self.seo_keywords_dict = self._load_seo_vocabulary()
        
    def _load_seo_vocabulary(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç SEO-—Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç–∏."""
        return {
            "technical_seo": [
                "–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è", "–∫—Ä–∞—É–ª–∏–Ω–≥", "sitemap", "robots.txt", "canonical", 
                "schema markup", "core web vitals", "page speed", "–º–æ–±–∏–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
            ],
            "content_seo": [
                "–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ", "LSI", "tf-idf", 
                "–∑–∞–≥–æ–ª–æ–≤–∫–∏", "–º–µ—Ç–∞-—Ç–µ–≥–∏", "alt-—Ç–µ–≥–∏", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞"
            ],
            "link_building": [
                "–≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏", "–∞–Ω–∫–æ—Ä–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã", "link juice", "nofollow", 
                "dofollow", "—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å—Å—ã–ª–æ–∫", "–∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å –¥–æ–º–µ–Ω–∞"
            ]
        }
    
    def _create_advanced_test_cases(self) -> List[SEOTestCase]:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã."""
        return [
            SEOTestCase(
                name="beginner_seo_basics",
                domain="seo-newbie.ru",
                test_text="–ü–æ–∏—Å–∫–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∞–π—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∞—Å–ø–µ–∫—Ç–∞–º–∏ –∏ —Å—Å—ã–ª–æ—á–Ω–æ–π –º–∞—Å—Å–æ–π. –í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å, –∫–∞–∫ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å —Å–∞–π—Ç–∞–º–∏.",
                expected_links_count=3,
                difficulty_level="easy",
                category="seo_basics",
                seo_complexity="low",
                target_keywords=["–ø–æ–∏—Å–∫–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è", "–∫–æ–Ω—Ç–µ–Ω—Ç", "—Å—Å—ã–ª–∫–∏"],
                contextual_depth=3,
                requires_domain_knowledge=False,
                rag_context_needed=False,
                expected_anchor_types=["exact", "partial"]
            ),
            SEOTestCase(
                name="advanced_technical_seo",
                domain="tech-seo-expert.com",
                test_text="Core Web Vitals —Å—Ç–∞–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è. Largest Contentful Paint, First Input Delay –∏ Cumulative Layout Shift —Ç—Ä–µ–±—É—é—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. JavaScript-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –∫—Ä–∞—É–ª–∏–Ω–≥, –∞ structured data –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞–º –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç.",
                expected_links_count=6,
                difficulty_level="hard",
                category="technical_seo",
                seo_complexity="expert",
                target_keywords=["Core Web Vitals", "LCP", "JavaScript", "structured data"],
                contextual_depth=8,
                requires_domain_knowledge=True,
                rag_context_needed=True,
                expected_anchor_types=["exact", "partial", "branded"]
            ),
            SEOTestCase(
                name="ecommerce_seo_strategy",
                domain="shop-optimization.ru",
                test_text="–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω —Ç—Ä–µ–±—É–µ—Ç –æ—Å–æ–±–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ SEO. –ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã, –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ - –ø–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ. –§–∞—Å–µ—Ç–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –¥—É–±–ª–∏, pagination –≤–ª–∏—è–µ—Ç –Ω–∞ –∫—Ä–∞—É–ª–∏–Ω–≥–æ–≤—ã–π –±—é–¥–∂–µ—Ç. Schema.org —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –∫—Ä–∏—Ç–∏—á–Ω–∞ –¥–ª—è rich snippets.",
                expected_links_count=8,
                difficulty_level="expert",
                category="ecommerce_seo",
                seo_complexity="expert",
                target_keywords=["–∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã", "—Ñ–∞—Å–µ—Ç–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è", "schema.org", "rich snippets"],
                contextual_depth=9,
                requires_domain_knowledge=True,
                rag_context_needed=True,
                expected_anchor_types=["exact", "partial", "branded", "generic"]
            ),
            SEOTestCase(
                name="content_marketing_integration",
                domain="content-seo.agency",
                test_text="–ö–æ–Ω—Ç–µ–Ω—Ç-–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ SEO –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å —Å–∏–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏. –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–∫—Ä—ã—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —è–¥—Ä–æ, –∞ pillar pages —Å–æ–∑–¥–∞—é—Ç —Ç–æ–ø–∏—á–µ—Å–∫—É—é –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å. User intent –∞–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.",
                expected_links_count=5,
                difficulty_level="medium",
                category="content_seo",
                seo_complexity="high",
                target_keywords=["—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", "pillar pages", "user intent"],
                contextual_depth=6,
                requires_domain_knowledge=True,
                rag_context_needed=True,
                expected_anchor_types=["exact", "partial", "branded"]
            ),
            SEOTestCase(
                name="local_seo_optimization",
                domain="local-business.ru",
                test_text="–õ–æ–∫–∞–ª—å–Ω–æ–µ SEO —Ç—Ä–µ–±—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Google My Business –ø—Ä–æ—Ñ–∏–ª—è, —Ä–∞–±–æ—Ç—ã —Å –º–µ—Å—Ç–Ω—ã–º–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è–º–∏ –∏ NAP –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏. Local pack –∑–∞–Ω–∏–º–∞–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø–æ–∏—Å–∫–µ, –∞ –æ—Ç–∑—ã–≤—ã –≤–ª–∏—è—é—Ç –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ.",
                expected_links_count=4,
                difficulty_level="medium", 
                category="local_seo",
                seo_complexity="medium",
                target_keywords=["Google My Business", "NAP", "local pack"],
                contextual_depth=5,
                requires_domain_knowledge=True,
                rag_context_needed=False,
                expected_anchor_types=["exact", "branded"]
            )
        ]
    
    async def benchmark_model_advanced(self, model_name: str, iterations: int = 3) -> AdvancedSEOMetrics:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –º–æ–¥–µ–ª–∏."""
        print(f"üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π SEO-–±–µ–Ω—á–º–∞—Ä–∫ –º–æ–¥–µ–ª–∏: {model_name}")
        print("=" * 70)
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å
        if not await self._switch_model(model_name):
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ {model_name}")
            
        test_cases = self._create_advanced_test_cases()
        response_times = []
        recommendations_data = []
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            for iteration in range(iterations):
                print(f"   üîÑ –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1}/{iterations}")
                
                for i, test_case in enumerate(test_cases, 1):
                    try:
                        start_time = time.time()
                        
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç /api/v1/recommend
                        response = await client.post(
                            f"{self.backend_url}/api/v1/recommend",
                            json={"text": test_case.test_text},
                            timeout=45.0
                        )
                        
                        end_time = time.time()
                        response_time = end_time - start_time
                        response_times.append(response_time)
                        
                        if response.status_code == 200:
                            result = response.json()
                            recommendations_count = len(result.get('links', []))
                            print(f"     ‚úÖ {test_case.name}: {recommendations_count} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, {response_time:.2f}—Å")
                            
                            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                            formatted_recommendations = []
                            for link in result.get('links', []):
                                if isinstance(link, str):
                                    # –ü—Ä–æ—Å—Ç–∞—è —Å—Å—ã–ª–∫–∞ - —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                                    formatted_recommendations.append({
                                        'anchor': link[:50] + '...' if len(link) > 50 else link,
                                        'anchor_text': link[:50] + '...' if len(link) > 50 else link,
                                        'reasoning': f'–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –æ {test_case.category}',
                                        'from': test_case.domain,
                                        'to': link,
                                        'quality_score': 0.7,
                                        'test_case': test_case
                                    })
                                elif isinstance(link, dict):
                                    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                                    link['test_case'] = test_case
                                    formatted_recommendations.append(link)
                            
                            recommendations_data.extend(formatted_recommendations)
                            
                        else:
                            print(f"     ‚ùå {test_case.name}: HTTP {response.status_code}")
                            
                    except asyncio.TimeoutError:
                        print(f"     ‚è±Ô∏è {test_case.name}: –¢–∞–π–º-–∞—É—Ç")
                    except Exception as e:
                        print(f"     ‚ùå {test_case.name}: {str(e)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        metrics = await self._analyze_advanced_results(model_name, recommendations_data, response_times)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {model_name}:")
        print(f"   –û–±—â–∏–π –±–∞–ª–ª: {metrics.overall_score:.2f}")
        print(f"   SEO-–ø–æ–Ω–∏–º–∞–Ω–∏–µ: {metrics.seo_understanding:.2f}")
        print(f"   –ö–∞—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–∞: {metrics.language_quality_score:.2f}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {metrics.avg_response_time:.2f}—Å")
        print(f"   –¢–æ–∫–µ–Ω–æ–≤/—Å–µ–∫: {metrics.tokens_per_second:.1f}")
        
        return metrics
    
    async def _analyze_advanced_results(
        self, 
        model_name: str, 
        recommendations_data: List[Dict], 
        response_times: List[float]
    ) -> AdvancedSEOMetrics:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏."""
        
        if not recommendations_data:
            # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            return AdvancedSEOMetrics(
                model_name=model_name,
                avg_response_time=statistics.mean(response_times) if response_times else 0,
                tokens_per_second=self._estimate_tokens_per_second(response_times),
                context_utilization=0.0,
                language_quality_score=0.0,
                copywriting_expertise=0.0,
                linguistic_diversity=0.0,
                seo_understanding=0.0,
                anchor_optimization=0.0,
                semantic_relevance=0.0,
                internal_linking_strategy=0.0,
                semantic_analysis_depth=0.0,
                context_retention=0.0,
                conceptual_understanding=0.0,
                topical_coherence=0.0,
                rag_integration_quality=0.0,
                database_utilization=0.0,
                learning_from_history=0.0,
                recommendation_evolution=0.0,
                domain_expertise=0.0,
                strategic_thinking=0.0,
                optimization_insights=0.0,
                overall_score=0.0,
                reliability_rate=0.0,
                hallucination_rate=0.0,
                timestamp=datetime.now().isoformat()
            )
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        avg_response_time = statistics.mean(response_times) if response_times else 0
        tokens_per_second = self._estimate_tokens_per_second(response_times)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–µ—Å—Ç-–∫–µ–π—Å–∞–º
        test_case_groups = {}
        for rec in recommendations_data:
            test_case = rec.get('test_case')
            if test_case:
                if test_case.name not in test_case_groups:
                    test_case_groups[test_case.name] = []
                test_case_groups[test_case.name].append(rec)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —è–∑—ã–∫–∞
        language_scores = []
        for group_name, group_recs in test_case_groups.items():
            if group_recs and group_recs[0].get('test_case'):
                test_case = group_recs[0]['test_case']
                lang_score = self._evaluate_language_quality(group_recs, test_case)
                language_scores.append(lang_score)
        
        language_quality_score = statistics.mean(language_scores) if language_scores else 0.5
        
        # –û—Ü–µ–Ω–∫–∞ SEO-–ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∞–Ω–∫–æ—Ä–æ–≤
        seo_scores = []
        anchor_scores = []
        
        for rec in recommendations_data:
            anchor = rec.get('anchor', '') or rec.get('anchor_text', '')
            
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ SEO –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–∫–æ—Ä–∞
            seo_score = 0.5
            if len(anchor) > 10 and len(anchor) < 100:  # –•–æ—Ä–æ—à–∞—è –¥–ª–∏–Ω–∞
                seo_score += 0.2
            if not any(word in anchor.lower() for word in ['–∑–¥–µ—Å—å', '—Ç—É—Ç', '—Å—Å—ã–ª–∫–∞']):  # –ù–µ –æ–±—â–∏–µ —Å–ª–æ–≤–∞
                seo_score += 0.2
            if len(anchor.split()) <= 5:  # –ù–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
                seo_score += 0.1
                
            seo_scores.append(min(seo_score, 1.0))
            anchor_scores.append(seo_score)
        
        seo_understanding = statistics.mean(seo_scores) if seo_scores else 0.5
        anchor_optimization = statistics.mean(anchor_scores) if anchor_scores else 0.5
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å - –±–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        semantic_relevance = 0.7  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∫–æ—Ä–æ–≤)
        unique_anchors = set()
        total_anchors = 0
        for rec in recommendations_data:
            anchor = rec.get('anchor', '') or rec.get('anchor_text', '')
            if anchor:
                unique_anchors.add(anchor.lower())
                total_anchors += 1
        
        linguistic_diversity = len(unique_anchors) / max(total_anchors, 1) if total_anchors > 0 else 0.5
        
        # –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        internal_linking_strategy = self._calculate_linking_strategy_score(recommendations_data)
        
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        topical_coherence = self._calculate_topical_coherence(recommendations_data)
        
        # –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å –≤ –¥–æ–º–µ–Ω–µ
        domain_expertise = self._calculate_domain_expertise(recommendations_data)
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
        strategic_thinking = self._evaluate_strategic_thinking(recommendations_data)
        
        # –ò–Ω—Å–∞–π—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimization_insights = self._evaluate_optimization_insights(recommendations_data)
        
        # –û—Ü–µ–Ω–∫–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
        hallucination_rate = self._estimate_hallucination_rate(recommendations_data)
        
        # –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å (–æ–±—Ä–∞—Ç–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ –æ—Ç –æ—à–∏–±–æ–∫)
        reliability_rate = max(0.0, 1.0 - hallucination_rate)
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –º–µ—Ç—Ä–∏–∫
        metrics = AdvancedSEOMetrics(
            model_name=model_name,
            avg_response_time=avg_response_time,
            tokens_per_second=tokens_per_second,
            context_utilization=0.7,  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            language_quality_score=language_quality_score,
            copywriting_expertise=min(language_quality_score + 0.1, 1.0),
            linguistic_diversity=linguistic_diversity,
            seo_understanding=seo_understanding,
            anchor_optimization=anchor_optimization,
            semantic_relevance=semantic_relevance,
            internal_linking_strategy=internal_linking_strategy,
            semantic_analysis_depth=0.6,  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            context_retention=0.6,  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            conceptual_understanding=0.7,  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            topical_coherence=topical_coherence,
            rag_integration_quality=0.5,  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ API
            database_utilization=0.3,  # –ù–∏–∑–∫–æ–µ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ API
            learning_from_history=0.2,  # –ù–∏–∑–∫–æ–µ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ API
            recommendation_evolution=0.2,  # –ù–∏–∑–∫–æ–µ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ API
            domain_expertise=domain_expertise,
            strategic_thinking=strategic_thinking,
            optimization_insights=optimization_insights,
            overall_score=0.0,  # –í—ã—á–∏—Å–ª–∏–º –ø–æ–∑–∂–µ
            reliability_rate=reliability_rate,
            hallucination_rate=hallucination_rate,
            timestamp=datetime.now().isoformat()
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –±–∞–ª–ª
        metrics.overall_score = self._calculate_overall_advanced_score(metrics)
        
        return metrics

    def _evaluate_language_quality(self, links: List, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–∞ –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö."""
        if not links:
            return 0
        
        score = 0
        for link_data in links:
            if isinstance(link_data, dict):
                anchor = link_data.get('anchor_text', '')
                reasoning = link_data.get('reasoning', '')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –∏ —Å—Ç–∏–ª—å
                if anchor:
                    # –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∫–æ—Ä–∞
                    if self._is_natural_anchor(anchor):
                        score += 20
                    # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —à—Ç–∞–º–ø–æ–≤
                    if not self._contains_cliches(anchor):
                        score += 15
                
                if reasoning:
                    # –ö–∞—á–µ—Å—Ç–≤–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                    if len(reasoning) > 50 and self._has_professional_vocabulary(reasoning):
                        score += 25
                    # –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è
                    if self._is_logical_reasoning(reasoning):
                        score += 20
        
        return min(score / len(links), 100)
    
    def _evaluate_copywriting_expertise(self, links: List, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä—Å–∫—É—é —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å."""
        if not links:
            return 0
            
        score = 0
        professional_indicators = 0
        
        for link_data in links:
            if isinstance(link_data, dict):
                anchor = link_data.get('anchor_text', '')
                reasoning = link_data.get('reasoning', '')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä—Å–∫–∏–µ –ø—Ä–∏–µ–º—ã
                if self._uses_action_words(anchor):
                    professional_indicators += 1
                
                if self._demonstrates_understanding_of_user_intent(reasoning):
                    professional_indicators += 1
                    
                if self._shows_conversion_awareness(reasoning):
                    professional_indicators += 1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 100
        score = (professional_indicators / (len(links) * 3)) * 100 if links else 0
        return min(score, 100)
    
    def _evaluate_seo_understanding(self, links: List, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ SEO-–ø—Ä–∏–Ω—Ü–∏–ø–æ–≤."""
        if not links:
            return 0
            
        seo_score = 0
        seo_knowledge_indicators = 0
        
        for link_data in links:
            if isinstance(link_data, dict):
                anchor = link_data.get('anchor_text', '')
                reasoning = link_data.get('reasoning', '')
                url = link_data.get('url', '')
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º SEO-–∑–Ω–∞–Ω–∏—è
                combined_text = f"{anchor} {reasoning} {url}".lower()
                
                # –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ SEO-—Ç–µ—Ä–º–∏–Ω–æ–≤
                for category, terms in self.seo_keywords_dict.items():
                    for term in terms:
                        if term.lower() in combined_text:
                            seo_knowledge_indicators += 1
                            break
                
                # –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∞–Ω–∫–æ—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                if self._is_seo_optimized_anchor(anchor, test_case):
                    seo_score += 20
                
                # –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if self._demonstrates_relevance_understanding(reasoning, test_case):
                    seo_score += 15
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏
        knowledge_score = min((seo_knowledge_indicators / len(links)) * 50, 50) if links else 0
        practical_score = min(seo_score / len(links), 50) if links else 0
        
        return knowledge_score + practical_score
    
    def _evaluate_anchor_optimization(self, links: List, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–Ω–∫–æ—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤."""
        if not links:
            return 0
            
        total_score = 0
        
        for link_data in links:
            if isinstance(link_data, dict):
                anchor = link_data.get('anchor_text', '')
                
                anchor_score = 0
                
                # –î–ª–∏–Ω–∞ –∞–Ω–∫–æ—Ä–∞ (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è 2-5 —Å–ª–æ–≤)
                word_count = len(anchor.split())
                if 2 <= word_count <= 5:
                    anchor_score += 20
                elif word_count == 1 or word_count == 6:
                    anchor_score += 10
                
                # –ù–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                anchor_lower = anchor.lower()
                keywords_found = sum(1 for kw in test_case.target_keywords 
                                   if kw.lower() in anchor_lower)
                if keywords_found > 0:
                    anchor_score += min(keywords_found * 15, 30)
                
                # –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å
                if self._is_natural_anchor(anchor):
                    anchor_score += 25
                
                # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (–Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è)
                other_anchors = [other.get('anchor_text', '') for other in links 
                               if other != link_data and isinstance(other, dict)]
                if anchor not in other_anchors:
                    anchor_score += 15
                
                total_score += min(anchor_score, 100)
        
        return total_score / len(links) if links else 0
    
    def _evaluate_semantic_relevance(self, links: List, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        if not links:
            return 0
            
        relevance_scores = []
        test_text_lower = test_case.test_text.lower()
        target_keywords_lower = [kw.lower() for kw in test_case.target_keywords]
        
        for link_data in links:
            if isinstance(link_data, dict):
                anchor = link_data.get('anchor_text', '').lower()
                reasoning = link_data.get('reasoning', '').lower()
                url = link_data.get('url', '').lower()
                
                relevance_score = 0
                
                # –ü—Ä—è–º–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —Ç–µ–∫—Å—Ç—É
                common_words = set(test_text_lower.split()) & set(f"{anchor} {reasoning}".split())
                if len(common_words) >= 3:
                    relevance_score += 30
                elif len(common_words) >= 1:
                    relevance_score += 15
                
                # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ü–µ–ª–µ–≤—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                kw_matches = sum(1 for kw in target_keywords_lower 
                               if kw in anchor or kw in reasoning)
                relevance_score += min(kw_matches * 20, 40)
                
                # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å
                if self._is_thematically_coherent(anchor, reasoning, test_case):
                    relevance_score += 30
                
                relevance_scores.append(min(relevance_score, 100))
        
        return statistics.mean(relevance_scores) if relevance_scores else 0
    
    def _evaluate_semantic_analysis(self, links: List, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–ª—É–±–∏–Ω—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
        if not links:
            return 0
            
        depth_indicators = 0
        total_possible = len(links) * 4  # 4 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ —Å—Å—ã–ª–∫—É
        
        for link_data in links:
            if isinstance(link_data, dict):
                reasoning = link_data.get('reasoning', '')
                
                # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                if len(reasoning) > 100:  # –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
                    depth_indicators += 1
                
                if self._mentions_semantic_concepts(reasoning):
                    depth_indicators += 1
                
                if self._shows_contextual_understanding(reasoning, test_case):
                    depth_indicators += 1
                
                if self._demonstrates_strategic_thinking(reasoning):
                    depth_indicators += 1
        
        return (depth_indicators / total_possible * 100) if total_possible > 0 else 0
    
    def _evaluate_context_retention(self, links: List, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —É–¥–µ—Ä–∂–∏–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç."""
        context_score = 0
        
        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        if len(links) > 0:
            context_score += 30  # –ù–∞–ª–∏—á–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        
        # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
        expected = test_case.expected_links_count
        actual = len(links)
        if actual == expected:
            context_score += 40
        elif abs(actual - expected) <= 1:
            context_score += 20
        
        # –£—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if test_case.contextual_depth >= 7 and len(links) >= expected:
            context_score += 30  # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–±–æ—Ç—É —Å–æ —Å–ª–æ–∂–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        return min(context_score, 100)
    
    def _evaluate_rag_integration(self, raw_response: Dict, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å RAG."""
        rag_score = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è RAG
        if 'analysis' in raw_response or 'insights' in raw_response:
            rag_score += 30
        
        if 'thematic_analysis' in raw_response:
            rag_score += 25
        
        if 'recommendations' in raw_response and len(raw_response['recommendations']) > 0:
            rag_score += 45
        
        return min(rag_score, 100)
    
    def _evaluate_database_utilization(self, raw_response: Dict, test_case: SEOTestCase) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
        db_score = 0
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ë–î
        indicators = [
            'analysis_history', 'domain_analysis', 'existing_connections',
            'cumulative_insights', 'thematic_clusters', 'semantic_connections'
        ]
        
        for indicator in indicators:
            if indicator in str(raw_response).lower():
                db_score += 15
        
        return min(db_score, 100)
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    def _is_natural_anchor(self, anchor: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∫–æ—Ä–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        unnatural_patterns = [
            r'^(—Å—Å—ã–ª–∫–∞|–ø–µ—Ä–µ–π—Ç–∏|—á–∏—Ç–∞—Ç—å|—É–∑–Ω–∞—Ç—å)\s*\d*$',
            r'^(–∑–¥–µ—Å—å|—Ç—É—Ç|–¥–∞–ª–µ–µ)$',
            r'^\d+$'
        ]
        
        for pattern in unnatural_patterns:
            if re.match(pattern, anchor.lower().strip()):
                return False
        
        return len(anchor.strip()) > 0 and len(anchor.split()) <= 6
    
    def _contains_cliches(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —à—Ç–∞–º–ø–æ–≤ –∏ –∫–ª–∏—à–µ."""
        cliches = [
            '–ø–æ–¥—Ä–æ–±–Ω–µ–µ –∑–¥–µ—Å—å', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ',
            '–ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–≤—Å–µ –¥–µ—Ç–∞–ª–∏', '–∫–ª–∏–∫–∞–π—Ç–µ –∑–¥–µ—Å—å'
        ]
        
        text_lower = text.lower()
        return any(cliche in text_lower for cliche in cliches)
    
    def _has_professional_vocabulary(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏."""
        professional_terms = [
            '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–∞–Ω–∞–ª–∏–∑',
            '–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è', '—Ä–µ—à–µ–Ω–∏–µ', '–ø–æ–¥—Ö–æ–¥', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è'
        ]
        
        text_lower = text.lower()
        return sum(1 for term in professional_terms if term in text_lower) >= 2
    
    def _is_logical_reasoning(self, reasoning: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è."""
        logical_connectors = [
            '–ø–æ—Ç–æ–º—É —á—Ç–æ', '–ø–æ—Å–∫–æ–ª—å–∫—É', '–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ', '—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ',
            '—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–∫—Ä–æ–º–µ —Ç–æ–≥–æ', '–±–æ–ª–µ–µ —Ç–æ–≥–æ', '–Ω–∞–ø—Ä–∏–º–µ—Ä'
        ]
        
        reasoning_lower = reasoning.lower()
        return any(connector in reasoning_lower for connector in logical_connectors)
    
    def _calculate_overall_advanced_score(self, metrics: AdvancedSEOMetrics) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–∏–π –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –±–∞–ª–ª."""
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        weights = {
            'language_quality': 0.15,
            'seo_understanding': 0.20,
            'semantic_analysis': 0.18,
            'rag_integration': 0.12,
            'anchor_optimization': 0.15,
            'domain_expertise': 0.10,
            'performance': 0.10
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞)
        performance_score = max(0, 100 - (metrics.avg_response_time * 10))
        
        overall = (
            metrics.language_quality_score * weights['language_quality'] +
            metrics.seo_understanding * weights['seo_understanding'] +
            metrics.semantic_analysis_depth * weights['semantic_analysis'] +
            metrics.rag_integration_quality * weights['rag_integration'] +
            metrics.anchor_optimization * weights['anchor_optimization'] +
            metrics.domain_expertise * weights['domain_expertise'] +
            performance_score * weights['performance']
        )
        
        return min(overall, 100)
    
    def _calculate_linking_strategy_score(self, recommendations_data: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏."""
        if not recommendations_data:
            return 0
        
        strategy_score = 0
        total_cases = len(recommendations_data)
        
        for rec_data in recommendations_data:
            links = rec_data.get('links', [])
            test_case = rec_data['test_case']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∞–Ω–∫–æ—Ä–æ–≤
            anchors = [link.get('anchor_text', '') for link in links if isinstance(link, dict)]
            unique_anchors = len(set(anchors))
            if unique_anchors == len(anchors) and len(anchors) > 1:
                strategy_score += 30
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            if test_case.seo_complexity == 'expert' and len(links) >= test_case.expected_links_count:
                strategy_score += 40
            elif test_case.seo_complexity == 'high' and len(links) >= test_case.expected_links_count - 1:
                strategy_score += 30
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å
            if self._has_thematic_coherence(links):
                strategy_score += 30
        
        return strategy_score / total_cases if total_cases > 0 else 0
    
    def _calculate_topical_coherence(self, recommendations_data: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å."""
        if not recommendations_data:
            return 0
        
        coherence_scores = []
        
        for rec_data in recommendations_data:
            links = rec_data.get('links', [])
            test_case = rec_data['test_case']
            
            coherence_score = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category_keywords = {
                'seo_basics': ['s–µ–æ', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '–ø–æ–∏—Å–∫'],
                'technical_seo': ['—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π', '—Å–∫–æ—Ä–æ—Å—Ç—å', '–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è'],
                'ecommerce_seo': ['–º–∞–≥–∞–∑–∏–Ω', '—Ç–æ–≤–∞—Ä', '–ø–æ–∫—É–ø–∫–∞'],
                'content_seo': ['–∫–æ–Ω—Ç–µ–Ω—Ç', '—Å—Ç–∞—Ç—å—è', '—Ç–µ–∫—Å—Ç'],
                'local_seo': ['–ª–æ–∫–∞–ª—å–Ω—ã–π', '–º–µ—Å—Ç–Ω—ã–π', '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π']
            }
            
            expected_keywords = category_keywords.get(test_case.category, [])
            
            for link in links:
                if isinstance(link, dict):
                    anchor = link.get('anchor_text', '').lower()
                    reasoning = link.get('reasoning', '').lower()
                    
                    matches = sum(1 for kw in expected_keywords if kw in anchor or kw in reasoning)
                    if matches > 0:
                        coherence_score += matches * 20
            
            coherence_scores.append(min(coherence_score, 100))
        
        return statistics.mean(coherence_scores) if coherence_scores else 0
    
    def _evaluate_historical_learning(self) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö."""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        return 75.0  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    
    def _evaluate_recommendation_evolution(self) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç–≤–æ–ª—é—Ü–∏—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        return 70.0  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    
    def _calculate_domain_expertise(self, recommendations_data: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å –≤ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏."""
        if not recommendations_data:
            return 0
        
        expertise_scores = []
        
        for rec_data in recommendations_data:
            links = rec_data.get('links', [])
            test_case = rec_data['test_case']
            
            expertise_score = 0
            
            # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–±–æ—Ç—É —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∫–µ–π—Å–∞–º–∏
            if test_case.requires_domain_knowledge:
                if len(links) >= test_case.expected_links_count:
                    expertise_score += 40
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
                for link in links:
                    if isinstance(link, dict):
                        reasoning = link.get('reasoning', '')
                        if self._has_professional_vocabulary(reasoning):
                            expertise_score += 30
                            break
            else:
                expertise_score += 60  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∫–µ–π—Å–æ–≤
            
            expertise_scores.append(min(expertise_score, 100))
        
        return statistics.mean(expertise_scores) if expertise_scores else 0
    
    def _evaluate_strategic_thinking(self, recommendations_data: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ."""
        if not recommendations_data:
            return 0
        
        strategic_indicators = 0
        total_links = 0
        
        for rec_data in recommendations_data:
            links = rec_data.get('links', [])
            total_links += len(links)
            
            for link in links:
                if isinstance(link, dict):
                    reasoning = link.get('reasoning', '').lower()
                    
                    # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è
                    strategic_phrases = [
                        '–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ä–∞–∑–≤–∏—Ç–∏–µ',
                        '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Ü–µ–ª—å'
                    ]
                    
                    if any(phrase in reasoning for phrase in strategic_phrases):
                        strategic_indicators += 1
        
        return (strategic_indicators / total_links * 100) if total_links > 0 else 0
    
    def _evaluate_optimization_insights(self, recommendations_data: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏–Ω—Å–∞–π—Ç—ã –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        if not recommendations_data:
            return 0
        
        insight_scores = []
        
        for rec_data in recommendations_data:
            links = rec_data.get('links', [])
            
            insight_score = 0
            
            for link in links:
                if isinstance(link, dict):
                    reasoning = link.get('reasoning', '')
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞
                    if len(reasoning) > 80:
                        insight_score += 25
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                    if any(word in reasoning.lower() for word in ['—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '–ø—Ä–µ–¥–ª–∞–≥–∞—é', '—Å—Ç–æ–∏—Ç', '–Ω—É–∂–Ω–æ']):
                        insight_score += 25
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
                    if self._is_logical_reasoning(reasoning):
                        insight_score += 25
            
            insight_scores.append(min(insight_score, 100))
        
        return statistics.mean(insight_scores) if insight_scores else 0
    
    def _estimate_hallucination_rate(self, recommendations_data: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
        if not recommendations_data:
            return 1.0
        
        hallucination_indicators = 0
        total_assessments = 0
        
        for rec_data in recommendations_data:
            links = rec_data.get('links', [])
            
            for link in links:
                if isinstance(link, dict):
                    anchor = link.get('anchor_text', '')
                    url = link.get('url', '')
                    
                    total_assessments += 1
                    
                    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
                    if not anchor or len(anchor.strip()) == 0:
                        hallucination_indicators += 1
                    
                    if url and not url.startswith(('http://', 'https://', '/')):
                        hallucination_indicators += 1
        
        return (hallucination_indicators / total_assessments) if total_assessments > 0 else 0
    
    def _uses_action_words(self, anchor: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∞–Ω–∫–æ—Ä–µ."""
        action_words = [
            '–∏–∑—É—á–∏—Ç—å', '—É–∑–Ω–∞—Ç—å', '–ø–æ–ª—É—á–∏—Ç—å', '–Ω–∞–π—Ç–∏', '–≤—ã–±—Ä–∞—Ç—å',
            '—Å—Ä–∞–≤–Ω–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–∫—É–ø–∏—Ç—å', '—Å–∫–∞—á–∞—Ç—å', '–ø—Ä–æ—á–∏—Ç–∞—Ç—å'
        ]
        
        anchor_lower = anchor.lower()
        return any(word in anchor_lower for word in action_words)
    
    def _demonstrates_understanding_of_user_intent(self, reasoning: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏–π."""
        intent_indicators = [
            '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å', '–ø–æ—Å–µ—Ç–∏—Ç–µ–ª—å', '–∫–ª–∏–µ–Ω—Ç', '—á–∏—Ç–∞—Ç–µ–ª—å',
            '–∏–Ω—Ç–µ—Ä–µ—Å', '–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å', '–∂–µ–ª–∞–Ω–∏–µ', '—Ü–µ–ª—å'
        ]
        
        reasoning_lower = reasoning.lower()
        return any(indicator in reasoning_lower for indicator in intent_indicators)
    
    def _shows_conversion_awareness(self, reasoning: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏."""
        conversion_terms = [
            '–∫–æ–Ω–≤–µ—Ä—Å–∏—è', '–ø—Ä–æ–¥–∞–∂–∞', '–ø–æ–∫—É–ø–∫–∞', '–∑–∞–∫–∞–∑',
            '–¥–µ–π—Å—Ç–≤–∏–µ', '–∫–ª–∏–∫', '–ø–µ—Ä–µ—Ö–æ–¥', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç'
        ]
        
        reasoning_lower = reasoning.lower()
        return any(term in reasoning_lower for term in conversion_terms)
    
    def _is_seo_optimized_anchor(self, anchor: str, test_case: SEOTestCase) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∞–Ω–∫–æ—Ä–∞."""
        anchor_lower = anchor.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        has_keywords = any(kw.lower() in anchor_lower for kw in test_case.target_keywords)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É (2-5 —Å–ª–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)
        word_count = len(anchor.split())
        optimal_length = 2 <= word_count <= 5
        
        return has_keywords and optimal_length and self._is_natural_anchor(anchor)
    
    def _demonstrates_relevance_understanding(self, reasoning: str, test_case: SEOTestCase) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏."""
        relevance_indicators = [
            '—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π', '—Å–≤—è–∑–∞–Ω–Ω—ã–π', '—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π', '–ø–æ–¥—Ö–æ–¥—è—â–∏–π',
            '—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π', '–ø–æ —Ç–µ–º–µ', '–æ—Ç–Ω–æ—Å—è—â–∏–π—Å—è'
        ]
        
        reasoning_lower = reasoning.lower()
        return any(indicator in reasoning_lower for indicator in relevance_indicators)
    
    def _is_thematically_coherent(self, anchor: str, reasoning: str, test_case: SEOTestCase) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å."""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        combined_text = f"{anchor} {reasoning}".lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_terms = {
            'seo_basics': ['seo', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '–ø–æ–∏—Å–∫', '—Å–∞–π—Ç'],
            'technical_seo': ['—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π', '—Å–∫–æ—Ä–æ—Å—Ç—å', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å'],
            'ecommerce_seo': ['–º–∞–≥–∞–∑–∏–Ω', '—Ç–æ–≤–∞—Ä', '–ø–æ–∫—É–ø–∫–∞', '–ø—Ä–æ–¥–∞–∂–∞'],
            'content_seo': ['–∫–æ–Ω—Ç–µ–Ω—Ç', '—Å—Ç–∞—Ç—å—è', '—Ç–µ–∫—Å—Ç', '–º–∞—Ç–µ—Ä–∏–∞–ª'],
            'local_seo': ['–ª–æ–∫–∞–ª—å–Ω—ã–π', '–º–µ—Å—Ç–Ω—ã–π', '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π']
        }
        
        expected_terms = category_terms.get(test_case.category, [])
        matches = sum(1 for term in expected_terms if term in combined_text)
        
        return matches >= 1
    
    def _mentions_semantic_concepts(self, reasoning: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π."""
        semantic_terms = [
            '—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π', '–∑–Ω–∞—á–µ–Ω–∏–µ', '—Å–º—ã—Å–ª', '–∫–æ–Ω—Ç–µ–∫—Å—Ç',
            '—Å–≤—è–∑—å', '–æ—Ç–Ω–æ—à–µ–Ω–∏–µ', '–∫–æ–Ω—Ü–µ–ø—Ü–∏—è', '–ø–æ–Ω—è—Ç–∏–µ'
        ]
        
        reasoning_lower = reasoning.lower()
        return any(term in reasoning_lower for term in semantic_terms)
    
    def _shows_contextual_understanding(self, reasoning: str, test_case: SEOTestCase) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        test_words = set(test_case.test_text.lower().split())
        reasoning_words = set(reasoning.lower().split())
        
        common_words = test_words & reasoning_words
        return len(common_words) >= 3
    
    def _demonstrates_strategic_thinking(self, reasoning: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ."""
        strategic_indicators = [
            '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–ø–ª–∞–Ω', '–ø–æ–¥—Ö–æ–¥', '–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è',
            '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Ü–µ–ª—å', '–∑–∞–¥–∞—á–∞'
        ]
        
        reasoning_lower = reasoning.lower()
        return any(indicator in reasoning_lower for indicator in strategic_indicators)
    
    def _has_thematic_coherence(self, links: List) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Å—Å—ã–ª–∫–∞–º–∏."""
        if len(links) < 2:
            return True
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤—Å–µ—Ö –∞–Ω–∫–æ—Ä–æ–≤
        all_words = []
        for link in links:
            if isinstance(link, dict):
                anchor = link.get('anchor_text', '')
                all_words.extend(anchor.lower().split())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö —Ç–µ–º
        word_counts = {}
        for word in all_words:
            if len(word) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # –ï—Å—Ç—å –ª–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–≤–∞?
        repeated_words = [word for word, count in word_counts.items() if count > 1]
        return len(repeated_words) > 0
    
    async def _switch_model(self, model_name: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ –±—ç–∫–µ–Ω–¥–µ."""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º model_name –≤ JSON —Ç–µ–ª–µ –∑–∞–ø—Ä–æ—Å–∞
                response = await client.post(
                    f"{self.backend_url}/api/v1/benchmark_model",
                    json={"model_name": model_name}  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ JSON
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("status") == "success"
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: HTTP {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"   –î–µ—Ç–∞–ª–∏: {error_detail}")
                    except:
                        print(f"   –û—Ç–≤–µ—Ç: {response.text}")
                    
        except Exception as e:
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        
        return False
    
    def _estimate_tokens_per_second(self, response_times: List[float]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥—É."""
        if not response_times:
            return 0
        avg_time = statistics.mean(response_times)
        estimated_tokens = 150  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        return estimated_tokens / avg_time if avg_time > 0 else 0
    
    async def compare_models_advanced(self, models: List[str], iterations: int = 2) -> Dict[str, AdvancedSEOMetrics]:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º."""
        print("üèÜ –ó–ê–ü–£–°–ö –ü–†–û–î–í–ò–ù–£–¢–û–ì–û SEO-–ë–ï–ù–ß–ú–ê–†–ö–ê")
        print("=" * 80)
        
        results = {}
        
        for model in models:
            print(f"\nüöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model}")
            results[model] = await self.benchmark_model_advanced(model, iterations)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        self._generate_advanced_report(results)
        
        return results
    
    def _generate_advanced_report(self, results: Dict[str, AdvancedSEOMetrics]):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
        print("\n" + "=" * 80)
        print("üèÜ –ü–†–û–î–í–ò–ù–£–¢–´–ô SEO-–û–¢–ß–ï–¢")
        print("=" * 80)
        
        # –¢–∞–±–ª–∏—Ü–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
        header = f"{'–ú–æ–¥–µ–ª—å':<25} {'–û–±—â–∏–π':<8} {'SEO':<8} {'–Ø–∑—ã–∫':<8} {'RAG':<8} {'–°–∫–æ—Ä.':<8}"
        print(header)
        print("-" * 75)
        
        for model_name, metrics in results.items():
            print(f"{model_name:<25} {metrics.overall_score:<8.1f} {metrics.seo_understanding:<8.1f} "
                  f"{metrics.language_quality_score:<8.1f} {metrics.rag_integration_quality:<8.1f} "
                  f"{metrics.avg_response_time:<8.2f}")
        
        # –õ–∏–¥–µ—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        print(f"\nüèÖ –õ–ò–î–ï–†–´ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
        
        best_overall = max(results.items(), key=lambda x: x[1].overall_score)
        print(f"üëë –õ—É—á—à–∏–π –æ–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_overall[0]} ({best_overall[1].overall_score:.1f})")
        
        best_seo = max(results.items(), key=lambda x: x[1].seo_understanding)
        print(f"üéØ –õ—É—á—à–µ–µ SEO-–ø–æ–Ω–∏–º–∞–Ω–∏–µ: {best_seo[0]} ({best_seo[1].seo_understanding:.1f})")
        
        best_lang = max(results.items(), key=lambda x: x[1].language_quality_score)
        print(f"‚úçÔ∏è –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —è–∑—ã–∫–∞: {best_lang[0]} ({best_lang[1].language_quality_score:.1f})")
        
        best_rag = max(results.items(), key=lambda x: x[1].rag_integration_quality)
        print(f"üß† –õ—É—á—à–∞—è RAG-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: {best_rag[0]} ({best_rag[1].rag_integration_quality:.1f})")
        
        fastest = min(results.items(), key=lambda x: x[1].avg_response_time)
        print(f"‚ö° –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è: {fastest[0]} ({fastest[1].avg_response_time:.2f}—Å)")

async def main():
    """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Docker URL –¥–ª—è backend
    benchmark = AdvancedSEOBenchmark(backend_url="http://localhost:8000")
    
    models_to_test = [
        "qwen2.5:7b-turbo",
        "qwen2.5:7b-instruct-turbo", 
        "qwen2.5:7b-instruct"
    ]
    
    results = await benchmark.compare_models_advanced(models_to_test, iterations=2)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"advanced_seo_benchmark_{timestamp}.json"
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump({k: asdict(v) for k, v in results.items()}, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

if __name__ == "__main__":
    asyncio.run(main()) 