#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–º–µ–Ω–∞ dagorod.ru
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Å—Ç—ã, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –∏ SEO –¥–∞–Ω–Ω—ã–µ
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Any, Set
import json
from datetime import datetime
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DomainIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–æ–º–µ–Ω–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è SEO –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.visited_urls: Set[str] = set()
        self.posts: List[Dict[str, Any]] = []
        self.internal_links: List[Dict[str, Any]] = []
        self.seo_data: Dict[str, Any] = {}
        
    async def index_domain(self) -> Dict[str, Any]:
        """–ü–æ–ª–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞"""
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–æ–º–µ–Ω–∞: {self.base_url}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            main_page = await self.fetch_page(self.base_url)
            if not main_page:
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º SEO –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            self.seo_data = await self.extract_seo_data(main_page, self.base_url)
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å—Ç—ã
            post_urls = await self.find_post_urls(main_page)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(post_urls)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç
            for url in post_urls[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                post_data = await self.index_post(url)
                if post_data:
                    self.posts.append(post_data)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            await self.analyze_internal_links()
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
            report = {
                "domain": self.domain,
                "base_url": self.base_url,
                "indexed_at": datetime.now().isoformat(),
                "seo_data": self.seo_data,
                "posts_count": len(self.posts),
                "posts": self.posts,
                "internal_links_count": len(self.internal_links),
                "internal_links": self.internal_links,
                "visited_urls_count": len(self.visited_urls)
            }
            
            logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(self.posts)} –ø–æ—Å—Ç–æ–≤")
            return report
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
            raise
    
    async def fetch_page(self, url: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                }
                async with session.get(url, headers=headers, timeout=30) as response:
                    if response.status == 200:
                        self.visited_urls.add(url)
                        return await response.text()
                    else:
                        logger.warning(f"–û—à–∏–±–∫–∞ {response.status} –¥–ª—è {url}")
                        return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {url}: {e}")
            return ""
    
    async def extract_seo_data(self, html: str, url: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ SEO –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ú–µ—Ç–∞-—Ç–µ–≥–∏
        title = soup.find('title')
        description = soup.find('meta', attrs={'name': 'description'})
        keywords = soup.find('meta', attrs={'name': 'keywords'})
        
        # Open Graph
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        og_description = soup.find('meta', attrs={'property': 'og:description'})
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        structured_data = []
        for script in soup.find_all('script', type='application/ld+json'):
            try:
                data = json.loads(script.string)
                structured_data.append(data)
            except:
                pass
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        headers = {}
        for i in range(1, 7):
            h_tags = soup.find_all(f'h{i}')
            headers[f'h{i}'] = [h.get_text(strip=True) for h in h_tags]
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src:
                images.append({
                    'src': urljoin(url, src),
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                })
        
        return {
            'url': url,
            'title': title.get_text(strip=True) if title else '',
            'meta_description': description.get('content', '') if description else '',
            'meta_keywords': keywords.get('content', '') if keywords else '',
            'og_title': og_title.get('content', '') if og_title else '',
            'og_description': og_description.get('content', '') if og_description else '',
            'og_image': og_image.get('content', '') if og_image else '',
            'structured_data': structured_data,
            'headers': headers,
            'images': images,
            'word_count': len(soup.get_text().split()),
            'links_count': len(soup.find_all('a'))
        }
    
    async def find_post_urls(self, html: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–æ—Å—Ç—ã"""
        soup = BeautifulSoup(html, 'html.parser')
        post_urls = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ—Å—Ç–∞–º–∏
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å—Å—ã–ª–∫–∞
                if urlparse(full_url).netloc == self.domain:
                    # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ—Å—Ç–æ–≤
                    if any(pattern in full_url.lower() for pattern in [
                        '/post/', '/article/', '/blog/', '/news/',
                        '/2024/', '/2023/', '/2022/', '/2021/',
                        '.html', '.php'
                    ]):
                        post_urls.append(full_url)
        
        return list(set(post_urls))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    async def index_post(self, url: str) -> Dict[str, Any]:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ—Å—Ç–∞"""
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ—Å—Ç: {url}")
        
        html = await self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º SEO –¥–∞–Ω–Ω—ã–µ
        seo_data = await self.extract_seo_data(html, url)
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content = ""
        content_selectors = [
            'article', '.post-content', '.entry-content', 
            '.content', '.main-content', '#content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(strip=True)
                break
        
        if not content:
            content = soup.get_text(strip=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        publish_date = ""
        date_selectors = [
            '.publish-date', '.post-date', '.entry-date',
            'time[datetime]', '.date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                publish_date = date_elem.get('datetime') or date_elem.get_text(strip=True)
                break
        
        # –ò—â–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –≤ –ø–æ—Å—Ç–µ
        internal_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                full_url = urljoin(url, href)
                if urlparse(full_url).netloc == self.domain:
                    internal_links.append({
                        'from_url': url,
                        'to_url': full_url,
                        'anchor_text': link.get_text(strip=True),
                        'title': link.get('title', '')
                    })
        
        return {
            'url': url,
            'title': seo_data['title'],
            'content': content[:1000] + "..." if len(content) > 1000 else content,
            'publish_date': publish_date,
            'word_count': seo_data['word_count'],
            'internal_links': internal_links,
            'seo_data': seo_data
        }
    
    async def analyze_internal_links(self):
        """–ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫"""
        link_map = {}
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
        for post in self.posts:
            for link in post.get('internal_links', []):
                to_url = link['to_url']
                if to_url not in link_map:
                    link_map[to_url] = []
                link_map[to_url].append({
                    'from_url': link['from_url'],
                    'anchor_text': link['anchor_text'],
                    'title': link['title']
                })
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        for to_url, links in link_map.items():
            self.internal_links.append({
                'target_url': to_url,
                'incoming_links': links,
                'incoming_links_count': len(links)
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
        self.internal_links.sort(key=lambda x: x['incoming_links_count'], reverse=True)

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    domain = "https://dagorod.ru"
    
    indexer = DomainIndexer(domain)
    
    try:
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–º–µ–Ω
        report = await indexer.index_domain()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        with open('dagorod_index.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {report['posts_count']}")
        print(f"üîó –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {report['internal_links_count']}")
        print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: dagorod_index.json")
        
        return report
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return None

if __name__ == "__main__":
    asyncio.run(main()) 