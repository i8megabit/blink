"""
üß† –ï–¥–∏–Ω—ã–π LLM-–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ reLink

–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–º RAG-–ø–æ–¥—Ö–æ–¥–µ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º –≤ —Å–µ—Ä–≤–∏—Å–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å Ollama.
"""

import asyncio
import aiohttp
import json
import logging
import platform
import psutil
import subprocess
import os
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import time
from contextlib import asynccontextmanager

from .config import settings
from .database import get_db
from .models import LLMRequest, LLMResponse, LLMEmbedding
from .cache import cache_manager
from .exceptions import LLMServiceError, OllamaConnectionError

logger = logging.getLogger(__name__)

class LLMServiceType(Enum):
    """–¢–∏–ø—ã LLM-—Å–µ—Ä–≤–∏—Å–æ–≤"""
    SEO_RECOMMENDATIONS = "seo_recommendations"
    DIAGRAM_GENERATION = "diagram_generation"
    CONTENT_ANALYSIS = "content_analysis"
    BENCHMARK_SERVICE = "benchmark_service"
    LLM_TUNING = "llm_tuning"

@dataclass
class SystemSpecs:
    """–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã"""
    platform: str
    architecture: str
    cpu_count: int
    memory_gb: float
    gpu_available: bool
    gpu_type: Optional[str] = None
    apple_silicon: bool = False
    m1_m2_m4: bool = False

@dataclass
class OptimizedConfig:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∏—Å—Ç–µ–º—ã"""
    model: str
    num_gpu: int
    num_thread: int
    batch_size: int
    f16_kv: bool
    temperature: float
    max_tokens: int
    context_length: int
    keep_alive: str
    request_timeout: int
    semaphore_limit: int
    cache_ttl: int

class SystemAnalyzer:
    """
    üîç –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
    - –¢–∏–ø –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ (Apple Silicon, Intel, AMD)
    - –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
    - –û–±—ä–µ–º –ø–∞–º—è—Ç–∏
    - –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Ollama
    """
    
    def __init__(self):
        self.specs: Optional[SystemSpecs] = None
        self.optimized_config: Optional[OptimizedConfig] = None
    
    async def analyze_system(self) -> SystemSpecs:
        """–ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π"""
        if self.specs:
            return self.specs
        
        platform_name = platform.system()
        architecture = platform.machine()
        cpu_count = psutil.cpu_count(logical=True)
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Apple Silicon
        apple_silicon = False
        m1_m2_m4 = False
        
        if platform_name == "Darwin" and "arm" in architecture.lower():
            apple_silicon = True
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
            try:
                result = subprocess.run(
                    ["sysctl", "-n", "machdep.cpu.brand_string"], 
                    capture_output=True, text=True
                )
                cpu_brand = result.stdout.lower()
                if any(x in cpu_brand for x in ["m1", "m2", "m3", "m4"]):
                    m1_m2_m4 = True
            except:
                pass
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GPU
        gpu_available = False
        gpu_type = None
        
        if apple_silicon:
            gpu_available = True
            gpu_type = "Apple Silicon GPU"
        else:
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ NVIDIA GPU
                result = subprocess.run(["nvidia-smi"], capture_output=True)
                if result.returncode == 0:
                    gpu_available = True
                    gpu_type = "NVIDIA"
                else:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ AMD GPU
                    result = subprocess.run(["rocm-smi"], capture_output=True)
                    if result.returncode == 0:
                        gpu_available = True
                        gpu_type = "AMD"
            except:
                pass
        
        self.specs = SystemSpecs(
            platform=platform_name,
            architecture=architecture,
            cpu_count=cpu_count,
            memory_gb=memory_gb,
            gpu_available=gpu_available,
            gpu_type=gpu_type,
            apple_silicon=apple_silicon,
            m1_m2_m4=m1_m2_m4
        )
        
        logger.info(f"üîç System analysis completed: {self.specs}")
        return self.specs
    
    async def optimize_config(self) -> OptimizedConfig:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–∏—Å—Ç–µ–º—ã"""
        if self.optimized_config:
            return self.optimized_config
        
        specs = await self.analyze_system()
        
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config = OptimizedConfig(
            model="qwen2.5:7b-instruct-turbo",
            num_gpu=0,
            num_thread=4,
            batch_size=512,
            f16_kv=True,
            temperature=0.7,
            max_tokens=2048,
            context_length=4096,
            keep_alive="2h",
            request_timeout=300,
            semaphore_limit=5,
            cache_ttl=3600
        )
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è Apple Silicon M1/M2/M4
        if specs.apple_silicon and specs.m1_m2_m4:
            config.num_gpu = 1
            config.num_thread = min(8, specs.cpu_count)
            config.batch_size = 1024
            config.f16_kv = True
            config.context_length = 8192
            config.semaphore_limit = 8
            logger.info("üçé Optimized for Apple Silicon M1/M2/M4")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –¥—Ä—É–≥–∏—Ö Apple Silicon
        elif specs.apple_silicon:
            config.num_gpu = 1
            config.num_thread = min(6, specs.cpu_count)
            config.batch_size = 768
            config.f16_kv = True
            config.context_length = 6144
            config.semaphore_limit = 6
            logger.info("üçé Optimized for Apple Silicon")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è NVIDIA GPU
        elif specs.gpu_available and specs.gpu_type == "NVIDIA":
            config.num_gpu = 1
            config.num_thread = min(6, specs.cpu_count)
            config.batch_size = 1024
            config.f16_kv = True
            config.context_length = 8192
            config.semaphore_limit = 6
            logger.info("üü¢ Optimized for NVIDIA GPU")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è AMD GPU
        elif specs.gpu_available and specs.gpu_type == "AMD":
            config.num_gpu = 1
            config.num_thread = min(6, specs.cpu_count)
            config.batch_size = 768
            config.f16_kv = True
            config.context_length = 6144
            config.semaphore_limit = 6
            logger.info("üî¥ Optimized for AMD GPU")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è CPU-only
        else:
            config.num_gpu = 0
            config.num_thread = min(8, specs.cpu_count)
            config.batch_size = 256
            config.f16_kv = False
            config.context_length = 4096
            config.semaphore_limit = 4
            logger.info("üíª Optimized for CPU-only")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ –ø–∞–º—è—Ç–∏
        if specs.memory_gb >= 32:
            config.context_length = min(config.context_length * 2, 16384)
            config.batch_size = min(config.batch_size * 1.5, 2048)
            config.semaphore_limit = min(config.semaphore_limit + 2, 10)
            logger.info("üíæ High memory optimization applied")
        elif specs.memory_gb < 8:
            config.context_length = min(config.context_length // 2, 2048)
            config.batch_size = min(config.batch_size // 2, 256)
            config.semaphore_limit = max(config.semaphore_limit - 2, 2)
            logger.info("üíæ Low memory optimization applied")
        
        self.optimized_config = config
        logger.info(f"‚öôÔ∏è Optimized config: {config}")
        return config
    
    async def get_environment_variables(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Ollama"""
        config = await self.optimize_config()
        
        env_vars = {
            "OLLAMA_HOST": "0.0.0.0",
            "OLLAMA_ORIGINS": "*",
            "OLLAMA_KEEP_ALIVE": config.keep_alive,
            "OLLAMA_CONTEXT_LENGTH": str(config.context_length),
            "OLLAMA_BATCH_SIZE": str(config.batch_size),
            "OLLAMA_NUM_PARALLEL": str(config.semaphore_limit),
            "REQUEST_TIMEOUT": str(config.request_timeout)
        }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Apple Silicon
        specs = await self.analyze_system()
        if specs.apple_silicon:
            env_vars.update({
                "OLLAMA_METAL": "1",
                "OLLAMA_FLASH_ATTENTION": "1",
                "OLLAMA_KV_CACHE_TYPE": "q8_0",
                "OLLAMA_MEM_FRACTION": "0.9"
            })
        
        return env_vars

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
system_analyzer = SystemAnalyzer()

@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–∑–∞–ø—Ä–æ—Å–∞"""
    service_type: LLMServiceType
    prompt: str
    context: Optional[Dict[str, Any]] = None
    model: str = "qwen2.5:7b-instruct-turbo"  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è Apple Silicon
    temperature: float = 0.7
    max_tokens: int = 2048
    use_rag: bool = True
    cache_ttl: int = 3600  # 1 —á–∞—Å
    priority: int = 1  # 1-10, –≥–¥–µ 10 - –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–æ—Ç–≤–µ—Ç–∞"""
    content: str
    service_type: LLMServiceType
    model_used: str
    tokens_used: int
    response_time: float
    cached: bool = False
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class LLMRouter:
    """
    üß† –ï–¥–∏–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä LLM —Å RAG-–ø–æ–¥—Ö–æ–¥–æ–º
    
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —É—Å–ø–µ—à–Ω–æ–º –æ–ø—ã—Ç–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
    - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    - RAG —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ fallback
    - –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore: Optional[asyncio.Semaphore] = None
        self.request_queue = asyncio.Queue()
        self.processing = False
        self.optimized_config: Optional[OptimizedConfig] = None
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "cached_responses": 0,
            "avg_response_time": 0.0,
            "system_specs": None,
            "optimization_applied": False
        }
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"""
        await self.start()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        await self.stop()
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞ —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if not self.session:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.optimized_config = await system_analyzer.optimize_config()
            specs = await system_analyzer.analyze_system()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats["system_specs"] = {
                "platform": specs.platform,
                "architecture": specs.architecture,
                "cpu_count": specs.cpu_count,
                "memory_gb": specs.memory_gb,
                "gpu_available": specs.gpu_available,
                "gpu_type": specs.gpu_type,
                "apple_silicon": specs.apple_silicon,
                "m1_m2_m4": specs.m1_m2_m4
            }
            self.stats["optimization_applied"] = True
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞—Ñ–æ—Ä–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º
            self.semaphore = asyncio.Semaphore(self.optimized_config.semaphore_limit)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
            timeout = aiohttp.ClientTimeout(total=self.optimized_config.request_timeout)
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers={"Content-Type": "application/json"}
            )
            
            logger.info(f"üöÄ LLM Router started with optimized config: {self.optimized_config}")
            logger.info(f"üîç System specs: {self.stats['system_specs']}")
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("üõë LLM Router stopped")
    
    def _generate_cache_key(self, request: LLMRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        content = f"{request.service_type.value}:{request.prompt}:{request.model}:{request.temperature}"
        if request.context:
            content += f":{json.dumps(request.context, sort_keys=True)}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    async def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            cached_data = await cache_manager.get(f"llm:{cache_key}")
            if cached_data:
                logger.info(f"üì¶ Cache hit for {cache_key[:16]}...")
                return LLMResponse(**cached_data, cached=True)
        except Exception as e:
            logger.warning(f"Cache error: {e}")
        return None
    
    async def _cache_response(self, cache_key: str, response: LLMResponse, ttl: int):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        try:
            cache_data = {
                "content": response.content,
                "service_type": response.service_type.value,
                "model_used": response.model_used,
                "tokens_used": response.tokens_used,
                "response_time": response.response_time,
                "metadata": response.metadata
            }
            await cache_manager.set(f"llm:{cache_key}", cache_data, ttl)
            logger.info(f"üíæ Cached response for {cache_key[:16]}...")
        except Exception as e:
            logger.warning(f"Cache error: {e}")
    
    async def _generate_rag_context(self, request: LLMRequest) -> str:
        """
        üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è RAG-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —É—Å–ø–µ—à–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
        - –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        - –û–±–æ–≥–∞—â–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        - –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤
        """
        if not request.use_rag:
            return request.prompt
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            embedding = await self._get_embedding(request.prompt)
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
            relevant_knowledge = await self._search_knowledge_base(
                embedding, 
                request.service_type,
                limit=3
            )
            
            if relevant_knowledge:
                context_parts = [request.prompt]
                context_parts.append("\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
                for knowledge in relevant_knowledge:
                    context_parts.append(f"- {knowledge['content']}")
                
                enhanced_prompt = "\n".join(context_parts)
                logger.info(f"üß† RAG enhanced prompt for {request.service_type.value}")
                return enhanced_prompt
            
        except Exception as e:
            logger.warning(f"RAG error: {e}")
        
        return request.prompt
    
    async def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            async with self.semaphore:
                async with self.session.post(
                    f"{settings.OLLAMA_URL}/api/embeddings",
                    json={"model": "qwen2.5:7b-instruct-turbo", "prompt": text}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("embedding", [])
                    else:
                        logger.error(f"Embedding error: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Embedding request failed: {e}")
            return []
    
    async def _search_knowledge_base(
        self, 
        embedding: List[float], 
        service_type: LLMServiceType,
        limit: int = 3
    ) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î (Chroma, Pinecone, etc.)
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
        except Exception as e:
            logger.error(f"Knowledge base search failed: {e}")
            return []
    
    async def _make_ollama_request(self, request: LLMRequest) -> LLMResponse:
        """
        üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ Apple Silicon M4
        
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
        - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        - –¢–∞–π–º–∞—É—Ç—ã –∏ retry
        - Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        start_time = time.time()
        
        try:
            async with self.semaphore:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å RAG
                enhanced_prompt = await self._generate_rag_context(request)
                
                # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama —Å Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                ollama_request = {
                    "model": request.model,
                    "prompt": enhanced_prompt,
                    "stream": False,
                    "options": {
                        "temperature": request.temperature,
                        "num_predict": request.max_tokens,
                        # üöÄ APPLE SILICON M4 –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò
                        "num_gpu": 1,                    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
                        "num_thread": 8,                 # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è M4
                        "num_ctx": 4096,                 # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                        "batch_size": 512,               # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        "f16_kv": True,                  # 16-–±–∏—Ç–Ω—ã–µ –∫–ª—é—á–∏-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        "use_mmap": True,                # Memory mapping –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                        "use_mlock": True,               # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –ø–∞–º—è—Ç–∏
                        "rope_freq_base": 10000,         # RoPE –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞
                        "rope_freq_scale": 0.5,          # RoPE –º–∞—Å—à—Ç–∞–± —á–∞—Å—Ç–æ—Ç—ã
                        "top_p": 0.9,                    # Top-p sampling
                        "top_k": 40,                     # Top-k sampling
                        "repeat_penalty": 1.1,           # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                        "seed": 42                       # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
                    }
                }
                
                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
                async with self.session.post(
                    f"{settings.OLLAMA_URL}/api/generate",
                    json=ollama_request
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        response_time = time.time() - start_time
                        
                        return LLMResponse(
                            content=data.get("response", ""),
                            service_type=request.service_type,
                            model_used=request.model,
                            tokens_used=data.get("eval_count", 0),
                            response_time=response_time,
                            metadata={
                                "prompt_tokens": data.get("prompt_eval_count", 0),
                                "total_duration": data.get("total_duration", 0),
                                "apple_silicon_optimized": True,
                                "gpu_used": True,
                                "batch_size": 512,
                                "context_length": 4096
                            }
                        )
                    else:
                        error_text = await response.text()
                        raise OllamaConnectionError(f"Ollama error {response.status}: {error_text}")
                        
        except asyncio.TimeoutError:
            raise LLMServiceError("Request timeout")
        except Exception as e:
            raise LLMServiceError(f"Request failed: {e}")
    
    async def process_request(self, request: LLMRequest) -> LLMResponse:
        """
        üéØ –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM-–∑–∞–ø—Ä–æ—Å–æ–≤
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π pipeline:
        1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        2. RAG-–æ–±–æ–≥–∞—â–µ–Ω–∏–µ
        3. –ó–∞–ø—Ä–æ—Å –∫ Ollama
        4. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        5. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        """
        self.stats["total_requests"] += 1
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞
            cache_key = self._generate_cache_key(request)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            if request.cache_ttl > 0:
                cached_response = await self._get_cached_response(cache_key)
                if cached_response:
                    self.stats["cached_responses"] += 1
                    return cached_response
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama
            response = await self._make_ollama_request(request)
            
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if request.cache_ttl > 0:
                await self._cache_response(cache_key, response, request.cache_ttl)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats["successful_requests"] += 1
            self.stats["avg_response_time"] = (
                (self.stats["avg_response_time"] * (self.stats["successful_requests"] - 1) + 
                 response.response_time) / self.stats["successful_requests"]
            )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            logger.info(
                f"‚úÖ {request.service_type.value} completed in {response.response_time:.2f}s "
                f"(tokens: {response.tokens_used})"
            )
            
            return response
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"‚ùå {request.service_type.value} failed: {e}")
            
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            return LLMResponse(
                content=f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                service_type=request.service_type,
                model_used=request.model,
                tokens_used=0,
                response_time=time.time() - start_time,
                error=str(e)
            )
    
    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        return {
            **self.stats,
            "active_connections": self.semaphore._value,
            "queue_size": self.request_queue.qsize() if hasattr(self.request_queue, 'qsize') else 0
        }
    
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Ollama"""
        try:
            async with self.session.get(f"{settings.OLLAMA_URL}/api/tags") as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞
llm_router = LLMRouter()

# –£—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def generate_seo_recommendations(prompt: str, context: Optional[Dict] = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.SEO_RECOMMENDATIONS,
        prompt=prompt,
        context=context,
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.6,  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è SEO –∑–∞–¥–∞—á
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def generate_diagram(prompt: str, diagram_type: str = "architecture") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SVG –¥–∏–∞–≥—Ä–∞–º–º—ã —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.DIAGRAM_GENERATION,
        prompt=f"–°–æ–∑–¥–∞–π SVG –¥–∏–∞–≥—Ä–∞–º–º—É —Ç–∏–ø–∞ '{diagram_type}': {prompt}",
        context={"diagram_type": diagram_type},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.8,  # –í—ã—Å–æ–∫–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º
        max_tokens=4096   # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è SVG
    )
    response = await llm_router.process_request(request)
    return response.content

async def analyze_content(content: str, analysis_type: str = "general") -> str:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.CONTENT_ANALYSIS,
        prompt=f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {analysis_type}): {content}",
        context={"analysis_type": analysis_type},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.5,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def run_benchmark(benchmark_type: str, parameters: Dict[str, Any]) -> str:
    """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.BENCHMARK_SERVICE,
        prompt=f"–í—ã–ø–æ–ª–Ω–∏ –±–µ–Ω—á–º–∞—Ä–∫ —Ç–∏–ø–∞ '{benchmark_type}' —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {json.dumps(parameters)}",
        context={"benchmark_type": benchmark_type, "parameters": parameters},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def tune_llm_model(model_config: Dict[str, Any], tuning_params: Dict[str, Any]) -> str:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.LLM_TUNING,
        prompt=f"–ù–∞—Å—Ç—Ä–æ–π –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π: {json.dumps(model_config)} –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {json.dumps(tuning_params)}",
        context={"model_config": model_config, "tuning_params": tuning_params},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.4,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content 