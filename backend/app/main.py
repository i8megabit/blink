"""FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ reLink."""

from __future__ import annotations

import asyncio
import json
import os
import re
import logging
import secrets
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple

import chromadb
import httpx
import nltk
import numpy as np
import ollama
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, Request, Response, Depends, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from pydantic import BaseModel, ValidationError
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import (
    DateTime, Float, ForeignKey, Index, Integer, JSON, String, Text,
    func, select, update, delete
)
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship, selectinload
from jose import JWTError, jwt

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('app.log')
    ]
)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è JWT
SECRET_KEY = os.getenv("SECRET_KEY", secrets.token_urlsafe(32))
ALGORITHM = "HS256"
security = HTTPBearer()

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª–∏
from .config import settings, get_settings
from .exceptions import (
    RelinkBaseException, ErrorHandler, ErrorResponse,
    ValidationException, AuthenticationException, AuthorizationException,
    NotFoundException, DatabaseException, OllamaException,
    raise_not_found, raise_validation_error, raise_authentication_error,
    raise_authorization_error, raise_database_error, raise_ollama_error
)
from .monitoring import (
    logger, metrics_collector, performance_monitor, 
    get_metrics, get_health_status, monitor_operation,
    MonitoringMiddleware
)
from .cache import (
    cache_manager, cache_result, invalidate_cache,
    SEOCache, UserCache
)
from .validation import (
    UserRegistrationModel, UserLoginModel, DomainAnalysisModel,
    SEORecommendationModel, ExportModel, PaginationModel,
    validate_request, validate_response, validate_and_clean_data
)
from .auth import (
    get_current_user, create_access_token, get_password_hash, verify_password,
    User, UserCreate, UserResponse, Token, TokenData,
    UserRegistrationRequest, UserLoginRequest
)
from .database import get_db, engine
from .models import Base, User, Domain, WordPressPost, AnalysisHistory, Diagram, DiagramEmbedding
from .models import (
    TestRequest, TestResponse, TestSuiteRequest, TestSuiteResponse, TestExecutionResponse,
    TestType, TestStatus, TestPriority, TestEnvironment, utc_now
)
from .llm_router import system_analyzer, llm_router
from .diagram_service import DiagramService, DiagramGenerationRequest
from .testing_service import testing_service, router as testing_router
from .api.auth import router as auth_router
from .api.optimization_router import router as optimization_router
from .llm_integration import get_llm_integration_service
from .database_service import get_database_rag_service

# –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
RUSSIAN_STOP_WORDS = set(stopwords.words('russian'))

# üîí –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –°–ï–ú–ê–§–û–† –î–õ–Ø –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –ù–ê–ì–†–£–ó–ö–ò –ù–ê OLLAMA
OLLAMA_SEMAPHORE = asyncio.Semaphore(1)

@dataclass
class SemanticEntity:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—É—â–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM."""
    entity_type: str
    value: str
    confidence: float
    context: str

@dataclass
class ThematicCluster:
    """–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä —Å—Ç–∞—Ç–µ–π."""
    cluster_id: str
    theme: str
    keywords: List[str]
    articles_count: int
    semantic_density: float

@dataclass
class AIThought:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—ã—Å–ª—å –ò–ò —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
    thought_id: str
    stage: str  # analyzing, connecting, evaluating, optimizing
    content: str
    confidence: float
    semantic_weight: float
    related_concepts: List[str]
    reasoning_chain: List[str]
    timestamp: datetime
    
@dataclass
class SemanticConnection:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏."""
    source_concept: str
    target_concept: str
    connection_type: str  # semantic, causal, hierarchical, temporal
    strength: float
    evidence: List[str]
    context_keywords: Set[str]

class WebSocketManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""

    def __init__(self) -> None:
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, client_id: str) -> None:
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞."""
        await websocket.accept()
        self.active_connections[client_id] = websocket
        logger.info(f"WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω: {client_id}")

    def disconnect(self, client_id: str) -> None:
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            logger.info(f"WebSocket –æ—Ç–∫–ª—é—á–µ–Ω: {client_id}")

    async def send_progress(self, client_id: str, message: dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–ª–∏–µ–Ω—Ç—É."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json(message)
                logger.debug(f"–ü—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω {client_id}: {message}")
            except Exception as e:
                logger.error(f"Error sending progress to {client_id}: {e}")

    async def send_error(self, client_id: str, error: str, details: str = "") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ—à–∏–±–∫–∏ –∫–ª–∏–µ–Ω—Ç—É."""
        await self.send_progress(client_id, {
            "type": "error",
            "message": error,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })

    async def send_step(self, client_id: str, step: str, current: int, total: int, details: str = "") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ."""
        await self.send_progress(client_id, {
            "type": "progress",
            "step": step,
            "current": current,
            "total": total,
            "percentage": round((current / total) * 100) if total > 0 else 0,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })

    async def send_ollama_info(self, client_id: str, info: dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞–±–æ—Ç–µ Ollama."""
        await self.send_progress(client_id, {
            "type": "ollama",
            "info": info,
            "timestamp": datetime.now().isoformat()
        })

    async def send_ai_thinking(self, client_id: str, thought: str, thinking_stage: str = "analyzing", emoji: str = "ü§î") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ '–º—ã—Å–ª–µ–π' –ò–ò –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json({
                    "type": "ai_thinking",
                    "thought": thought,
                    "thinking_stage": thinking_stage,
                    "emoji": emoji,
                    "timestamp": datetime.now().isoformat()
                })
                logger.debug(f"–ú—ã—Å–ª—å –ò–ò –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ {client_id}: {thought[:50]}...")
            except Exception as e:
                logger.error(f"Error sending AI thinking to {client_id}: {e}")
    
    async def send_enhanced_ai_thinking(self, client_id: str, ai_thought: AIThought) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º—ã—Å–ª–µ–π –ò–ò —Å –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json({
                    "type": "enhanced_ai_thinking",
                    "thought_id": ai_thought.thought_id,
                    "stage": ai_thought.stage,
                    "content": ai_thought.content,
                    "confidence": ai_thought.confidence,
                    "semantic_weight": ai_thought.semantic_weight,
                    "related_concepts": ai_thought.related_concepts,
                    "reasoning_chain": ai_thought.reasoning_chain,
                    "timestamp": ai_thought.timestamp.isoformat()
                })
                logger.debug(f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –º—ã—Å–ª—å –ò–ò –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ {client_id}: {ai_thought.content[:50]}...")
            except Exception as e:
                logger.error(f"Error sending enhanced AI thinking to {client_id}: {e}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä WebSocket
websocket_manager = WebSocketManager()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title=settings.api.title,
    description=settings.api.description,
    version=settings.api.version,
    debug=settings.api.debug,
    docs_url=settings.api.docs_url,
    redoc_url=settings.api.redoc_url
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.api.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ middleware –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
app.add_middleware(MonitoringMiddleware)

# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
async def get_current_user_simple(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> dict:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±–µ–∑ –ë–î."""
    token = credentials.credentials
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Could not validate credentials"
            )
        return {"username": username, "id": 1}  # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials"
        )

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã
def initialize_rag_system() -> None:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã —Å ChromaDB."""
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞ ChromaDB
        chroma_client = chromadb.Client()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        collection = chroma_client.create_collection(
            name="relink_documents",
            metadata={"description": "–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã reLink"}
        )
        
        logger.info("RAG —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å ChromaDB")
        return collection
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG —Å–∏—Å—Ç–µ–º—ã: {e}")
        return None

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è RAG –∫–æ–ª–ª–µ–∫—Ü–∏–∏
rag_collection = initialize_rag_system()

class AdvancedRAGManager:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä RAG —Å–∏—Å—Ç–µ–º—ã."""
    
    def __init__(self) -> None:
        self.collection = rag_collection
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=RUSSIAN_STOP_WORDS,
            ngram_range=(1, 2)
        )
    
    async def create_semantic_knowledge_base(self, domain, posts, client_id=None):
        # –ü—Ä–æ–∫—Å–∏—Ä—É–µ–º –≤—ã–∑–æ–≤ –∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—É –º—ã—Å–ª–µ–π
        return await generate_ai_thoughts_for_domain(domain, posts, client_id)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä RAG
rag_manager = AdvancedRAGManager()

# Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
class RecommendRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""
    text: str

class WPRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ WordPress-—Å–∞–π—Ç–∞."""
    domain: str
    client_id: Optional[str] = None
    comprehensive: Optional[bool] = False

class BenchmarkRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    name: str
    description: Optional[str] = None
    benchmark_type: str = "seo_advanced"
    models: List[str] = []
    iterations: int = 3
    client_id: Optional[str] = None

class ModelConfigRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏."""
    llm_model: str
    display_name: Optional[str] = None
    description: Optional[str] = None
    default_parameters: Optional[dict] = None
    seo_optimized_params: Optional[dict] = None
    benchmark_params: Optional[dict] = None

class SEOAnalysisResult(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç SEO –∞–Ω–∞–ª–∏–∑–∞."""
    domain: str
    analysis_date: datetime
    score: float
    recommendations: List[dict]
    metrics: dict
    status: str

class DomainAnalysisRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞."""
    domain: str
    comprehensive: Optional[bool] = False
    client_id: Optional[str] = None

class CompetitorAnalysisRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤."""
    domain: str
    competitors: List[str]

class AnalysisHistoryRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤."""
    limit: int = 10
    offset: int = 0

class ExportRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö."""
    format: str  # json, csv, pdf
    analysis_ids: List[int]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è Ollama
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct-turbo")

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
OPTIMAL_CONTEXT_SIZE = 3072
OPTIMAL_PREDICTION_SIZE = 800
OPTIMAL_TEMPERATURE = 0.3
OPTIMAL_TOP_P = 0.85
OPTIMAL_TOP_K = 50
OPTIMAL_REPEAT_PENALTY = 1.08

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—ã—Å–ª–µ–π –ò–ò
async def generate_ai_thoughts_for_domain(domain: str, posts: List[dict], client_id: str = None) -> List[AIThought]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º—ã—Å–ª–µ–π –ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞."""
    thoughts = []
    
    if client_id:
        await websocket_manager.send_ai_thinking(
            client_id, 
            f"–ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞ {domain}...", 
            "analyzing", 
            "üîç"
        )
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_thought = AIThought(
        thought_id=f"content_analysis_{domain}",
        stage="analyzing",
        content=f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(posts)} —Å—Ç–∞—Ç–µ–π –Ω–∞ –¥–æ–º–µ–Ω–µ {domain}",
        confidence=0.8,
        semantic_weight=0.7,
        related_concepts=["–∫–æ–Ω—Ç–µ–Ω—Ç", "–∞–Ω–∞–ª–∏–∑", "—Å–µ–º–∞–Ω—Ç–∏–∫–∞"],
        reasoning_chain=["–ø–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–µ–π", "–æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"],
        timestamp=datetime.utcnow()
    )
    thoughts.append(content_thought)
    
    if client_id:
        await websocket_manager.send_enhanced_ai_thinking(client_id, content_thought)
    
    # –ü–æ–∏—Å–∫ —Å–≤—è–∑–µ–π
    connection_thought = AIThought(
        thought_id=f"connection_search_{domain}",
        stage="connecting",
        content="–ò—â—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏",
        confidence=0.9,
        semantic_weight=0.8,
        related_concepts=["—Å–≤—è–∑–∏", "—Å–µ–º–∞–Ω—Ç–∏–∫–∞", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"],
        reasoning_chain=["–∞–Ω–∞–ª–∏–∑ —Ç–µ–º", "–ø–æ–∏—Å–∫ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π"],
        timestamp=datetime.utcnow()
    )
    thoughts.append(connection_thought)
    
    if client_id:
        await websocket_manager.send_enhanced_ai_thinking(client_id, connection_thought)
    
    return thoughts

# API endpoints
@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint."""
    return {"message": "reLink SEO Platform v4.1.1.022 –∑–∞–ø—É—â–µ–Ω!"}

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    return {"status": "healthy", "version": settings.api.version}

@app.get("/api/v1/health")
async def api_health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API."""
    return {"status": "healthy", "version": settings.api.version}

@app.get("/api/v1/version")
async def get_version():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    try:
        version_file = Path("VERSION")
        if version_file.exists():
            with open(version_file, 'r', encoding='utf-8') as f:
                version = f.read().strip()
        else:
            version = settings.api.version
        
        return {
            "version": version,
            "buildDate": datetime.now().strftime('%Y-%m-%d'),
            "commitHash": os.getenv("GIT_COMMIT_HASH", ""),
            "environment": os.getenv("ENVIRONMENT", "development")
        }
    except Exception as e:
        return {
            "version": settings.api.version,
            "buildDate": datetime.now().strftime('%Y-%m-%d'),
            "error": str(e)
        }

@app.get("/api/v1/settings")
async def get_settings():
    """–ó–∞–≥–ª—É—à–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞."""
    return {
        "theme": "light",
        "language": "ru",
        "features": {
            "ai_recommendations": True,
            "advanced_benchmark": True,
            "notifications": True,
            "export": True
        },
        "version": settings.api.version
    }

@app.get("/api/v1/ollama_status")
async def get_ollama_status():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama."""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://ollama:11434/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model.get("name", "") for model in models]
                
                return {
                    "ready_for_work": len(model_names) > 0,
                    "server_available": True,
                    "model_loaded": len(model_names) > 0,
                    "message": f"–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ ({len(model_names)} –º–æ–¥–µ–ª–µ–π)" if model_names else "–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã",
                    "status": "available",
                    "connection": "connected",
                    "models_count": len(model_names),
                    "available_models": model_names,
                    "timestamp": datetime.now().isoformat(),
                    "last_check": datetime.now().isoformat()
                }
            else:
                return {
                    "ready_for_work": False,
                    "server_available": False,
                    "model_loaded": False,
                    "message": f"Ollama –æ—Ç–≤–µ—Ç–∏–ª —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º {response.status_code}",
                    "status": "error",
                    "connection": "disconnected",
                    "models_count": 0,
                    "available_models": [],
                    "timestamp": datetime.now().isoformat(),
                    "last_check": datetime.now().isoformat()
                }
    except Exception as e:
        return {
            "ready_for_work": False,
            "server_available": False,
            "model_loaded": False,
            "message": f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {str(e)}",
            "status": "unavailable",
            "connection": "disconnected",
            "models_count": 0,
            "available_models": [],
            "timestamp": datetime.now().isoformat(),
            "last_check": datetime.now().isoformat()
        }

@app.get("/api/v1/domains")
async def get_domains():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–º–µ–Ω–æ–≤."""
    try:
        async with engine.begin() as conn:
            result = await conn.execute(select(Domain))
            domains = result.scalars().all()
            return [
                {
                    "id": domain.id,
                    "name": domain.name,
                    "display_name": domain.display_name,
                    "description": domain.description,
                    "total_posts": domain.total_posts,
                    "total_analyses": domain.total_analyses,
                    "last_analysis_at": domain.last_analysis_at.isoformat() if domain.last_analysis_at else None
                }
                for domain in domains
            ]
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/v1/analysis_history")
async def get_analysis_history():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤."""
    try:
        async with engine.begin() as conn:
            result = await conn.execute(select(AnalysisHistory).order_by(AnalysisHistory.created_at.desc()))
            histories = result.scalars().all()
            return [
                {
                    "id": history.id,
                    "domain_id": history.domain_id,
                    "posts_analyzed": history.posts_analyzed,
                    "connections_found": history.connections_found,
                    "recommendations_generated": history.recommendations_generated,
                    "created_at": history.created_at.isoformat(),
                    "completed_at": history.completed_at.isoformat() if history.completed_at else None
                }
                for history in histories
            ]
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/v1/benchmarks")
async def get_benchmarks():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""
    try:
        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
        return [
            {
                "id": 1,
                "name": "SEO Basic Benchmark",
                "description": "–ë–∞–∑–æ–≤—ã–π SEO –±–µ–Ω—á–º–∞—Ä–∫",
                "benchmark_type": "seo_basic",
                "status": "completed",
                "overall_score": 85.5,
                "created_at": datetime.now().isoformat()
            }
        ]
    except Exception as e:
        return {"error": str(e)}

@app.get("/metrics")
async def get_metrics():
    """Endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ Prometheus"""
    return await get_metrics()

@app.get("/api/v1/monitoring/health")
async def get_monitoring_health():
    """Endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º"""
    return await get_health_status()

@app.get("/api/v1/monitoring/stats")
async def get_monitoring_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    try:
        return {
            "uptime": "99.9%",
            "response_time": "150ms",
            "error_rate": "0.1%",
            "active_connections": 10,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")

@app.get("/api/v1/optimization/report")
async def get_optimization_report():
    """
    üß† –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    - –ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    - –ò—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ LLM
    """
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ—Ç SystemAnalyzer
        report = await system_analyzer.get_optimization_report()
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        enhanced_report = {
            **report,
            "optimization_status": {
                "llm_recommendations_applied": True,
                "adaptive_optimization_active": True,
                "performance_monitoring": True,
                "knowledge_base_entries": len(system_analyzer.knowledge_base),
                "last_optimization": datetime.now().isoformat()
            },
            "system_insights": {
                "apple_silicon_detected": report["system_specs"]["apple_silicon"],
                "gpu_acceleration_available": report["system_specs"]["gpu_available"],
                "memory_optimization": "high" if report["system_specs"]["memory_gb"] >= 16 else "medium",
                "cpu_utilization_optimal": report["system_specs"]["cpu_count"] >= 6
            },
            "performance_metrics": {
                "avg_response_time": report["performance_history"].get("avg_response_time", 0),
                "success_rate": report["performance_history"].get("success_rate", 1.0),
                "total_requests_processed": report["performance_history"].get("total_records", 0),
                "optimization_effectiveness": "excellent" if report["performance_history"].get("avg_response_time", 0) < 2.0 else "good"
            },
            "llm_insights": {
                "used_model": report["optimized_config"]["model"],
                "temperature_optimized": report["optimized_config"]["temperature"],
                "context_length_optimized": report["optimized_config"]["context_length"],
                "batch_size_optimized": report["optimized_config"]["batch_size"]
            }
        }
        
        return enhanced_report
        
    except Exception as e:
        logger.error(f"Error getting optimization report: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {str(e)}"
        )

@app.get("/api/v1/optimization/environment")
async def get_optimization_environment():
    """
    üîß –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Ollama
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Ollama —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
    """
    try:
        env_vars = await system_analyzer.get_environment_variables()
        
        return {
            "environment_variables": env_vars,
            "optimization_applied": True,
            "recommended_ollama_command": f"OLLAMA_HOST={env_vars['OLLAMA_HOST']} OLLAMA_ORIGINS={env_vars['OLLAMA_ORIGINS']} ollama serve",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting environment variables: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è: {str(e)}"
        )

@app.post("/api/v1/optimization/trigger")
async def trigger_optimization():
    """
    üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    
    –°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç
    –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
    try:
        # –°–±—Ä–æ—Å —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        system_analyzer.optimized_config = None
        
        # –ó–∞–ø—É—Å–∫ –Ω–æ–≤–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        new_config = await system_analyzer.optimize_config()
        
        return {
            "message": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–∞",
            "new_config": {
                "used_model": new_config.model,
                "num_gpu": new_config.num_gpu,
                "num_thread": new_config.num_thread,
                "batch_size": new_config.batch_size,
                "context_length": new_config.context_length,
                "semaphore_limit": new_config.semaphore_limit
            },
            "optimization_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error triggering optimization: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {str(e)}"
        )

@app.get("/api/v1/cache/stats")
async def get_cache_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
    try:
        return {
            "memory_cache_size": 0,
            "redis_cache_size": 0,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞")

@app.post("/api/v1/cache/clear")
async def clear_cache():
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
    try:
        return {"success": True, "message": "–ö—ç—à –æ—á–∏—â–µ–Ω"}
    except Exception as e:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞")

@app.delete("/api/v1/cache/{pattern}")
async def clear_cache_pattern(pattern: str):
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
    try:
        return {"success": True, "deleted_count": 0, "pattern": pattern}
    except Exception as e:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞")

# Endpoints –¥–ª—è SEO –∞–Ω–∞–ª–∏–∑–∞ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
@app.post("/api/v1/seo/analyze", response_model=SEOAnalysisResult)
async def analyze_domain(
    request_data: DomainAnalysisRequest,
    current_user: dict = Depends(get_current_user_simple)
):
    """–ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    try:
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞: {request_data.domain}")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞
        analysis_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–º–µ–Ω {request_data.domain} –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
        
        –ï—Å–ª–∏ —ç—Ç–æ —Å–∞–π—Ç –æ —Å–∞–¥–æ–≤–æ–¥—Å—Ç–≤–µ –∏ –æ–≥–æ—Ä–æ–¥–Ω–∏—á–µ—Å—Ç–≤–µ, —Å—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞:
        - –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–∫–∞—Ö –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏ –æ —Ä–∞—Å—Ç–µ–Ω–∏—è—Ö
        - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Å–∞–¥–æ–≤–æ–¥—Å—Ç–≤—É
        - –£–ª—É—á—à–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞
        
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏.
        """
        
        # –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ Ollama
        import httpx
        import time
        
        start_time = time.time()
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "qwen2.5:7b-instruct",
                    "prompt": analysis_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 1000
                    }
                },
                timeout=60.0
            )
            
            if response.status_code == 200:
                llm_response = response.json()
                response_text = llm_response.get("response", "")
                response_time = time.time() - start_time
                
                # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM
                recommendations = _parse_llm_recommendations(response_text)
                
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
                analysis_result = SEOAnalysisResult(
                    domain=request_data.domain,
                    analysis_date=datetime.utcnow(),
                    score=78.5,  # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä
                    recommendations=recommendations,
                    metrics={
                        "total_posts": 45,
                        "internal_links": 23,
                        "semantic_density": 0.72,
                        "avg_content_length": 1200,
                        "keyword_diversity": 0.68,
                        "llm_model_used": "qwen2.5:7b-instruct",
                        "tokens_used": len(response_text.split()),
                        "response_time": response_time,
                        "rag_enhanced": False
                    },
                    status="completed"
                )
                
                logger.info(f"–ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞ {request_data.domain} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return analysis_result
            else:
                raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞ {request_data.domain}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞: {str(e)}")

def _parse_llm_recommendations(llm_response: str) -> List[dict]:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
    recommendations = []
    
    # –ë–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞
    default_recommendations = [
        {
            "type": "internal_linking",
            "priority": "high",
            "description": "–î–æ–±–∞–≤–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏ –æ —Å–∞–¥–æ–≤–æ–¥—Å—Ç–≤–µ –∏ –æ–≥–æ—Ä–æ–¥–Ω–∏—á–µ—Å—Ç–≤–µ"
        },
        {
            "type": "content_optimization",
            "priority": "medium",
            "description": "–£–ª—É—á—à–∏—Ç—å –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–µ"
        },
        {
            "type": "semantic_clustering",
            "priority": "medium",
            "description": "–°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∞—Ç—å–∏ –ø–æ —Ç–µ–º–∞–º: –æ–≤–æ—â–∏, —Ñ—Ä—É–∫—Ç—ã, —Ü–≤–µ—Ç—ã, –ª–∞–Ω–¥—à–∞—Ñ—Ç–Ω—ã–π –¥–∏–∑–∞–π–Ω"
        }
    ]
    
    try:
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        response_lower = llm_response.lower()
        
        if "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏" in response_lower or "internal links" in response_lower:
            recommendations.append({
                "type": "internal_linking",
                "priority": "high",
                "description": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏"
            })
        
        if "—Å–µ–º–∞–Ω—Ç–∏–∫" in response_lower or "semantic" in response_lower:
            recommendations.append({
                "type": "semantic_optimization",
                "priority": "medium",
                "description": "–£–ª—É—á—à–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
            })
        
        if "–∫–æ–Ω—Ç–µ–Ω—Ç" in response_lower or "content" in response_lower:
            recommendations.append({
                "type": "content_quality",
                "priority": "medium",
                "description": "–ü–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
            })
        
        if "–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞" in response_lower or "keywords" in response_lower:
            recommendations.append({
                "type": "keyword_optimization",
                "priority": "high",
                "description": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"
            })
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ
        if not recommendations:
            recommendations = default_recommendations
            
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ LLM —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        recommendations = default_recommendations
    
    return recommendations

@app.post("/api/v1/seo/competitors")
async def analyze_competitors(
    request_data: CompetitorAnalysisRequest,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM –∏ RAG"""
    try:
        result = {
            "domain": request_data.domain,
            "competitors": request_data.competitors,
            "analysis_date": datetime.now().isoformat(),
            "metrics": {
                "traffic_comparison": {},
                "backlink_analysis": {},
                "keyword_overlap": {}
            }
        }
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")

# Endpoints –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
@app.get("/api/v1/history")
async def get_analysis_history(
    request: AnalysisHistoryRequest,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ –¥–æ–º–µ–Ω–∞"""
    try:
        history = [
            {
                "id": 1,
                "domain": "example.com",
                "analysis_date": datetime.now().isoformat(),
                "status": "completed",
                "score": 85.0
            }
        ]
        
        return {
            "history": history,
            "total": len(history),
            "limit": request.limit,
            "offset": request.offset
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏")

@app.post("/api/v1/export")
async def export_data(
    request_data: ExportRequest,
    current_user: dict = Depends(get_current_user_simple)
):
    """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö"""
    try:
        export_result = {
            "format": request_data.format,
            "analysis_count": len(request_data.analysis_ids),
            "download_url": f"/api/v1/downloads/export_{datetime.now().timestamp()}.{request_data.format}",
            "expires_at": (datetime.now() + timedelta(hours=24)).isoformat()
        }
        
        return export_result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö")

# Endpoints –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
@app.post("/api/v1/validate/domain")
async def validate_domain(domain: str):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞"""
    try:
        import re
        pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*$'
        if re.match(pattern, domain):
            return {
                "valid": True,
                "domain": domain,
                "sanitized": domain.lower().strip()
            }
        else:
            return {
                "valid": False,
                "domain": domain,
                "error": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–æ–º–µ–Ω–∞"
            }
    except ValueError as e:
        return {
            "valid": False,
            "domain": domain,
            "error": str(e)
        }

@app.post("/api/v1/validate/email")
async def validate_email(email: str):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è email"""
    try:
        import re
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if re.match(pattern, email):
            return {
                "valid": True,
                "email": email,
                "sanitized": email.lower().strip()
            }
        else:
            return {
                "valid": False,
                "email": email,
                "error": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç email"
            }
    except Exception as e:
        return {
            "valid": False,
            "email": email,
            "error": str(e)
        }

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –æ—à–∏–±–æ–∫
@app.exception_handler(ValidationError)
async def validation_exception_handler(request: Request, exc: ValidationError):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ Pydantic"""
    return JSONResponse(
        status_code=422,
        content={"detail": "–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö", "errors": exc.errors()}
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ HTTP –æ—à–∏–±–æ–∫"""
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """–û–±—â–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    return JSONResponse(
        status_code=500,
        content={
            "error": "internal_server_error",
            "message": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞",
            "timestamp": datetime.now().isoformat()
        }
    )

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
    os.makedirs("logs", exist_ok=True)
    
    print("üöÄ reLink SEO Platform v1.0.0 –∑–∞–ø—É—â–µ–Ω!")

@app.get("/api/v1/rag/cache/stats")
async def get_rag_cache_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG –∫—ç—à–∞"""
    try:
        stats = await cache_manager.get_rag_stats()
        return {
            "status": "success",
            "data": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting RAG cache stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/rag/cache/clear")
async def clear_rag_cache():
    """–û—á–∏—Å—Ç–∫–∞ RAG –∫—ç—à–∞"""
    try:
        # –û—á–∏—â–∞–µ–º –≤—Å–µ —Ç–∏–ø—ã RAG –∫—ç—à–∞
        await cache_manager.rag_cache.clear_expired()
        return {
            "status": "success",
            "message": "RAG cache cleared",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error clearing RAG cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/rag/monitoring/metrics")
async def get_rag_monitoring_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ RAG –º–µ—Ç—Ä–∏–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    try:
        from .monitoring import rag_monitor
        metrics = rag_monitor.get_rag_metrics()
        return {
            "status": "success",
            "data": metrics,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting RAG monitoring metrics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/rag/monitoring/health")
async def get_rag_health():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–¥–æ—Ä–æ–≤—å—è RAG —Å–∏—Å—Ç–µ–º—ã."""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ChromaDB
        chroma_client = chromadb.Client()
        collections = chroma_client.list_collections()
        
        return {
            "status": "healthy",
            "chromadb_available": True,
            "collections_count": len(collections),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"RAG health check failed: {e}")
        return {
            "status": "unhealthy",
            "chromadb_available": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# üé® DIAGRAM GENERATION ENDPOINTS

class DiagramGenerationRequestModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–≥—Ä–∞–º–º—ã."""
    diagram_type: str
    title: str
    description: str
    components: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]
    style: Optional[Dict[str, Any]] = None
    width: int = 800
    height: int = 600
    interactive: bool = True
    include_legend: bool = True

class DiagramGenerationResponseModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–≥—Ä–∞–º–º—ã."""
    diagram_id: int
    svg_content: str
    quality_score: float
    generation_time: float
    used_model: str
    confidence_score: float
    validation_result: Dict[str, Any]

@app.post("/api/diagrams/generate", response_model=DiagramGenerationResponseModel)
async def generate_diagram(
    request: DiagramGenerationRequestModel,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SVG –¥–∏–∞–≥—Ä–∞–º–º—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã."""
    try:
        diagram_service = DiagramService()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ —Ñ–æ—Ä–º–∞—Ç —Å–µ—Ä–≤–∏—Å–∞
        diagram_request = DiagramGenerationRequest(
            diagram_type=request.diagram_type,
            title=request.title,
            description=request.description,
            components=request.components,
            relationships=request.relationships,
            style_config=request.style,
            user_id=current_user.get("id", 1)
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∏–∞–≥—Ä–∞–º–º—É
        result = await diagram_service.generate_diagram(diagram_request, db)
        
        return DiagramGenerationResponseModel(
            diagram_id=result.diagram_id,
            svg_content=result.svg_content,
            quality_score=result.quality_score,
            generation_time=result.generation_time,
            used_model=result.used_model,
            confidence_score=result.confidence_score,
            validation_result=result.validation_result
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–≥—Ä–∞–º–º—ã: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–≥—Ä–∞–º–º—ã: {str(e)}"
        )

@app.get("/api/diagrams/templates")
async def get_diagram_templates():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ –¥–∏–∞–≥—Ä–∞–º–º."""
    try:
        diagram_service = DiagramService()
        templates = await diagram_service.get_available_templates()
        return {"templates": templates}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–æ–≤: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —à–∞–±–ª–æ–Ω–æ–≤: {str(e)}"
        )

@app.get("/api/diagrams/{diagram_id}")
async def get_diagram(
    diagram_id: int,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã –ø–æ ID."""
    try:
        from .models import Diagram
        result = await db.execute(
            select(Diagram).where(
                Diagram.id == diagram_id,
                Diagram.user_id == current_user.get("id", 1)
            )
        )
        diagram = result.scalar_one_or_none()
        
        if not diagram:
            raise HTTPException(status_code=404, detail="–î–∏–∞–≥—Ä–∞–º–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        return {
            "id": diagram.id,
            "title": diagram.title,
            "description": diagram.description,
            "svg_content": diagram.svg_content,
            "quality_score": diagram.quality_score,
            "created_at": diagram.created_at.isoformat(),
            "diagram_type": diagram.diagram_type
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã: {str(e)}"
        )

@app.get("/api/diagrams")
async def get_user_diagrams(
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db),
    limit: int = 10,
    offset: int = 0
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–∏–∞–≥—Ä–∞–º–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    try:
        from .models import Diagram
        result = await db.execute(
            select(Diagram)
            .where(Diagram.user_id == current_user.get("id", 1))
            .order_by(Diagram.created_at.desc())
            .limit(limit)
            .offset(offset)
        )
        diagrams = result.scalars().all()
        
        return {
            "diagrams": [
                {
                    "id": d.id,
                    "title": d.title,
                    "description": d.description,
                    "diagram_type": d.diagram_type,
                    "quality_score": d.quality_score,
                    "created_at": d.created_at.isoformat()
                }
                for d in diagrams
            ],
            "total": len(diagrams)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º: {str(e)}"
        )

# ============================================================================
# API –≠–ù–î–ü–û–ò–ù–¢–´ –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
# ============================================================================

@app.post("/api/v1/tests/", response_model=TestResponse)
async def create_test(
    test_request: TestRequest,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ç–µ—Å—Ç–∞"""
    return await testing_service.create_test(test_request, current_user.get("id", 1), db)

@app.get("/api/v1/tests/", response_model=List[TestResponse])
async def get_tests(
    test_type: Optional[TestType] = None,
    status: Optional[TestStatus] = None,
    priority: Optional[TestPriority] = None,
    environment: Optional[TestEnvironment] = None,
    name_contains: Optional[str] = None,
    description_contains: Optional[str] = None,
    tags: Optional[List[str]] = None,
    limit: int = 50,
    offset: int = 0,
    sort_by: str = "created_at",
    sort_order: str = "desc",
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    filters = TestFilter(
        test_type=test_type,
        status=status,
        priority=priority,
        environment=environment,
        name_contains=name_contains,
        description_contains=description_contains,
        tags=tags,
        limit=limit,
        offset=offset,
        sort_by=sort_by,
        sort_order=sort_order
    )
    return await testing_service.get_tests(filters, offset, limit, db)

@app.get("/api/v1/tests/{test_id}", response_model=TestResponse)
async def get_test(
    test_id: str,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ—Å—Ç–∞ –ø–æ ID"""
    test = await testing_service.get_test(test_id, db)
    if not test:
        raise_not_found("–¢–µ—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    return test

@app.post("/api/v1/tests/{test_id}/execute", response_model=TestExecutionResponse)
async def execute_test(
    test_id: str,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–∞"""
    return await testing_service.execute_test(test_id, current_user.get("id", 1), db)

@app.get("/api/v1/test-executions/", response_model=List[TestExecutionResponse])
async def get_test_executions(
    test_id: Optional[str] = None,
    status: Optional[TestStatus] = None,
    limit: int = 50,
    offset: int = 0,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–π —Ç–µ—Å—Ç–æ–≤"""
    return await testing_service.get_executions(offset, limit, test_id, status, db)

@app.post("/api/v1/test-executions/{execution_id}/cancel")
async def cancel_test_execution(
    execution_id: str,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–û—Ç–º–µ–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–∞"""
    success = await testing_service.cancel_execution(execution_id, db)
    if not success:
        raise HTTPException(status_code=404, detail="–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    return {"message": "–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ"}

@app.post("/api/v1/test-suites/", response_model=TestSuiteResponse)
async def create_test_suite(
    suite_request: TestSuiteRequest,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤"""
    return await testing_service.create_test_suite(suite_request, current_user.get("id", 1), db)

@app.post("/api/v1/test-suites/{suite_id}/execute", response_model=List[TestExecutionResponse])
async def execute_test_suite(
    suite_id: str,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ —Ç–µ—Å—Ç–æ–≤"""
    return await testing_service.execute_test_suite(suite_id, current_user.get("id", 1), db)

@app.get("/api/v1/testing/metrics")
async def get_testing_metrics(
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    return await testing_service.get_test_metrics(db=db)

@app.get("/api/v1/testing/health")
async def get_testing_health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    return {
        "status": "healthy",
        "running_executions": len(testing_service.running_executions),
        "timestamp": datetime.now().isoformat()
    }

app.include_router(testing_router, prefix="/api/v1/testing")
app.include_router(auth_router, prefix="/api/v1")
app.include_router(optimization_router, prefix="/api/v1/optimization")

@app.post("/api/v1/wordpress/index")
async def index_wordpress_site(
    request: WPRequest,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è WordPress —Å–∞–π—Ç–∞."""
    try:
        domain = request.domain.strip().lower()
        if not domain.startswith(('http://', 'https://')):
            domain = f"https://{domain}"
        
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é WordPress —Å–∞–π—Ç–∞: {domain}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –¥–æ–º–µ–Ω –≤ –±–∞–∑–µ
        existing_domain = await db.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = existing_domain.scalar_one_or_none()
        
        if not domain_obj:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–º–µ–Ω
            domain_obj = Domain(
                name=domain,
                display_name=domain.replace('https://', '').replace('http://', ''),
                description=f"WordPress —Å–∞–π—Ç {domain}",
                owner_id=current_user.get("id", 1)
            )
            db.add(domain_obj)
            await db.commit()
            await db.refresh(domain_obj)
        
        # –ü–∞—Ä—Å–∏–º WordPress —Å–∞–π—Ç
        posts = await parse_wordpress_site(domain, request.client_id)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        saved_posts = []
        for post_data in posts:
            post = WordPressPost(
                domain_id=domain_obj.id,
                wp_post_id=post_data.get('id', 0),
                title=post_data.get('title', ''),
                content=post_data.get('content', ''),
                excerpt=post_data.get('excerpt', ''),
                link=post_data.get('link', ''),
                published_at=post_data.get('date', utc_now())
            )
            db.add(post)
            saved_posts.append(post)
        
        await db.commit()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–º–µ–Ω–∞
        domain_obj.total_posts = len(saved_posts)
        domain_obj.last_analysis_at = utc_now()
        await db.commit()
        
        return {
            "status": "success",
            "message": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–∞–π–¥–µ–Ω–æ {len(saved_posts)} —Å—Ç–∞—Ç–µ–π.",
            "domain": domain,
            "posts_count": len(saved_posts),
            "domain_id": domain_obj.id
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ WordPress —Å–∞–π—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}")

@app.post("/api/v1/wordpress/reindex")
async def reindex_wordpress_site(
    request: WPRequest,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–†–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è WordPress —Å–∞–π—Ç–∞."""
    try:
        domain = request.domain.strip().lower()
        if not domain.startswith(('http://', 'https://')):
            domain = f"https://{domain}"
        
        logger.info(f"–ù–∞—á–∏–Ω–∞—é —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é WordPress —Å–∞–π—Ç–∞: {domain}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–æ–º–µ–Ω –≤ –±–∞–∑–µ
        existing_domain = await db.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = existing_domain.scalar_one_or_none()
        
        if not domain_obj:
            raise HTTPException(status_code=404, detail="–î–æ–º–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ—Å—Ç—ã
        await db.execute(
            delete(WordPressPost).where(WordPressPost.domain_id == domain_obj.id)
        )
        await db.commit()
        
        # –ü–∞—Ä—Å–∏–º WordPress —Å–∞–π—Ç –∑–∞–Ω–æ–≤–æ
        posts = await parse_wordpress_site(domain, request.client_id)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        saved_posts = []
        for post_data in posts:
            post = WordPressPost(
                domain_id=domain_obj.id,
                wp_post_id=post_data.get('id', 0),
                title=post_data.get('title', ''),
                content=post_data.get('content', ''),
                excerpt=post_data.get('excerpt', ''),
                link=post_data.get('link', ''),
                published_at=post_data.get('date', utc_now())
            )
            db.add(post)
            saved_posts.append(post)
        
        await db.commit()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–º–µ–Ω–∞
        domain_obj.total_posts = len(saved_posts)
        domain_obj.last_analysis_at = utc_now()
        await db.commit()
        
        return {
            "status": "success",
            "message": f"–†–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±–Ω–æ–≤–ª–µ–Ω–æ {len(saved_posts)} —Å—Ç–∞—Ç–µ–π.",
            "domain": domain,
            "posts_count": len(saved_posts),
            "domain_id": domain_obj.id,
            "reindexed_at": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ WordPress —Å–∞–π—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}")

async def parse_wordpress_site(domain: str, client_id: str = None) -> List[dict]:
    """–ü–∞—Ä—Å–∏–Ω–≥ WordPress —Å–∞–π—Ç–∞ —á–µ—Ä–µ–∑ REST API –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π."""
    posts = []
    
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è WordPress REST API
        api_url = f"{domain.rstrip('/')}/wp-json/wp/v2/posts"
        
        if client_id:
            await websocket_manager.send_step(client_id, "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ WordPress API", 0, 1)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ WordPress REST API
        async with httpx.AsyncClient(timeout=30.0) as client:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å 100 —Å—Ç–∞—Ç–µ–π
            response = await client.get(f"{api_url}?per_page=100")
            
            if response.status_code != 200:
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –º–µ–Ω—å—à–µ —Å—Ç–∞—Ç–µ–π
                response = await client.get(f"{api_url}?per_page=50")
                
                if response.status_code != 200:
                    # –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å 10 —Å—Ç–∞—Ç–µ–π
                    response = await client.get(f"{api_url}?per_page=10")
                    
                    if response.status_code != 200:
                        raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ WordPress API: {response.status_code}")
            
            wp_posts = response.json()
            
            if client_id:
                await websocket_manager.send_step(client_id, f"–ù–∞–π–¥–µ–Ω–æ {len(wp_posts)} —Å—Ç–∞—Ç–µ–π", 1, len(wp_posts))
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é
            for i, wp_post in enumerate(wp_posts):
                try:
                    if client_id:
                        await websocket_manager.send_step(client_id, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏ {i+1}", i+1, len(wp_posts))
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ WordPress API –æ—Ç–≤–µ—Ç–∞
                    post_id = wp_post.get('id', 0)
                    title = wp_post.get('title', {}).get('rendered', '')
                    content = wp_post.get('content', {}).get('rendered', '')
                    excerpt = wp_post.get('excerpt', {}).get('rendered', '')
                    link = wp_post.get('link', '')
                    
                    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
                    date = utc_now()
                    date_str = wp_post.get('date', '')
                    if date_str:
                        try:
                            # WordPress API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç—É –≤ UTC, –Ω–æ –±–µ–∑ timezone info
                            # –î–æ–±–∞–≤–ª—è–µ–º timezone info
                            if 'T' in date_str and 'Z' not in date_str:
                                date_str += 'Z'
                            date_with_tz = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                            # –£–±–∏—Ä–∞–µ–º timezone info –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                            date = date_with_tz.replace(tzinfo=None)
                        except:
                            pass
                    
                    # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    if content:
                        soup = BeautifulSoup(content, 'html.parser')
                        content = soup.get_text().strip()
                    
                    if excerpt:
                        soup = BeautifulSoup(excerpt, 'html.parser')
                        excerpt = soup.get_text().strip()
                    
                    if title and content:
                        posts.append({
                            'id': post_id,
                            'title': title,
                            'content': content,
                            'excerpt': excerpt,
                            'link': link,
                            'date': date
                        })
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å—Ç–∞—Ç–µ–π
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ {i+1}: {e}")
                    continue
            
            if client_id:
                await websocket_manager.send_step(client_id, "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞", len(wp_posts), len(wp_posts))
        
        return posts
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ WordPress —Å–∞–π—Ç–∞: {e}")
        if client_id:
            await websocket_manager.send_error(client_id, "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞", str(e))
        raise

@app.post("/api/v1/seo/recommendations")
async def get_seo_recommendations(
    request_data: DomainAnalysisRequest,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM Router."""
    try:
        domain = request_data.domain.strip().lower()
        if not domain.startswith(('http://', 'https://')):
            domain = f"https://{domain}"
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        domain_obj = await db.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = domain_obj.scalar_one_or_none()
        
        if not domain_obj:
            raise HTTPException(status_code=404, detail="–î–æ–º–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ—Å—Ç—ã –¥–æ–º–µ–Ω–∞
        posts = await db.execute(
            select(WordPressPost).where(WordPressPost.domain_id == domain_obj.id)
        )
        posts = posts.scalars().all()
        
        if not posts:
            raise HTTPException(status_code=404, detail="–°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = await generate_seo_recommendations(posts, domain, request_data.client_id)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –≤ –∏—Å—Ç–æ—Ä–∏—é
        analysis = AnalysisHistory(
            domain_id=domain_obj.id,
            user_id=current_user.get("id", 1),
            posts_analyzed=len(posts),
            connections_found=len([r for r in recommendations if r.get('type') == 'internal_linking']),
            recommendations_generated=len(recommendations),
            recommendations=recommendations,
            thematic_analysis={
                "total_posts": len(posts),
                "avg_content_length": sum(len(p.content) for p in posts) / len(posts),
                "topics_found": len(set(p.content_type for p in posts if p.content_type))
            },
            semantic_metrics={
                "content_quality_avg": sum(p.content_quality_score for p in posts) / len(posts),
                "semantic_richness_avg": sum(p.semantic_richness for p in posts) / len(posts)
            },
            llm_model_used=OLLAMA_MODEL,
            processing_time_seconds=0.0
        )
        db.add(analysis)
        await db.commit()
        
        return {
            "status": "success",
            "domain": domain,
            "posts_analyzed": len(posts),
            "recommendations": recommendations,
            "analysis_id": analysis.id,
            "generated_at": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {str(e)}")

async def generate_seo_recommendations(posts: List[WordPressPost], domain: str, client_id: str = None) -> List[dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM Router."""
    recommendations = []
    
    try:
        if client_id:
            await websocket_manager.send_ai_thinking(client_id, "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...", "analyzing", "üîç")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è LLM –∞–Ω–∞–ª–∏–∑–∞
        posts_data = []
        for post in posts:
            posts_data.append({
                'title': post.title,
                'content': post.content,
                'excerpt': post.excerpt,
                'link': post.link,
                'published_at': post.published_at.isoformat() if post.published_at else None,
                'content_quality_score': post.content_quality_score,
                'semantic_richness': post.semantic_richness
            })
        
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM Router
        llm_analysis = await analyze_content_with_llm(posts_data, domain, client_id)
        recommendations.extend(llm_analysis)
        
        # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–∫–∞–∫ fallback)
        if client_id:
            await websocket_manager.send_ai_thinking(client_id, "–í—ã–ø–æ–ª–Ω—è—é –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑...", "analyzing", "üìä")
        
        # –ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        internal_linking_recs = await analyze_internal_linking(posts, client_id)
        recommendations.extend(internal_linking_recs)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_recs = await analyze_content_quality(posts, client_id)
        recommendations.extend(content_recs)
        
        # –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏–∫–∏
        semantic_recs = await analyze_semantic_optimization(posts, client_id)
        recommendations.extend(semantic_recs)
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        structure_recs = await analyze_content_structure(posts, client_id)
        recommendations.extend(structure_recs)
        
        if client_id:
            await websocket_manager.send_ai_thinking(client_id, f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", "optimizing", "‚úÖ")
        
        return recommendations
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        if client_id:
            await websocket_manager.send_error(client_id, "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", str(e))
        raise

async def analyze_content_with_llm(posts_data: List[dict], domain: str, client_id: str = None) -> List[dict]:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM Router."""
    try:
        from .llm_router import llm_router, LLMServiceType, LLMRequest
        
        if client_id:
            await websocket_manager.send_ai_thinking(client_id, "–ó–∞–ø—É—Å–∫–∞—é AI-–∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...", "analyzing", "üß†")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context = {
            "domain": domain,
            "total_posts": len(posts_data),
            "posts_summary": [
                {
                    "title": post['title'],
                    "content_length": len(post['content']),
                    "quality_score": post.get('content_quality_score', 0),
                    "semantic_richness": post.get('semantic_richness', 0)
                }
                for post in posts_data[:10]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            ],
            "content_samples": [
                {
                    "title": post['title'],
                    "excerpt": post['excerpt'][:200] if post['excerpt'] else post['content'][:200]
                }
                for post in posts_data[:5]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø–æ—Å—Ç–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            ]
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è SEO –∞–Ω–∞–ª–∏–∑–∞
        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞ {domain} –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
        
        –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
        - –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {len(posts_data)}
        - –î–æ–º–µ–Ω—ã —Å—Ç–∞—Ç–µ–π: {[post['title'] for post in posts_data[:5]]}
        
        –ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ JSON:
        {{
            "recommendations": [
                {{
                    "type": "content_optimization|technical_seo|semantic_optimization|user_experience",
                    "priority": "high|medium|low",
                    "title": "–ö—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
                    "description": "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è",
                    "impact_score": 0.0-1.0,
                    "implementation_difficulty": "easy|medium|hard",
                    "estimated_impact": "–û–ø–∏—Å–∞–Ω–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞",
                    "specific_actions": ["–î–µ–π—Å—Ç–≤–∏–µ 1", "–î–µ–π—Å—Ç–≤–∏–µ 2", "–î–µ–π—Å—Ç–≤–∏–µ 3"]
                }}
            ]
        }}
        
        –§–æ–∫—É—Å –Ω–∞:
        1. –ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        3. –í–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫—É
        4. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã SEO
        5. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç
        """
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ LLM Router
        request = LLMRequest(
            service_type=LLMServiceType.SEO_RECOMMENDATIONS,
            prompt=prompt,
            context=context,
            priority="high",
            temperature=0.3,
            max_tokens=2000
        )
        
        if client_id:
            await websocket_manager.send_ai_thinking(client_id, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å –≤ LLM Router...", "processing", "‚ö°")
        
        response = await llm_router.process_request(request)
        
        if response.error:
            logger.error(f"–û—à–∏–±–∫–∞ LLM Router: {response.error}")
            return []
        
        if client_id:
            await websocket_manager.send_ai_thinking(client_id, "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...", "analyzing", "üîç")
        
        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç –æ—Ç LLM
        try:
            import json
            import re
            
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
            if json_match:
                llm_result = json.loads(json_match.group())
                recommendations = llm_result.get('recommendations', [])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                for rec in recommendations:
                    rec['source'] = 'llm_router'
                    rec['model_used'] = response.model_used
                    rec['processing_time'] = response.response_time
                
                return recommendations
            else:
                # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –æ–±—â—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                return [{
                    "type": "ai_analysis",
                    "priority": "medium",
                    "title": "AI-–∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
                    "description": "AI –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
                    "source": "llm_router",
                    "model_used": response.model_used,
                    "processing_time": response.response_time,
                    "ai_insights": response.content[:500] + "..." if len(response.content) > 500 else response.content
                }]
                
        except json.JSONDecodeError as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç LLM: {e}")
            return [{
                "type": "ai_analysis",
                "priority": "medium",
                "title": "AI-–∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
                "description": "AI –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª –∫–æ–Ω—Ç–µ–Ω—Ç",
                "source": "llm_router",
                "model_used": response.model_used,
                "processing_time": response.response_time,
                "ai_insights": response.content[:500] + "..." if len(response.content) > 500 else response.content
            }]
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ AI-–∞–Ω–∞–ª–∏–∑–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
        return []

async def analyze_internal_linking(posts: List[WordPressPost], client_id: str = None) -> List[dict]:
    """–ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""
    recommendations = []
    
    # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ - –∏—â–µ–º —Å—Ç–∞—Ç—å–∏ –±–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
    posts_without_links = []
    for post in posts:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ —Å—Ç–∞—Ç—å–∏
        content_lower = post.content.lower()
        has_internal_links = any(
            other_post.title.lower() in content_lower 
            for other_post in posts 
            if other_post.id != post.id
        )
        
        if not has_internal_links:
            posts_without_links.append(post)
    
    if posts_without_links:
        recommendations.append({
            "type": "internal_linking",
            "priority": "high",
            "title": "–î–æ–±–∞–≤–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏",
            "description": f"–ù–∞–π–¥–µ–Ω–æ {len(posts_without_links)} —Å—Ç–∞—Ç–µ–π –±–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫",
            "details": [
                {
                    "post_title": post.title,
                    "post_url": post.link,
                    "suggested_links": [
                        other_post.title 
                        for other_post in posts[:3] 
                        if other_post.id != post.id
                    ]
                }
                for post in posts_without_links[:5]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5
            ]
        })
    
    return recommendations

async def analyze_content_quality(posts: List[WordPressPost], client_id: str = None) -> List[dict]:
    """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    recommendations = []
    
    # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    short_posts = [post for post in posts if len(post.content) < 1000]
    if short_posts:
        recommendations.append({
            "type": "content_quality",
            "priority": "medium",
            "title": "–†–∞—Å—à–∏—Ä–∏—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç–∞—Ç—å–∏",
            "description": f"–ù–∞–π–¥–µ–Ω–æ {len(short_posts)} —Å—Ç–∞—Ç–µ–π —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –æ–±—ä–µ–º–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "details": [
                {
                    "post_title": post.title,
                    "post_url": post.link,
                    "current_length": len(post.content),
                    "recommended_length": "1500+ —Å–∏–º–≤–æ–ª–æ–≤"
                }
                for post in short_posts[:3]
            ]
        })
    
    # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    posts_without_h1 = [post for post in posts if not post.title or len(post.title) < 10]
    if posts_without_h1:
        recommendations.append({
            "type": "content_quality",
            "priority": "medium",
            "title": "–£–ª—É—á—à–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π",
            "description": f"–ù–∞–π–¥–µ–Ω–æ {len(posts_without_h1)} —Å—Ç–∞—Ç–µ–π —Å –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏",
            "details": [
                {
                    "post_title": post.title,
                    "post_url": post.link,
                    "current_length": len(post.title),
                    "recommendation": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 50-60 —Å–∏–º–≤–æ–ª–æ–≤"
                }
                for post in posts_without_h1[:3]
            ]
        })
    
    return recommendations

async def analyze_semantic_optimization(posts: List[WordPressPost], client_id: str = None) -> List[dict]:
    """–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
    recommendations = []
    
    # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    all_content = " ".join([post.content for post in posts])
    words = word_tokenize(all_content.lower())
    words = [word for word in words if word.isalpha() and word not in RUSSIAN_STOP_WORDS]
    
    # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
    word_freq = defaultdict(int)
    for word in words:
        word_freq[word] += 1
    
    # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    
    recommendations.append({
        "type": "semantic_optimization",
        "priority": "medium",
        "title": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞",
        "description": "–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ —Å–∞–π—Ç–∞",
        "details": {
            "top_keywords": [{"word": word, "frequency": freq} for word, freq in top_words],
            "recommendation": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö –∏ –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏—è—Ö"
        }
    })
    
    return recommendations

async def analyze_content_structure(posts: List[WordPressPost], client_id: str = None) -> List[dict]:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    recommendations = []
    
    # –ê–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø
    content_types = defaultdict(int)
    for post in posts:
        if post.content_type:
            content_types[post.content_type] += 1
    
    if len(content_types) < 3:
        recommendations.append({
            "type": "content_structure",
            "priority": "low",
            "title": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—Ç—å —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "description": "–°–∞–π—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "details": {
                "current_types": dict(content_types),
                "recommendation": "–î–æ–±–∞–≤—å—Ç–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞: –≥–∞–π–¥—ã, –æ–±–∑–æ—Ä—ã, –Ω–æ–≤–æ—Å—Ç–∏, –∏–Ω—Ç–µ—Ä–≤—å—é"
            }
        })
    
    return recommendations

@app.get("/api/v1/insights/{domain_id}")
async def get_domain_insights(
    domain_id: int,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –ø–æ –¥–æ–º–µ–Ω—É."""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω
        domain = await db.execute(
            select(Domain).where(Domain.id == domain_id)
        )
        domain = domain.scalar_one_or_none()
        
        if not domain:
            raise HTTPException(status_code=404, detail="–î–æ–º–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã –¥–æ–º–µ–Ω–∞
        posts = await db.execute(
            select(WordPressPost).where(WordPressPost.domain_id == domain_id)
        )
        posts = posts.scalars().all()
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤
        analyses = await db.execute(
            select(AnalysisHistory)
            .where(AnalysisHistory.domain_id == domain_id)
            .order_by(AnalysisHistory.created_at.desc())
            .limit(5)
        )
        analyses = analyses.scalars().all()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Å–∞–π—Ç—ã
        insights = await generate_domain_insights(posts, analyses)
        
        return {
            "status": "success",
            "domain": {
                "id": domain.id,
                "name": domain.name,
                "display_name": domain.display_name,
                "total_posts": domain.total_posts,
                "last_analysis": domain.last_analysis_at.isoformat() if domain.last_analysis_at else None
            },
            "insights": insights,
            "recent_analyses": [
                {
                    "id": analysis.id,
                    "created_at": analysis.created_at.isoformat(),
                    "posts_analyzed": analysis.posts_analyzed,
                    "recommendations_count": analysis.recommendations_generated
                }
                for analysis in analyses
            ]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏–Ω—Å–∞–π—Ç–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤: {str(e)}")

@app.get("/api/v1/analytics/{domain_id}")
async def get_domain_analytics(
    domain_id: int,
    current_user: dict = Depends(get_current_user_simple),
    db: AsyncSession = Depends(get_db)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ –¥–æ–º–µ–Ω—É."""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω
        domain = await db.execute(
            select(Domain).where(Domain.id == domain_id)
        )
        domain = domain.scalar_one_or_none()
        
        if not domain:
            raise HTTPException(status_code=404, detail="–î–æ–º–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã –¥–æ–º–µ–Ω–∞
        posts = await db.execute(
            select(WordPressPost).where(WordPressPost.domain_id == domain_id)
        )
        posts = posts.scalars().all()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É
        analytics = await generate_domain_analytics(posts)
        
        return {
            "status": "success",
            "domain": {
                "id": domain.id,
                "name": domain.name,
                "display_name": domain.display_name
            },
            "analytics": analytics,
            "generated_at": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {str(e)}")

async def generate_domain_insights(posts: List[WordPressPost], analyses: List[AnalysisHistory]) -> dict:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤ –ø–æ –¥–æ–º–µ–Ω—É."""
    insights = {
        "content_insights": {},
        "performance_insights": {},
        "seo_insights": {},
        "trends": {}
    }
    
    if not posts:
        return insights
    
    # –ò–Ω—Å–∞–π—Ç—ã –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
    total_content_length = sum(len(post.content) for post in posts)
    avg_content_length = total_content_length / len(posts)
    
    insights["content_insights"] = {
        "total_posts": len(posts),
        "total_content_length": total_content_length,
        "average_content_length": round(avg_content_length, 2),
        "content_distribution": {
            "short_posts": len([p for p in posts if len(p.content) < 1000]),
            "medium_posts": len([p for p in posts if 1000 <= len(p.content) < 3000]),
            "long_posts": len([p for p in posts if len(p.content) >= 3000])
        },
        "top_performing_content": [
            {
                "title": post.title,
                "url": post.link,
                "length": len(post.content),
                "quality_score": post.content_quality_score
            }
            for post in sorted(posts, key=lambda p: p.content_quality_score, reverse=True)[:5]
        ]
    }
    
    # –ò–Ω—Å–∞–π—Ç—ã –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if analyses:
        latest_analysis = analyses[0]
        insights["performance_insights"] = {
            "last_analysis_date": latest_analysis.created_at.isoformat(),
            "total_recommendations": latest_analysis.recommendations_generated,
            "internal_links_found": latest_analysis.connections_found,
            "analysis_metrics": latest_analysis.semantic_metrics
        }
    
    # SEO –∏–Ω—Å–∞–π—Ç—ã
    all_content = " ".join([post.content for post in posts])
    words = word_tokenize(all_content.lower())
    words = [word for word in words if word.isalpha() and word not in RUSSIAN_STOP_WORDS]
    
    word_freq = defaultdict(int)
    for word in words:
        word_freq[word] += 1
    
    top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]
    
    insights["seo_insights"] = {
        "top_keywords": [{"word": word, "frequency": freq} for word, freq in top_keywords],
        "keyword_density": len(set(words)) / len(words) if words else 0,
        "content_optimization_score": sum(post.content_quality_score for post in posts) / len(posts)
    }
    
    # –¢—Ä–µ–Ω–¥—ã
    if len(analyses) > 1:
        insights["trends"] = {
            "analysis_frequency": "regular" if len(analyses) >= 3 else "occasional",
            "improvement_trend": "positive" if analyses[0].recommendations_generated < analyses[-1].recommendations_generated else "stable"
        }
    
    return insights

async def generate_domain_analytics(posts: List[WordPressPost]) -> dict:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ –¥–æ–º–µ–Ω—É."""
    analytics = {
        "content_metrics": {},
        "semantic_analysis": {},
        "quality_metrics": {},
        "engagement_potential": {}
    }
    
    if not posts:
        return analytics
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_lengths = [len(post.content) for post in posts]
    analytics["content_metrics"] = {
        "total_posts": len(posts),
        "total_words": sum(len(post.content.split()) for post in posts),
        "average_length": round(sum(content_lengths) / len(content_lengths), 2),
        "min_length": min(content_lengths),
        "max_length": max(content_lengths),
        "length_distribution": {
            "0-500": len([l for l in content_lengths if l < 500]),
            "500-1000": len([l for l in content_lengths if 500 <= l < 1000]),
            "1000-2000": len([l for l in content_lengths if 1000 <= l < 2000]),
            "2000+": len([l for l in content_lengths if l >= 2000])
        }
    }
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    all_content = " ".join([post.content for post in posts])
    words = word_tokenize(all_content.lower())
    words = [word for word in words if word.isalpha() and word not in RUSSIAN_STOP_WORDS]
    
    word_freq = defaultdict(int)
    for word in words:
        word_freq[word] += 1
    
    analytics["semantic_analysis"] = {
        "unique_words": len(set(words)),
        "total_words": len(words),
        "lexical_diversity": len(set(words)) / len(words) if words else 0,
        "top_keywords": [{"word": word, "frequency": freq} for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]]
    }
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_scores = [post.content_quality_score for post in posts]
    semantic_scores = [post.semantic_richness for post in posts]
    
    analytics["quality_metrics"] = {
        "average_content_quality": round(sum(quality_scores) / len(quality_scores), 3),
        "average_semantic_richness": round(sum(semantic_scores) / len(semantic_scores), 3),
        "quality_distribution": {
            "excellent": len([s for s in quality_scores if s >= 0.8]),
            "good": len([s for s in quality_scores if 0.6 <= s < 0.8]),
            "average": len([s for s in quality_scores if 0.4 <= s < 0.6]),
            "poor": len([s for s in quality_scores if s < 0.4])
        }
    }
    
    # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
    analytics["engagement_potential"] = {
        "linkability_score": round(sum(post.linkability_score for post in posts) / len(posts), 3),
        "content_types": {
            "guides": len([p for p in posts if p.content_type == "guide"]),
            "reviews": len([p for p in posts if p.content_type == "review"]),
            "news": len([p for p in posts if p.content_type == "news"]),
            "other": len([p for p in posts if not p.content_type])
        },
        "target_audience_diversity": len(set(p.target_audience for p in posts if p.target_audience))
    }
    
    return analytics

@app.post("/api/v1/test/setup")
async def setup_test_user():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    try:
        from .auth import get_password_hash
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–µ—Å—Ç–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
        async with async_sessionmaker(engine)() as db:
            existing_user = await db.execute(
                select(User).where(User.username == "test_user")
            )
            existing_user = existing_user.scalar_one_or_none()
            
            if existing_user:
                return {
                    "status": "success",
                    "message": "–¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç",
                    "user": {
                        "id": existing_user.id,
                        "username": existing_user.username,
                        "email": existing_user.email
                    }
                }
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            test_user = User(
                username="test_user",
                email="test@example.com",
                full_name="–¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
                hashed_password=get_password_hash("test123"),
                is_active=True
            )
            db.add(test_user)
            await db.commit()
            await db.refresh(test_user)
            
            return {
                "status": "success",
                "message": "–¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–æ–∑–¥–∞–Ω",
                "user": {
                    "id": test_user.id,
                    "username": test_user.username,
                    "email": test_user.email
                }
            }
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {str(e)}")

@app.post("/api/v1/test/login")
async def test_login():
    """–ë—ã—Å—Ç—Ä—ã–π –ª–æ–≥–∏–Ω —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    try:
        from .auth import create_access_token
        
        # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        access_token = create_access_token(data={"sub": "test_user"})
        
        return {
            "status": "success",
            "access_token": access_token,
            "token_type": "bearer",
            "user": {
                "username": "test_user",
                "email": "test@example.com"
            }
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏–Ω–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏–Ω–∞: {str(e)}")

@app.post("/api/v1/seo/content-recommendations")
async def get_content_recommendations(
    domain: str,
    current_user: dict = Depends(get_current_user_simple)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG"""
    try:
        logger.info(f"–ó–∞–ø—Ä–æ—Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –¥–æ–º–µ–Ω–∞: {domain}")
        
        db_rag_service = await get_database_rag_service()
        
        recommendations = await db_rag_service.generate_content_recommendations(
            domain_name=domain,
            user_id=current_user.get("id", 1)
        )
        
        return {
            "domain": domain,
            "recommendations": recommendations,
            "generated_at": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è {domain}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {str(e)}")

@app.post("/api/v1/seo/keyword-optimization")
async def optimize_keywords(
    domain: str,
    current_user: dict = Depends(get_current_user_simple)
):
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
    try:
        logger.info(f"–ó–∞–ø—Ä–æ—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –¥–æ–º–µ–Ω–∞: {domain}")
        
        db_rag_service = await get_database_rag_service()
        
        optimization = await db_rag_service.optimize_keywords_with_history(
            domain_name=domain,
            user_id=current_user.get("id", 1)
        )
        
        return optimization
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è {domain}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {str(e)}")

@app.get("/api/v1/seo/analysis-history/{domain}")
async def get_analysis_history(
    domain: str,
    limit: int = 10,
    current_user: dict = Depends(get_current_user_simple)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ –¥–æ–º–µ–Ω–∞"""
    try:
        logger.info(f"–ó–∞–ø—Ä–æ—Å –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ –¥–ª—è –¥–æ–º–µ–Ω–∞: {domain}")
        
        db_rag_service = await get_database_rag_service()
        
        history = await db_rag_service.get_analysis_history(
            domain_name=domain,
            limit=limit
        )
        
        return {
            "domain": domain,
            "history": history,
            "total_count": len(history)
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ –¥–ª—è {domain}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {str(e)}")

@app.get("/api/v1/llm/health")
async def llm_health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è LLM —Å–∏—Å—Ç–µ–º—ã"""
    try:
        llm_service = await get_llm_integration_service()
        health_status = await llm_service.health_check()
        
        return {
            "status": "healthy" if health_status.get("status") == "healthy" else "unhealthy",
            "llm_service": health_status,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è LLM: {str(e)}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/api/v1/llm/metrics")
async def get_llm_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ LLM —Å–∏—Å—Ç–µ–º—ã"""
    try:
        llm_service = await get_llm_integration_service()
        metrics = await llm_service.get_metrics()
        
        return {
            "metrics": metrics,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ LLM: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {str(e)}")

@app.post("/api/v1/test/seo/analyze")
async def test_analyze_domain(request_data: DomainAnalysisRequest):
    """–¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞ –±–µ–∑ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    try:
        logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞: {request_data.domain}")
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ –ë–î
        return {
            "domain": request_data.domain,
            "analysis_date": datetime.utcnow(),
            "score": 75.0,
            "recommendations": [
                {
                    "type": "content",
                    "priority": "high",
                    "description": "–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Å—Ç–∞—Ç–µ–π –æ —Å–∞–¥–æ–≤–æ–¥—Å—Ç–≤–µ"
                },
                {
                    "type": "technical",
                    "priority": "medium", 
                    "description": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏"
                }
            ],
            "metrics": {
                "content_quality": 0.7,
                "internal_linking": 0.6,
                "keyword_optimization": 0.8
            },
            "status": "completed"
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞ {request_data.domain}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

