"""
üß† –ï–¥–∏–Ω—ã–π LLM-–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ reLink

–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–º RAG-–ø–æ–¥—Ö–æ–¥–µ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º –≤ —Å–µ—Ä–≤–∏—Å–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å Ollama.
"""

import asyncio
import aiohttp
import json
import logging
import platform
import psutil
import subprocess
import os
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import time
from contextlib import asynccontextmanager

from .config import settings
from .database import get_db
from .models import LLMRequest, LLMResponse, LLMEmbedding
from .cache import cache_manager
from .exceptions import LLMServiceError, OllamaConnectionError

logger = logging.getLogger(__name__)

class LLMServiceType(Enum):
    """–¢–∏–ø—ã LLM-—Å–µ—Ä–≤–∏—Å–æ–≤"""
    SEO_RECOMMENDATIONS = "seo_recommendations"
    DIAGRAM_GENERATION = "diagram_generation"
    CONTENT_ANALYSIS = "content_analysis"
    BENCHMARK_SERVICE = "benchmark_service"
    LLM_TUNING = "llm_tuning"

@dataclass
class SystemSpecs:
    """–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã"""
    platform: str
    architecture: str
    cpu_count: int
    memory_gb: float
    gpu_available: bool
    gpu_type: Optional[str] = None
    apple_silicon: bool = False
    m1_m2_m4: bool = False

@dataclass
class OptimizedConfig:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∏—Å—Ç–µ–º—ã"""
    model: str
    num_gpu: int
    num_thread: int
    batch_size: int
    f16_kv: bool
    temperature: float
    max_tokens: int
    context_length: int
    keep_alive: str
    request_timeout: int
    semaphore_limit: int
    cache_ttl: int

class SystemAnalyzer:
    """
    üîç –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏—Å—Ç–µ–º—ã —Å RAG –∏ LLM
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å–ø–æ–ª—å–∑—É—è:
    - –ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    - RAG —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - LLM –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    
    def __init__(self):
        self.specs: Optional[SystemSpecs] = None
        self.optimized_config: Optional[OptimizedConfig] = None
        self.performance_history: List[Dict[str, Any]] = []
        self.knowledge_base: List[Dict[str, Any]] = []
        self._initialize_knowledge_base()
    
    def _initialize_knowledge_base(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.knowledge_base = [
            {
                "system_type": "apple_silicon_m1_m2_m4",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 8,
                    "batch_size": 1024,
                    "f16_kv": True,
                    "context_length": 8192,
                    "semaphore_limit": 8
                },
                "performance_notes": "Apple Silicon M1/M2/M4 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å GPU acceleration –∏ –±–æ–ª—å—à–∏–º–∏ batch sizes",
                "memory_optimization": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Unified Memory Architecture –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"
            },
            {
                "system_type": "apple_silicon_generic",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 6,
                    "batch_size": 768,
                    "f16_kv": True,
                    "context_length": 6144,
                    "semaphore_limit": 6
                },
                "performance_notes": "–û–±—â–∏–µ Apple Silicon –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å Metal acceleration",
                "memory_optimization": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU memory"
            },
            {
                "system_type": "nvidia_gpu",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 6,
                    "batch_size": 1024,
                    "f16_kv": True,
                    "context_length": 8192,
                    "semaphore_limit": 6
                },
                "performance_notes": "NVIDIA GPU –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å CUDA acceleration",
                "memory_optimization": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPU memory –¥–ª—è KV cache"
            },
            {
                "system_type": "amd_gpu",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 6,
                    "batch_size": 768,
                    "f16_kv": True,
                    "context_length": 6144,
                    "semaphore_limit": 6
                },
                "performance_notes": "AMD GPU —Ä–∞–±–æ—Ç–∞–µ—Ç —Å ROCm acceleration",
                "memory_optimization": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM"
            },
            {
                "system_type": "cpu_only_high_memory",
                "optimal_config": {
                    "num_gpu": 0,
                    "num_thread": 8,
                    "batch_size": 512,
                    "f16_kv": False,
                    "context_length": 8192,
                    "semaphore_limit": 6
                },
                "performance_notes": "CPU-only —Å–∏—Å—Ç–µ–º—ã —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º –ø–∞–º—è—Ç–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã",
                "memory_optimization": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç RAM –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"
            },
            {
                "system_type": "cpu_only_low_memory",
                "optimal_config": {
                    "num_gpu": 0,
                    "num_thread": 4,
                    "batch_size": 256,
                    "f16_kv": False,
                    "context_length": 2048,
                    "semaphore_limit": 3
                },
                "performance_notes": "–°–∏—Å—Ç–µ–º—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é —Ç—Ä–µ–±—É—é—Ç –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫",
                "memory_optimization": "–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"
            }
        ]
    
    async def analyze_system(self) -> SystemSpecs:
        """–ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π"""
        if self.specs:
            return self.specs
        
        platform_name = platform.system()
        architecture = platform.machine()
        cpu_count = psutil.cpu_count(logical=True)
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Apple Silicon
        apple_silicon = False
        m1_m2_m4 = False
        
        if platform_name == "Darwin" and "arm" in architecture.lower():
            apple_silicon = True
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
            try:
                result = subprocess.run(
                    ["sysctl", "-n", "machdep.cpu.brand_string"], 
                    capture_output=True, text=True
                )
                cpu_brand = result.stdout.lower()
                if any(x in cpu_brand for x in ["m1", "m2", "m3", "m4"]):
                    m1_m2_m4 = True
            except:
                pass
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GPU
        gpu_available = False
        gpu_type = None
        
        if apple_silicon:
            gpu_available = True
            gpu_type = "Apple Silicon GPU"
        else:
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ NVIDIA GPU
                result = subprocess.run(["nvidia-smi"], capture_output=True)
                if result.returncode == 0:
                    gpu_available = True
                    gpu_type = "NVIDIA"
                else:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ AMD GPU
                    result = subprocess.run(["rocm-smi"], capture_output=True)
                    if result.returncode == 0:
                        gpu_available = True
                        gpu_type = "AMD"
            except:
                pass
        
        self.specs = SystemSpecs(
            platform=platform_name,
            architecture=architecture,
            cpu_count=cpu_count,
            memory_gb=memory_gb,
            gpu_available=gpu_available,
            gpu_type=gpu_type,
            apple_silicon=apple_silicon,
            m1_m2_m4=m1_m2_m4
        )
        
        logger.info(f"üîç System analysis completed: {self.specs}")
        return self.specs
    
    async def _get_llm_recommendation(self, specs: SystemSpecs, performance_data: List[Dict]) -> Dict[str, Any]:
        """
        üß† –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç LLM –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç RAG —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –∏ –∏—Å—Ç–æ—Ä–∏–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        try:
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM
            context_parts = [
                f"–°–∏—Å—Ç–µ–º–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:",
                f"- –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞: {specs.platform}",
                f"- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {specs.architecture}",
                f"- CPU —è–¥–µ—Ä: {specs.cpu_count}",
                f"- –ü–∞–º—è—Ç—å: {specs.memory_gb:.1f} GB",
                f"- GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {specs.gpu_available}",
                f"- –¢–∏–ø GPU: {specs.gpu_type}",
                f"- Apple Silicon: {specs.apple_silicon}",
                f"- M1/M2/M4: {specs.m1_m2_m4}"
            ]
            
            if performance_data:
                context_parts.append("\n–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
                for perf in performance_data[-3:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –∑–∞–ø–∏—Å–∏
                    context_parts.append(f"- {perf['timestamp']}: {perf['avg_response_time']:.2f}s, {perf['success_rate']:.1%}")
            
            context_parts.append("\n–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
            for kb in self.knowledge_base:
                context_parts.append(f"- {kb['system_type']}: {kb['performance_notes']}")
            
            context = "\n".join(context_parts)
            
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            prompt = f"""
            –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM —Å–∏—Å—Ç–µ–º. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã –∏ –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –¥–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Ollama.

            –ö–æ–Ω—Ç–µ–∫—Å—Ç:
            {context}

            –ó–∞–¥–∞—á–∞: –û–ø—Ä–µ–¥–µ–ª–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏.

            –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:
            {{
                "num_gpu": <0 –∏–ª–∏ 1>,
                "num_thread": <–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤>,
                "batch_size": <—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞>,
                "f16_kv": <true/false>,
                "context_length": <–¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞>,
                "semaphore_limit": <–ª–∏–º–∏—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤>,
                "temperature": <—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏>,
                "max_tokens": <–º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤>,
                "keep_alive": <–≤—Ä–µ–º—è –∂–∏–∑–Ω–∏ –º–æ–¥–µ–ª–∏>,
                "request_timeout": <—Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö>,
                "cache_ttl": <–≤—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö>,
                "reasoning": "<–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤>"
            }}
            """
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ LLM —Ä–æ—É—Ç–µ—Ä–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            temp_router = LLMRouter()
            await temp_router.start()
            
            request = LLMRequest(
                service_type=LLMServiceType.LLM_TUNING,
                prompt=prompt,
                context={"task": "system_optimization"},
                model="qwen2.5:7b-instruct-turbo",
                temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                max_tokens=1024,
                use_rag=False  # –û—Ç–∫–ª—é—á–∞–µ–º RAG –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            )
            
            response = await temp_router.process_request(request)
            await temp_router.stop()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
            try:
                import re
                json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
                if json_match:
                    recommendation = json.loads(json_match.group())
                    logger.info(f"üß† LLM recommendation: {recommendation['reasoning']}")
                    return recommendation
            except Exception as e:
                logger.warning(f"Failed to parse LLM recommendation: {e}")
            
        except Exception as e:
            logger.error(f"LLM recommendation failed: {e}")
        
        # Fallback –∫ –±–∞–∑–æ–≤—ã–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º
        return None
    
    async def _search_knowledge_base(self, specs: SystemSpecs) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        relevant_knowledge = []
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å–∏—Å—Ç–µ–º—ã
        if specs.apple_silicon and specs.m1_m2_m4:
            system_type = "apple_silicon_m1_m2_m4"
        elif specs.apple_silicon:
            system_type = "apple_silicon_generic"
        elif specs.gpu_available and specs.gpu_type == "NVIDIA":
            system_type = "nvidia_gpu"
        elif specs.gpu_available and specs.gpu_type == "AMD":
            system_type = "amd_gpu"
        elif specs.memory_gb >= 16:
            system_type = "cpu_only_high_memory"
        else:
            system_type = "cpu_only_low_memory"
        
        # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π
        for kb in self.knowledge_base:
            if kb["system_type"] == system_type:
                relevant_knowledge.append(kb)
                break
        
        return relevant_knowledge
    
    async def optimize_config(self) -> OptimizedConfig:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM"""
        if self.optimized_config:
            return self.optimized_config
        
        specs = await self.analyze_system()
        
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        config = OptimizedConfig(
            model="qwen2.5:7b-instruct-turbo",
            num_gpu=0,
            num_thread=4,
            batch_size=512,
            f16_kv=True,
            temperature=0.7,
            max_tokens=2048,
            context_length=4096,
            keep_alive="2h",
            request_timeout=300,
            semaphore_limit=5,
            cache_ttl=3600
        )
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç LLM
        llm_recommendation = await self._get_llm_recommendation(specs, self.performance_history)
        
        if llm_recommendation:
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π LLM
            config.num_gpu = llm_recommendation.get("num_gpu", config.num_gpu)
            config.num_thread = llm_recommendation.get("num_thread", config.num_thread)
            config.batch_size = llm_recommendation.get("batch_size", config.batch_size)
            config.f16_kv = llm_recommendation.get("f16_kv", config.f16_kv)
            config.context_length = llm_recommendation.get("context_length", config.context_length)
            config.semaphore_limit = llm_recommendation.get("semaphore_limit", config.semaphore_limit)
            config.temperature = llm_recommendation.get("temperature", config.temperature)
            config.max_tokens = llm_recommendation.get("max_tokens", config.max_tokens)
            config.keep_alive = llm_recommendation.get("keep_alive", config.keep_alive)
            config.request_timeout = llm_recommendation.get("request_timeout", config.request_timeout)
            config.cache_ttl = llm_recommendation.get("cache_ttl", config.cache_ttl)
            
            logger.info(f"üß† Applied LLM recommendations: {llm_recommendation.get('reasoning', 'No reasoning provided')}")
        else:
            # Fallback –∫ –ø—Ä–∞–≤–∏–ª–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–Ω–∏–π
            relevant_knowledge = await self._search_knowledge_base(specs)
            
            if relevant_knowledge:
                kb_config = relevant_knowledge[0]["optimal_config"]
                config.num_gpu = kb_config.get("num_gpu", config.num_gpu)
                config.num_thread = min(kb_config.get("num_thread", config.num_thread), specs.cpu_count)
                config.batch_size = kb_config.get("batch_size", config.batch_size)
                config.f16_kv = kb_config.get("f16_kv", config.f16_kv)
                config.context_length = kb_config.get("context_length", config.context_length)
                config.semaphore_limit = kb_config.get("semaphore_limit", config.semaphore_limit)
                
                logger.info(f"üìö Applied knowledge base config: {relevant_knowledge[0]['system_type']}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ –ø–∞–º—è—Ç–∏
        if specs.memory_gb >= 32:
            config.context_length = min(config.context_length * 2, 16384)
            config.batch_size = min(config.batch_size * 1.5, 2048)
            config.semaphore_limit = min(config.semaphore_limit + 2, 10)
            logger.info("üíæ High memory optimization applied")
        elif specs.memory_gb < 8:
            config.context_length = min(config.context_length // 2, 2048)
            config.batch_size = min(config.batch_size // 2, 256)
            config.semaphore_limit = max(config.semaphore_limit - 2, 2)
            logger.info("üíæ Low memory optimization applied")
        
        self.optimized_config = config
        logger.info(f"‚öôÔ∏è Optimized config: {config}")
        return config
    
    async def record_performance(self, response_time: float, success: bool, tokens_used: int):
        """–ó–∞–ø–∏—Å—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        performance_record = {
            "timestamp": datetime.utcnow().isoformat(),
            "response_time": response_time,
            "success": success,
            "tokens_used": tokens_used,
            "config_snapshot": {
                "num_gpu": self.optimized_config.num_gpu if self.optimized_config else 0,
                "num_thread": self.optimized_config.num_thread if self.optimized_config else 4,
                "batch_size": self.optimized_config.batch_size if self.optimized_config else 512,
                "context_length": self.optimized_config.context_length if self.optimized_config else 4096
            }
        }
        
        self.performance_history.append(performance_record)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 100 –∑–∞–ø–∏—Å—è–º–∏
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-100:]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º
        await self._analyze_and_adapt()
    
    async def _analyze_and_adapt(self):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"""
        if len(self.performance_history) < 10:
            return
        
        recent_performance = self.performance_history[-10:]
        avg_response_time = sum(p["response_time"] for p in recent_performance) / len(recent_performance)
        success_rate = sum(1 for p in recent_performance if p["success"]) / len(recent_performance)
        
        # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É—Ö—É–¥—à–∏–ª–∞—Å—å, –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º
        if avg_response_time > 5.0 or success_rate < 0.8:
            logger.warning(f"Performance degradation detected: avg_time={avg_response_time:.2f}s, success_rate={success_rate:.1%}")
            logger.info("üîÑ Triggering adaptive reoptimization...")
            
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ø–µ—Ä–µ—Å—á–µ—Ç–∞
            self.optimized_config = None
            await self.optimize_config()
    
    async def get_environment_variables(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Ollama"""
        config = await self.optimize_config()
        
        env_vars = {
            "OLLAMA_HOST": "0.0.0.0",
            "OLLAMA_ORIGINS": "*",
            "OLLAMA_KEEP_ALIVE": config.keep_alive,
            "OLLAMA_CONTEXT_LENGTH": str(config.context_length),
            "OLLAMA_BATCH_SIZE": str(config.batch_size),
            "OLLAMA_NUM_PARALLEL": str(config.semaphore_limit),
            "REQUEST_TIMEOUT": str(config.request_timeout)
        }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Apple Silicon
        specs = await self.analyze_system()
        if specs.apple_silicon:
            env_vars.update({
                "OLLAMA_METAL": "1",
                "OLLAMA_FLASH_ATTENTION": "1",
                "OLLAMA_KV_CACHE_TYPE": "q8_0",
                "OLLAMA_MEM_FRACTION": "0.9"
            })
        
        return env_vars
    
    async def get_optimization_report(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        specs = await self.analyze_system()
        config = await self.optimize_config()
        
        return {
            "system_specs": {
                "platform": specs.platform,
                "architecture": specs.architecture,
                "cpu_count": specs.cpu_count,
                "memory_gb": specs.memory_gb,
                "gpu_available": specs.gpu_available,
                "gpu_type": specs.gpu_type,
                "apple_silicon": specs.apple_silicon,
                "m1_m2_m4": specs.m1_m2_m4
            },
            "optimized_config": {
                "model": config.model,
                "num_gpu": config.num_gpu,
                "num_thread": config.num_thread,
                "batch_size": config.batch_size,
                "f16_kv": config.f16_kv,
                "temperature": config.temperature,
                "max_tokens": config.max_tokens,
                "context_length": config.context_length,
                "keep_alive": config.keep_alive,
                "request_timeout": config.request_timeout,
                "semaphore_limit": config.semaphore_limit,
                "cache_ttl": config.cache_ttl
            },
            "performance_history": {
                "total_records": len(self.performance_history),
                "recent_avg_response_time": sum(p["response_time"] for p in self.performance_history[-10:]) / min(10, len(self.performance_history)) if self.performance_history else 0,
                "recent_success_rate": sum(1 for p in self.performance_history[-10:] if p["success"]) / min(10, len(self.performance_history)) if self.performance_history else 0
            },
            "knowledge_base_entries": len(self.knowledge_base)
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
system_analyzer = SystemAnalyzer()

@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–∑–∞–ø—Ä–æ—Å–∞"""
    service_type: LLMServiceType
    prompt: str
    context: Optional[Dict[str, Any]] = None
    model: str = "qwen2.5:7b-instruct-turbo"  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è Apple Silicon
    temperature: float = 0.7
    max_tokens: int = 2048
    use_rag: bool = True
    cache_ttl: int = 3600  # 1 —á–∞—Å
    priority: int = 1  # 1-10, –≥–¥–µ 10 - –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–æ—Ç–≤–µ—Ç–∞"""
    content: str
    service_type: LLMServiceType
    model_used: str
    tokens_used: int
    response_time: float
    cached: bool = False
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class LLMRouter:
    """
    üß† –ï–¥–∏–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä LLM —Å RAG-–ø–æ–¥—Ö–æ–¥–æ–º
    
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —É—Å–ø–µ—à–Ω–æ–º –æ–ø—ã—Ç–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
    - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    - RAG —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ fallback
    - –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore: Optional[asyncio.Semaphore] = None
        self.request_queue = asyncio.Queue()
        self.processing = False
        self.optimized_config: Optional[OptimizedConfig] = None
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "cached_responses": 0,
            "avg_response_time": 0.0,
            "system_specs": None,
            "optimization_applied": False
        }
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"""
        await self.start()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        await self.stop()
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞ —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if not self.session:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.optimized_config = await system_analyzer.optimize_config()
            specs = await system_analyzer.analyze_system()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats["system_specs"] = {
                "platform": specs.platform,
                "architecture": specs.architecture,
                "cpu_count": specs.cpu_count,
                "memory_gb": specs.memory_gb,
                "gpu_available": specs.gpu_available,
                "gpu_type": specs.gpu_type,
                "apple_silicon": specs.apple_silicon,
                "m1_m2_m4": specs.m1_m2_m4
            }
            self.stats["optimization_applied"] = True
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞—Ñ–æ—Ä–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º
            self.semaphore = asyncio.Semaphore(self.optimized_config.semaphore_limit)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
            timeout = aiohttp.ClientTimeout(total=self.optimized_config.request_timeout)
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers={"Content-Type": "application/json"}
            )
            
            logger.info(f"üöÄ LLM Router started with optimized config: {self.optimized_config}")
            logger.info(f"üîç System specs: {self.stats['system_specs']}")
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("üõë LLM Router stopped")
    
    def _generate_cache_key(self, request: LLMRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        content = f"{request.service_type.value}:{request.prompt}:{request.model}:{request.temperature}"
        if request.context:
            content += f":{json.dumps(request.context, sort_keys=True)}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    async def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            cached_data = await cache_manager.get(f"llm:{cache_key}")
            if cached_data:
                logger.info(f"üì¶ Cache hit for {cache_key[:16]}...")
                return LLMResponse(**cached_data, cached=True)
        except Exception as e:
            logger.warning(f"Cache error: {e}")
        return None
    
    async def _cache_response(self, cache_key: str, response: LLMResponse, ttl: int):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        try:
            cache_data = {
                "content": response.content,
                "service_type": response.service_type.value,
                "model_used": response.model_used,
                "tokens_used": response.tokens_used,
                "response_time": response.response_time,
                "metadata": response.metadata
            }
            await cache_manager.set(f"llm:{cache_key}", cache_data, ttl)
            logger.info(f"üíæ Cached response for {cache_key[:16]}...")
        except Exception as e:
            logger.warning(f"Cache error: {e}")
    
    async def _generate_rag_context(self, request: LLMRequest) -> str:
        """
        üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è RAG-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —É—Å–ø–µ—à–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
        - –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        - –û–±–æ–≥–∞—â–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        - –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤
        """
        if not request.use_rag:
            return request.prompt
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            embedding = await self._get_embedding(request.prompt)
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
            relevant_knowledge = await self._search_knowledge_base(
                embedding, 
                request.service_type,
                limit=3
            )
            
            if relevant_knowledge:
                context_parts = [request.prompt]
                context_parts.append("\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
                for knowledge in relevant_knowledge:
                    context_parts.append(f"- {knowledge['content']}")
                
                enhanced_prompt = "\n".join(context_parts)
                logger.info(f"üß† RAG enhanced prompt for {request.service_type.value}")
                return enhanced_prompt
            
        except Exception as e:
            logger.warning(f"RAG error: {e}")
        
        return request.prompt
    
    async def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            async with self.semaphore:
                async with self.session.post(
                    f"{settings.OLLAMA_URL}/api/embeddings",
                    json={"model": "qwen2.5:7b-instruct-turbo", "prompt": text}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("embedding", [])
                    else:
                        logger.error(f"Embedding error: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Embedding request failed: {e}")
            return []
    
    async def _search_knowledge_base(
        self, 
        embedding: List[float], 
        service_type: LLMServiceType,
        limit: int = 3
    ) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î (Chroma, Pinecone, etc.)
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
        except Exception as e:
            logger.error(f"Knowledge base search failed: {e}")
            return []
    
    async def _make_ollama_request(self, request: LLMRequest) -> LLMResponse:
        """
        üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ Apple Silicon M4
        
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
        - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        - –¢–∞–π–º–∞—É—Ç—ã –∏ retry
        - Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        start_time = time.time()
        
        try:
            async with self.semaphore:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å RAG
                enhanced_prompt = await self._generate_rag_context(request)
                
                # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama —Å Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                ollama_request = {
                    "model": request.model,
                    "prompt": enhanced_prompt,
                    "stream": False,
                    "options": {
                        "temperature": request.temperature,
                        "num_predict": request.max_tokens,
                        # üöÄ APPLE SILICON M4 –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò
                        "num_gpu": 1,                    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
                        "num_thread": 8,                 # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è M4
                        "num_ctx": 4096,                 # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                        "batch_size": 512,               # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        "f16_kv": True,                  # 16-–±–∏—Ç–Ω—ã–µ –∫–ª—é—á–∏-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        "use_mmap": True,                # Memory mapping –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                        "use_mlock": True,               # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –ø–∞–º—è—Ç–∏
                        "rope_freq_base": 10000,         # RoPE –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞
                        "rope_freq_scale": 0.5,          # RoPE –º–∞—Å—à—Ç–∞–± —á–∞—Å—Ç–æ—Ç—ã
                        "top_p": 0.9,                    # Top-p sampling
                        "top_k": 40,                     # Top-k sampling
                        "repeat_penalty": 1.1,           # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                        "seed": 42                       # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
                    }
                }
                
                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
                async with self.session.post(
                    f"{settings.OLLAMA_URL}/api/generate",
                    json=ollama_request
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        response_time = time.time() - start_time
                        
                        return LLMResponse(
                            content=data.get("response", ""),
                            service_type=request.service_type,
                            model_used=request.model,
                            tokens_used=data.get("eval_count", 0),
                            response_time=response_time,
                            metadata={
                                "prompt_tokens": data.get("prompt_eval_count", 0),
                                "total_duration": data.get("total_duration", 0),
                                "apple_silicon_optimized": True,
                                "gpu_used": True,
                                "batch_size": 512,
                                "context_length": 4096
                            }
                        )
                    else:
                        error_text = await response.text()
                        raise OllamaConnectionError(f"Ollama error {response.status}: {error_text}")
                        
        except asyncio.TimeoutError:
            raise LLMServiceError("Request timeout")
        except Exception as e:
            raise LLMServiceError(f"Request failed: {e}")
    
    async def process_request(self, request: LLMRequest) -> LLMResponse:
        """
        üéØ –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM-–∑–∞–ø—Ä–æ—Å–æ–≤
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π pipeline:
        1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        2. RAG-–æ–±–æ–≥–∞—â–µ–Ω–∏–µ
        3. –ó–∞–ø—Ä–æ—Å –∫ Ollama
        4. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        5. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        """
        self.stats["total_requests"] += 1
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞
            cache_key = self._generate_cache_key(request)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            if request.cache_ttl > 0:
                cached_response = await self._get_cached_response(cache_key)
                if cached_response:
                    self.stats["cached_responses"] += 1
                    return cached_response
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama
            response = await self._make_ollama_request(request)
            
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if request.cache_ttl > 0:
                await self._cache_response(cache_key, response, request.cache_ttl)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats["successful_requests"] += 1
            self.stats["avg_response_time"] = (
                (self.stats["avg_response_time"] * (self.stats["successful_requests"] - 1) + 
                 response.response_time) / self.stats["successful_requests"]
            )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            logger.info(
                f"‚úÖ {request.service_type.value} completed in {response.response_time:.2f}s "
                f"(tokens: {response.tokens_used})"
            )
            
            return response
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"‚ùå {request.service_type.value} failed: {e}")
            
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            return LLMResponse(
                content=f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                service_type=request.service_type,
                model_used=request.model,
                tokens_used=0,
                response_time=time.time() - start_time,
                error=str(e)
            )
    
    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        return {
            **self.stats,
            "active_connections": self.semaphore._value,
            "queue_size": self.request_queue.qsize() if hasattr(self.request_queue, 'qsize') else 0
        }
    
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Ollama"""
        try:
            async with self.session.get(f"{settings.OLLAMA_URL}/api/tags") as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞
llm_router = LLMRouter()

# –£—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def generate_seo_recommendations(prompt: str, context: Optional[Dict] = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.SEO_RECOMMENDATIONS,
        prompt=prompt,
        context=context,
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.6,  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è SEO –∑–∞–¥–∞—á
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def generate_diagram(prompt: str, diagram_type: str = "architecture") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SVG –¥–∏–∞–≥—Ä–∞–º–º—ã —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.DIAGRAM_GENERATION,
        prompt=f"–°–æ–∑–¥–∞–π SVG –¥–∏–∞–≥—Ä–∞–º–º—É —Ç–∏–ø–∞ '{diagram_type}': {prompt}",
        context={"diagram_type": diagram_type},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.8,  # –í—ã—Å–æ–∫–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º
        max_tokens=4096   # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è SVG
    )
    response = await llm_router.process_request(request)
    return response.content

async def analyze_content(content: str, analysis_type: str = "general") -> str:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.CONTENT_ANALYSIS,
        prompt=f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {analysis_type}): {content}",
        context={"analysis_type": analysis_type},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.5,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def run_benchmark(benchmark_type: str, parameters: Dict[str, Any]) -> str:
    """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.BENCHMARK_SERVICE,
        prompt=f"–í—ã–ø–æ–ª–Ω–∏ –±–µ–Ω—á–º–∞—Ä–∫ —Ç–∏–ø–∞ '{benchmark_type}' —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {json.dumps(parameters)}",
        context={"benchmark_type": benchmark_type, "parameters": parameters},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def tune_llm_model(model_config: Dict[str, Any], tuning_params: Dict[str, Any]) -> str:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.LLM_TUNING,
        prompt=f"–ù–∞—Å—Ç—Ä–æ–π –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π: {json.dumps(model_config)} –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {json.dumps(tuning_params)}",
        context={"model_config": model_config, "tuning_params": tuning_params},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.4,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content 