"""
üöÄ –°–ï–†–í–ò–°–´ –ë–ï–ù–ß–ú–ê–†–ö –ú–ò–ö–†–û–°–ï–†–í–ò–°–ê
–û—Å–Ω–æ–≤–Ω–∞—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –º–æ–¥–µ–ª–µ–π
"""

import asyncio
import time
import statistics
import psutil
import httpx
import ollama
import uuid
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Tuple
import structlog
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import textstat

from .config import settings, BENCHMARK_TYPES, MODEL_CONFIGS
from .models import (
    BenchmarkRequest, BenchmarkResult, BenchmarkMetrics, PerformanceMetrics,
    QualityMetrics, SEOMetrics, ReliabilityMetrics, BenchmarkStatus, BenchmarkType
)
from .cache import get_cache

logger = structlog.get_logger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')


class OllamaService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama."""
    
    def __init__(self):
        self.client = ollama.Client(host=settings.ollama_url)
        self.timeout = settings.ollama_timeout
    
    async def check_model_availability(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
        try:
            models = await asyncio.to_thread(self.client.list)
            return any(model['name'] == model_name for model in models['models'])
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    async def generate_response(
        self, 
        model_name: str, 
        prompt: str, 
        parameters: Optional[Dict[str, Any]] = None
    ) -> Tuple[str, float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏."""
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            model_params = MODEL_CONFIGS.get(model_name, {}).get('benchmark_params', {})
            if parameters:
                model_params.update(parameters)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = await asyncio.to_thread(
                self.client.generate,
                model=model_name,
                prompt=prompt,
                **model_params
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            return response['response'], response_time
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            raise
    
    async def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏."""
        try:
            models = await asyncio.to_thread(self.client.list)
            for model in models['models']:
                if model['name'] == model_name:
                    return {
                        'name': model['name'],
                        'size': model.get('size', 0),
                        'modified_at': model.get('modified_at'),
                        'digest': model.get('digest', '')
                    }
            return {}
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return {}


class MetricsCalculator:
    """–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –º–µ—Ç—Ä–∏–∫."""
    
    @staticmethod
    def calculate_performance_metrics(response_times: List[float], tokens_per_response: List[int]) -> PerformanceMetrics:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        if not response_times:
            raise ValueError("–°–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω –æ—Ç–≤–µ—Ç–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
        response_time_avg = statistics.mean(response_times)
        response_time_min = min(response_times)
        response_time_max = max(response_times)
        response_time_std = statistics.stdev(response_times) if len(response_times) > 1 else 0
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤
        total_tokens = sum(tokens_per_response)
        total_time = sum(response_times)
        tokens_per_second = total_tokens / total_time if total_time > 0 else 0
        
        # –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
        throughput = len(response_times) / total_time if total_time > 0 else 0
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
        memory_usage_mb = psutil.virtual_memory().used / (1024 * 1024)
        cpu_usage_percent = psutil.cpu_percent(interval=1)
        
        return PerformanceMetrics(
            response_time_avg=response_time_avg,
            response_time_min=response_time_min,
            response_time_max=response_time_max,
            response_time_std=response_time_std,
            tokens_per_second=tokens_per_second,
            throughput=throughput,
            memory_usage_mb=memory_usage_mb,
            cpu_usage_percent=cpu_usage_percent
        )
    
    @staticmethod
    def calculate_quality_metrics(
        responses: List[str], 
        expected_responses: List[str],
        prompts: List[str]
    ) -> QualityMetrics:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞."""
        if not responses:
            raise ValueError("–°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            all_texts = responses + expected_responses + prompts
            tfidf_matrix = vectorizer.fit_transform(all_texts)
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            response_vectors = tfidf_matrix[:len(responses)]
            expected_vectors = tfidf_matrix[len(responses):len(responses) + len(expected_responses)]
            
            similarities = []
            for resp_vec, exp_vec in zip(response_vectors, expected_vectors):
                similarity = cosine_similarity(resp_vec, exp_vec)[0][0]
                similarities.append(similarity)
            
            semantic_similarity = statistics.mean(similarities) if similarities else 0
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
            semantic_similarity = 0
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
        accuracy_scores = []
        relevance_scores = []
        coherence_scores = []
        fluency_scores = []
        
        for response in responses:
            # –û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)
            accuracy_score = min(1.0, len(response.split()) / 50)  # –ß–µ–º –±–æ–ª—å—à–µ —Å–ª–æ–≤, —Ç–µ–º –ª—É—á—à–µ
            accuracy_scores.append(accuracy_score)
            
            # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            relevance_score = min(1.0, len(set(response.lower().split()) & set(prompts[0].lower().split())) / 10)
            relevance_scores.append(relevance_score)
            
            # –û—Ü–µ–Ω–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
            sentences = sent_tokenize(response)
            coherence_score = min(1.0, len(sentences) / 5)
            coherence_scores.append(coherence_score)
            
            # –û—Ü–µ–Ω–∫–∞ –±–µ–≥–ª–æ—Å—Ç–∏ (—á–∏—Ç–∞–µ–º–æ—Å—Ç—å)
            try:
                flesch_score = textstat.flesch_reading_ease(response)
                fluency_score = max(0, min(1, flesch_score / 100))
            except:
                fluency_score = 0.5
            fluency_scores.append(fluency_score)
        
        # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        accuracy_score = statistics.mean(accuracy_scores)
        relevance_score = statistics.mean(relevance_scores)
        coherence_score = statistics.mean(coherence_scores)
        fluency_score = statistics.mean(fluency_scores)
        
        # –û—Ü–µ–Ω–∫–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π (–ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
        hallucination_rate = 0.1  # –ó–∞–≥–ª—É—à–∫–∞
        
        # –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
        factual_accuracy = 0.8  # –ó–∞–≥–ª—É—à–∫–∞
        
        return QualityMetrics(
            accuracy_score=accuracy_score,
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            fluency_score=fluency_score,
            semantic_similarity=semantic_similarity,
            hallucination_rate=hallucination_rate,
            factual_accuracy=factual_accuracy
        )
    
    @staticmethod
    def calculate_seo_metrics(responses: List[str], prompts: List[str]) -> SEOMetrics:
        """–†–∞—Å—á–µ—Ç SEO-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
        if not responses:
            raise ValueError("–°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        seo_scores = []
        anchor_scores = []
        semantic_scores = []
        linking_scores = []
        keyword_scores = []
        content_scores = []
        intent_scores = []
        
        for response in responses:
            # –ü–æ–Ω–∏–º–∞–Ω–∏–µ SEO (–Ω–∞–ª–∏—á–∏–µ SEO-—Ç–µ—Ä–º–∏–Ω–æ–≤)
            seo_terms = ['seo', 'search', 'optimization', 'ranking', 'keywords', 'meta', 'title']
            seo_count = sum(1 for term in seo_terms if term in response.lower())
            seo_score = min(1.0, seo_count / len(seo_terms))
            seo_scores.append(seo_score)
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–Ω–∫–æ—Ä–æ–≤ (–Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫)
            anchor_count = response.lower().count('href=') + response.lower().count('link')
            anchor_score = min(1.0, anchor_count / 5)
            anchor_scores.append(anchor_score)
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            response_words = set(response.lower().split())
            prompt_words = set(prompts[0].lower().split())
            semantic_score = len(response_words & prompt_words) / len(prompt_words) if prompt_words else 0
            semantic_scores.append(semantic_score)
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
            internal_links = response.lower().count('internal') + response.lower().count('linking')
            linking_score = min(1.0, internal_links / 3)
            linking_scores.append(linking_score)
            
            # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            words = response.lower().split()
            keyword_density = len([w for w in words if w in prompt_words]) / len(words) if words else 0
            keyword_scores.append(min(1.0, keyword_density * 10))
            
            # –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–¥–ª–∏–Ω–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
            content_score = min(1.0, len(response) / 1000)
            content_scores.append(content_score)
            
            # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            intent_score = 0.8  # –ó–∞–≥–ª—É—à–∫–∞
            intent_scores.append(intent_score)
        
        return SEOMetrics(
            seo_understanding=statistics.mean(seo_scores),
            anchor_optimization=statistics.mean(anchor_scores),
            semantic_relevance=statistics.mean(semantic_scores),
            internal_linking_strategy=statistics.mean(linking_scores),
            keyword_density=statistics.mean(keyword_scores),
            content_quality=statistics.mean(content_scores),
            user_intent_alignment=statistics.mean(intent_scores)
        )
    
    @staticmethod
    def calculate_reliability_metrics(
        success_count: int, 
        total_count: int, 
        response_times: List[float]
    ) -> ReliabilityMetrics:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏."""
        success_rate = success_count / total_count if total_count > 0 else 0
        error_rate = 1 - success_rate
        timeout_rate = 0.05  # –ó–∞–≥–ª—É—à–∫–∞
        
        # –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞)
        if len(response_times) > 1:
            consistency_score = 1 - (statistics.stdev(response_times) / statistics.mean(response_times))
            consistency_score = max(0, min(1, consistency_score))
        else:
            consistency_score = 1.0
        
        # –û—Ü–µ–Ω–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        stability_score = 0.9  # –ó–∞–≥–ª—É—à–∫–∞
        
        return ReliabilityMetrics(
            success_rate=success_rate,
            error_rate=error_rate,
            timeout_rate=timeout_rate,
            consistency_score=consistency_score,
            stability_score=stability_score
        )


class BenchmarkService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    
    def __init__(self):
        self.ollama_service = OllamaService()
        self.metrics_calculator = MetricsCalculator()
        self.cache = None
        self.active_benchmarks: Dict[str, asyncio.Task] = {}
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞."""
        self.cache = await get_cache()
        logger.info("BenchmarkService –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def run_benchmark(self, request: BenchmarkRequest) -> List[BenchmarkResult]:
        """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π."""
        results = []
        
        for model_name in request.models:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                if not await self.ollama_service.check_model_availability(model_name):
                    logger.warning(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                    continue
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–∏
                result = await self._run_single_benchmark(request, model_name)
                results.append(result)
                
                # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if self.cache:
                    await self.cache.set_benchmark_result(
                        str(result.benchmark_id), 
                        result.dict(),
                        ttl=settings.cache_ttl
                    )
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π
                error_result = await self._create_error_result(request, model_name, str(e))
                results.append(error_result)
        
        return results
    
    async def _run_single_benchmark(
        self, 
        request: BenchmarkRequest, 
        model_name: str
    ) -> BenchmarkResult:
        """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        benchmark_id = uuid.uuid4()
        started_at = datetime.utcnow()
        
        logger.info(f"–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ {benchmark_id} –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_data = await self._get_test_data(request.benchmark_type)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏
        response_times = []
        responses = []
        tokens_per_response = []
        success_count = 0
        
        for i in range(request.iterations):
            try:
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ—Å—Ç
                test = test_data[i % len(test_data)]
                prompt = test['prompt']
                expected = test.get('expected', '')
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                response, response_time = await self.ollama_service.generate_response(
                    model_name, 
                    prompt, 
                    request.parameters
                )
                
                response_times.append(response_time)
                responses.append(response)
                tokens_per_response.append(len(response.split()))
                success_count += 1
                
                logger.debug(f"–ò—Ç–µ—Ä–∞—Ü–∏—è {i+1}/{request.iterations} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {response_time:.2f}—Å")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {i+1}: {e}")
                response_times.append(30.0)  # –¢–∞–π–º–∞—É—Ç
                responses.append("")
                tokens_per_response.append(0)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        performance_metrics = self.metrics_calculator.calculate_performance_metrics(
            response_times, tokens_per_response
        )
        
        quality_metrics = self.metrics_calculator.calculate_quality_metrics(
            responses, [test.get('expected', '') for test in test_data[:len(responses)]], 
            [test['prompt'] for test in test_data[:len(responses)]]
        )
        
        reliability_metrics = self.metrics_calculator.calculate_reliability_metrics(
            success_count, request.iterations, response_times
        )
        
        # SEO –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è SEO –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
        seo_metrics = None
        if request.benchmark_type in [BenchmarkType.SEO_BASIC, BenchmarkType.SEO_ADVANCED]:
            seo_metrics = self.metrics_calculator.calculate_seo_metrics(responses, [test['prompt'] for test in test_data[:len(responses)]])
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        benchmark_metrics = BenchmarkMetrics(
            performance=performance_metrics,
            quality=quality_metrics,
            seo=seo_metrics,
            reliability=reliability_metrics
        )
        
        completed_at = datetime.utcnow()
        
        return BenchmarkResult(
            benchmark_id=benchmark_id,
            name=request.name,
            description=request.description,
            benchmark_type=request.benchmark_type,
            model_name=model_name,
            status=BenchmarkStatus.COMPLETED,
            metrics=benchmark_metrics,
            iterations=request.iterations,
            parameters=request.parameters,
            started_at=started_at,
            completed_at=completed_at,
            raw_data={
                'responses': responses,
                'response_times': response_times,
                'tokens_per_response': tokens_per_response
            }
        )
    
    async def _create_error_result(
        self, 
        request: BenchmarkRequest, 
        model_name: str, 
        error_message: str
    ) -> BenchmarkResult:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –æ—à–∏–±–∫–æ–π."""
        return BenchmarkResult(
            name=request.name,
            description=request.description,
            benchmark_type=request.benchmark_type,
            model_name=model_name,
            status=BenchmarkStatus.FAILED,
            metrics=BenchmarkMetrics(
                performance=PerformanceMetrics(
                    response_time_avg=0, response_time_min=0, response_time_max=0,
                    response_time_std=0, tokens_per_second=0, throughput=0,
                    memory_usage_mb=0, cpu_usage_percent=0
                ),
                quality=QualityMetrics(
                    accuracy_score=0, relevance_score=0, coherence_score=0,
                    fluency_score=0, semantic_similarity=0, hallucination_rate=0,
                    factual_accuracy=0
                ),
                reliability=ReliabilityMetrics(
                    success_rate=0, error_rate=1, timeout_rate=0,
                    consistency_score=0, stability_score=0
                )
            ),
            iterations=request.iterations,
            parameters=request.parameters,
            started_at=datetime.utcnow(),
            error_message=error_message
        )
    
    async def _get_test_data(self, benchmark_type: BenchmarkType) -> List[Dict[str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        if benchmark_type == BenchmarkType.SEO_BASIC:
            return [
                {
                    'prompt': '–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è —Å–∞–π—Ç–∞ –æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö –≤ –ú–æ—Å–∫–≤–µ',
                    'expected': 'SEO –∞–Ω–∞–ª–∏–∑ –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –º–µ—Ç–∞-—Ç–µ–≥–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—É URL'
                },
                {
                    'prompt': '–°–æ–∑–¥–∞–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1 –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ –¥–æ—Å—Ç–∞–≤–∫–µ –ø–∏—Ü—Ü—ã',
                    'expected': '–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'
                },
                {
                    'prompt': '–ù–∞–ø–∏—à–∏ meta description –¥–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞ –æ–¥–µ–∂–¥—ã',
                    'expected': 'Meta description –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –∏ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–º'
                }
            ]
        elif benchmark_type == BenchmarkType.SEO_ADVANCED:
            return [
                {
                    'prompt': '–†–∞–∑—Ä–∞–±–æ—Ç–∞–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –±–ª–æ–≥–∞ –æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è—Ö',
                    'expected': '–°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–æ–ª–∂–Ω–∞ –≤–∫–ª—é—á–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞, –∞–Ω–∫–æ—Ä—ã, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–∞'
                },
                {
                    'prompt': '–°–æ–∑–¥–∞–π –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω –¥–ª—è —Å–∞–π—Ç–∞ –æ —Ñ–∏—Ç–Ω–µ—Å–µ —Å —É—á–µ—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞',
                    'expected': '–ü–ª–∞–Ω –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å —Ç–µ–º—ã, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞'
                },
                {
                    'prompt': '–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ –Ω–∏—à–µ –æ–Ω–ª–∞–π–Ω-–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è',
                    'expected': '–ê–Ω–∞–ª–∏–∑ –¥–æ–ª–∂–µ–Ω –≤–∫–ª—é—á–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∫–æ–Ω—Ç–µ–Ω—Ç, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã'
                }
            ]
        elif benchmark_type == BenchmarkType.PERFORMANCE:
            return [
                {
                    'prompt': '–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?"',
                    'expected': '–ö—Ä–∞—Ç–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ò–ò'
                },
                {
                    'prompt': '–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"',
                    'expected': 'Hello, how are you?'
                },
                {
                    'prompt': '–ü–æ—Å—á–∏—Ç–∞–π: 2 + 2 = ?',
                    'expected': '4'
                }
            ]
        else:
            return [
                {
                    'prompt': '–û–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –±–ª–æ–∫—á–µ–π–Ω',
                    'expected': '–ü—Ä–æ—Å—Ç–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –±–ª–æ–∫—á–µ–π–Ω–∞'
                },
                {
                    'prompt': '–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å—Ç–∞—Ç—å–∏ –æ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö',
                    'expected': '–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ'
                }
            ]
    
    async def get_benchmark_history(self, limit: int = 50) -> List[BenchmarkResult]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""
        if self.cache:
            cached_results = await self.cache.get_benchmark_history(limit)
            return [BenchmarkResult(**result) for result in cached_results]
        return []
    
    async def get_model_performance(self, model_name: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
        if self.cache:
            return await self.cache.get_model_performance(model_name)
        return None
    
    async def cancel_benchmark(self, benchmark_id: str) -> bool:
        """–û—Ç–º–µ–Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        if benchmark_id in self.active_benchmarks:
            task = self.active_benchmarks[benchmark_id]
            task.cancel()
            del self.active_benchmarks[benchmark_id]
            logger.info(f"–ë–µ–Ω—á–º–∞—Ä–∫ {benchmark_id} –æ—Ç–º–µ–Ω–µ–Ω")
            return True
        return False


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
benchmark_service = BenchmarkService()


async def get_benchmark_service() -> BenchmarkService:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Å–µ—Ä–≤–∏—Å–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    if benchmark_service.cache is None:
        await benchmark_service.initialize()
    return benchmark_service 