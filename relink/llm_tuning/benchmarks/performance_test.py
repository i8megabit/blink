#!/usr/bin/env python3
"""
–ë–µ–Ω—á–º–∞—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è LLM Tuning Microservice

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö API —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤:
- A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
- –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã
- –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
"""

import asyncio
import time
import statistics
import psutil
import aiohttp
import json
from typing import Dict, List, Any
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import numpy as np


class PerformanceBenchmark:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
        self.results = {}
        self.memory_usage = []
        self.cpu_usage = []
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _get_system_metrics(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        process = psutil.Process()
        memory_info = process.memory_info()
        cpu_percent = process.cpu_percent()
        
        self.memory_usage.append(memory_info.rss / 1024 / 1024)  # MB
        self.cpu_usage.append(cpu_percent)
        
        return {
            "memory_mb": memory_info.rss / 1024 / 1024,
            "cpu_percent": cpu_percent,
            "timestamp": datetime.utcnow()
        }
    
    async def _make_request(self, method: str, endpoint: str, data: Dict = None) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏"""
        start_time = time.time()
        start_metrics = self._get_system_metrics()
        
        try:
            if method.upper() == "GET":
                async with self.session.get(f"{self.base_url}{endpoint}") as response:
                    result = await response.json()
            elif method.upper() == "POST":
                async with self.session.post(f"{self.base_url}{endpoint}", json=data) as response:
                    result = await response.json()
            elif method.upper() == "PUT":
                async with self.session.put(f"{self.base_url}{endpoint}", json=data) as response:
                    result = await response.json()
            
            end_time = time.time()
            end_metrics = self._get_system_metrics()
            
            return {
                "success": True,
                "response_time": end_time - start_time,
                "status_code": response.status,
                "start_metrics": start_metrics,
                "end_metrics": end_metrics,
                "result": result
            }
            
        except Exception as e:
            end_time = time.time()
            return {
                "success": False,
                "response_time": end_time - start_time,
                "error": str(e)
            }
    
    async def benchmark_ab_testing(self, num_requests: int = 100) -> Dict:
        """–ë–µ–Ω—á–º–∞—Ä–∫ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print(f"üß™ –ë–µ–Ω—á–º–∞—Ä–∫ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ({num_requests} –∑–∞–ø—Ä–æ—Å–æ–≤)...")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_data = {
            "name": f"Benchmark Test {datetime.utcnow().timestamp()}",
            "description": "Performance benchmark test",
            "model_id": 1,
            "variant_a": "llama2:7b",
            "variant_b": "llama2:13b",
            "traffic_split": 0.5,
            "test_duration_days": 1,
            "success_metrics": ["response_time", "quality_score"],
            "minimum_sample_size": 10
        }
        
        results = []
        
        # –°–æ–∑–¥–∞–Ω–∏–µ A/B —Ç–µ—Å—Ç–∞
        create_result = await self._make_request("POST", "/api/v1/ab-tests", test_data)
        if create_result["success"]:
            test_id = create_result["result"]["id"]
            results.append(("create_ab_test", create_result))
            
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è A/B —Ç–µ—Å—Ç–∞
            select_data = {
                "request_type": "benchmark_test",
                "user_id": "benchmark_user"
            }
            
            for i in range(num_requests):
                select_result = await self._make_request(
                    "POST", f"/api/v1/ab-tests/{test_id}/select-model", select_data
                )
                results.append((f"select_model_{i}", select_result))
                
                # –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                record_data = {
                    "model_variant": "llama2:13b",
                    "metrics": {
                        "response_time": 2.0 + (i % 3) * 0.5,
                        "quality_score": 8.0 + (i % 5) * 0.2,
                        "success": True
                    }
                }
                
                record_result = await self._make_request(
                    "POST", f"/api/v1/ab-tests/{test_id}/record-result", record_data
                )
                results.append((f"record_result_{i}", record_result))
        
        return self._analyze_results("ab_testing", results)
    
    async def benchmark_optimization(self, num_requests: int = 50) -> Dict:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        print(f"‚ö° –ë–µ–Ω—á–º–∞—Ä–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ({num_requests} –∑–∞–ø—Ä–æ—Å–æ–≤)...")
        
        results = []
        
        for i in range(num_requests):
            optimization_data = {
                "model_id": 1,
                "optimization_type": "performance",
                "target_metrics": {
                    "response_time": 1.5 + (i % 3) * 0.2,
                    "quality_score": 8.0 + (i % 5) * 0.1
                },
                "optimization_strategies": ["quantization", "pruning"]
            }
            
            result = await self._make_request("POST", "/api/v1/optimization", optimization_data)
            results.append((f"optimize_model_{i}", result))
            
            if result["success"] and "id" in result["result"]:
                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                status_result = await self._make_request(
                    "GET", f"/api/v1/optimization/{result['result']['id']}"
                )
                results.append((f"get_optimization_{i}", status_result))
        
        return self._analyze_results("optimization", results)
    
    async def benchmark_quality_assessment(self, num_requests: int = 100) -> Dict:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        print(f"üéØ –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ ({num_requests} –∑–∞–ø—Ä–æ—Å–æ–≤)...")
        
        results = []
        
        for i in range(num_requests):
            assessment_data = {
                "model_id": 1,
                "request_text": f"Benchmark request {i}: Create SEO content about AI",
                "response_text": f"AI (Artificial Intelligence) is a field of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence. This includes learning, reasoning, problem-solving, perception, and language understanding. AI has applications in various industries including healthcare, finance, transportation, and entertainment. Machine learning, a subset of AI, enables computers to learn and improve from experience without being explicitly programmed. Deep learning, a type of machine learning, uses neural networks with multiple layers to analyze various factors of data. The field continues to evolve rapidly, with new breakthroughs and applications emerging regularly.",
                "context_documents": [
                    "AI is transforming industries worldwide",
                    "Machine learning is a key component of AI",
                    "Deep learning enables complex pattern recognition"
                ],
                "assessment_criteria": ["relevance", "accuracy", "completeness", "seo_optimization"]
            }
            
            result = await self._make_request("POST", "/api/v1/quality/assess", assessment_data)
            results.append((f"assess_quality_{i}", result))
        
        return self._analyze_results("quality_assessment", results)
    
    async def benchmark_system_health(self, num_requests: int = 200) -> Dict:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
        print(f"üè• –ë–µ–Ω—á–º–∞—Ä–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–¥–æ—Ä–æ–≤—å—è ({num_requests} –∑–∞–ø—Ä–æ—Å–æ–≤)...")
        
        results = []
        
        for i in range(num_requests):
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–¥–æ—Ä–æ–≤—å—è
            health_result = await self._make_request("GET", "/api/v1/health/system")
            results.append((f"get_health_{i}", health_result))
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∑–¥–æ—Ä–æ–≤—å—è (–∫–∞–∂–¥—ã–π 10-–π –∑–∞–ø—Ä–æ—Å)
            if i % 10 == 0:
                history_result = await self._make_request(
                    "GET", "/api/v1/health/system/history?hours=1"
                )
                results.append((f"get_history_{i}", history_result))
        
        return self._analyze_results("system_health", results)
    
    async def benchmark_extended_stats(self, num_requests: int = 100) -> Dict:
        """–ë–µ–Ω—á–º–∞—Ä–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print(f"üìä –ë–µ–Ω—á–º–∞—Ä–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ ({num_requests} –∑–∞–ø—Ä–æ—Å–æ–≤)...")
        
        results = []
        
        for i in range(num_requests):
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏
            model_stats_result = await self._make_request(
                "GET", f"/api/v1/stats/models/1?days={1 + (i % 30)}"
            )
            results.append((f"model_stats_{i}", model_stats_result))
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã (–∫–∞–∂–¥—ã–π 5-–π –∑–∞–ø—Ä–æ—Å)
            if i % 5 == 0:
                system_stats_result = await self._make_request("GET", "/api/v1/stats/system")
                results.append((f"system_stats_{i}", system_stats_result))
        
        return self._analyze_results("extended_stats", results)
    
    def _analyze_results(self, test_name: str, results: List[tuple]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
        response_times = []
        success_count = 0
        error_count = 0
        
        for name, result in results:
            if result["success"]:
                success_count += 1
                response_times.append(result["response_time"])
            else:
                error_count += 1
        
        if response_times:
            analysis = {
                "test_name": test_name,
                "total_requests": len(results),
                "successful_requests": success_count,
                "failed_requests": error_count,
                "success_rate": success_count / len(results),
                "avg_response_time": statistics.mean(response_times),
                "median_response_time": statistics.median(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "std_response_time": statistics.stdev(response_times) if len(response_times) > 1 else 0,
                "requests_per_second": len(response_times) / sum(response_times) if sum(response_times) > 0 else 0
            }
        else:
            analysis = {
                "test_name": test_name,
                "total_requests": len(results),
                "successful_requests": 0,
                "failed_requests": len(results),
                "success_rate": 0,
                "avg_response_time": 0,
                "median_response_time": 0,
                "min_response_time": 0,
                "max_response_time": 0,
                "std_response_time": 0,
                "requests_per_second": 0
            }
        
        self.results[test_name] = analysis
        return analysis
    
    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        report = []
        report.append("üöÄ –û–¢–ß–ï–¢ –û –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò LLM TUNING MICROSERVICE")
        report.append("=" * 60)
        report.append(f"üìÖ –î–∞—Ç–∞: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")
        report.append("")
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if self.memory_usage:
            report.append("üíæ –°–ò–°–¢–ï–ú–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
            report.append(f"   –°—Ä–µ–¥–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {statistics.mean(self.memory_usage):.2f} MB")
            report.append(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {max(self.memory_usage):.2f} MB")
            report.append(f"   –°—Ä–µ–¥–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU: {statistics.mean(self.cpu_usage):.2f}%")
            report.append(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU: {max(self.cpu_usage):.2f}%")
            report.append("")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤
        for test_name, result in self.results.items():
            report.append(f"üìà {test_name.upper()}:")
            report.append(f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {result['total_requests']}")
            report.append(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {result['successful_requests']}")
            report.append(f"   –ù–µ—É–¥–∞—á–Ω—ã—Ö: {result['failed_requests']}")
            report.append(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {result['success_rate']:.2%}")
            report.append(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {result['avg_response_time']:.3f}—Å")
            report.append(f"   –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {result['median_response_time']:.3f}—Å")
            report.append(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {result['min_response_time']:.3f}—Å")
            report.append(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {result['max_response_time']:.3f}—Å")
            report.append(f"   –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {result['std_response_time']:.3f}—Å")
            report.append(f"   –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {result['requests_per_second']:.2f}")
            report.append("")
        
        # –û–±—â–∏–µ –≤—ã–≤–æ–¥—ã
        report.append("üéØ –û–ë–©–ò–ï –í–´–í–û–î–´:")
        
        total_requests = sum(r['total_requests'] for r in self.results.values())
        total_success = sum(r['successful_requests'] for r in self.results.values())
        overall_success_rate = total_success / total_requests if total_requests > 0 else 0
        
        avg_response_times = [r['avg_response_time'] for r in self.results.values() if r['avg_response_time'] > 0]
        overall_avg_response_time = statistics.mean(avg_response_times) if avg_response_times else 0
        
        report.append(f"   –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {overall_success_rate:.2%}")
        report.append(f"   –û–±—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {overall_avg_response_time:.3f}—Å")
        
        if overall_avg_response_time < 1.0:
            report.append("   ‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å!")
        elif overall_avg_response_time < 3.0:
            report.append("   ‚ö†Ô∏è –•–æ—Ä–æ—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        else:
            report.append("   ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
        
        return "\n".join(report)
    
    def plot_results(self, save_path: str = "benchmark_results.png"):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.results:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤")
            return
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
        test_names = list(self.results.keys())
        avg_times = [self.results[name]['avg_response_time'] for name in test_names]
        
        ax1.bar(test_names, avg_times, color='skyblue')
        ax1.set_title('–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –ø–æ —Ç–µ—Å—Ç–∞–º')
        ax1.set_ylabel('–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)')
        ax1.tick_params(axis='x', rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞
        success_rates = [self.results[name]['success_rate'] * 100 for name in test_names]
        
        ax2.bar(test_names, success_rates, color='lightgreen')
        ax2.set_title('–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤')
        ax2.set_ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç (%)')
        ax2.tick_params(axis='x', rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
        rps = [self.results[name]['requests_per_second'] for name in test_names]
        
        ax3.bar(test_names, rps, color='orange')
        ax3.set_title('–ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É')
        ax3.set_ylabel('RPS')
        ax3.tick_params(axis='x', rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        if self.memory_usage:
            ax4.plot(self.memory_usage, color='red')
            ax4.set_title('–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–æ–≤')
            ax4.set_xlabel('–ó–∞–ø—Ä–æ—Å')
            ax4.set_ylabel('–ü–∞–º—è—Ç—å (MB)')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")


async def run_full_benchmark():
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM Tuning Microservice")
    print("=" * 70)
    
    async with PerformanceBenchmark() as benchmark:
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
        await benchmark.benchmark_ab_testing(50)
        await benchmark.benchmark_optimization(20)
        await benchmark.benchmark_quality_assessment(100)
        await benchmark.benchmark_system_health(150)
        await benchmark.benchmark_extended_stats(80)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = benchmark.generate_report()
        print(report)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        with open("benchmark_report.txt", "w", encoding="utf-8") as f:
            f.write(report)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        benchmark.plot_results()
        
        print("\n‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ benchmark_report.txt")
        print("üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ benchmark_results.png")


async def run_specific_benchmark(benchmark_name: str, num_requests: int = 100):
    """–ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    print(f"üéØ –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞: {benchmark_name}")
    
    async with PerformanceBenchmark() as benchmark:
        if benchmark_name == "ab_testing":
            result = await benchmark.benchmark_ab_testing(num_requests)
        elif benchmark_name == "optimization":
            result = await benchmark.benchmark_optimization(num_requests)
        elif benchmark_name == "quality_assessment":
            result = await benchmark.benchmark_quality_assessment(num_requests)
        elif benchmark_name == "system_health":
            result = await benchmark.benchmark_system_health(num_requests)
        elif benchmark_name == "extended_stats":
            result = await benchmark.benchmark_extended_stats(num_requests)
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫: {benchmark_name}")
            return
        
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã {benchmark_name}:")
        for key, value in result.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.3f}")
            else:
                print(f"   {key}: {value}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞
        benchmark_name = sys.argv[1]
        num_requests = int(sys.argv[2]) if len(sys.argv) > 2 else 100
        asyncio.run(run_specific_benchmark(benchmark_name, num_requests))
    else:
        # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞
        asyncio.run(run_full_benchmark()) 