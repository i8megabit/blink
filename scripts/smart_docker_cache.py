#!/usr/bin/env python3
"""
üöÄ –£–ú–ù–´–ô –ö–ï–® –î–õ–Ø DOCKER –°–ë–û–†–ö–ò
–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å–±–æ—Ä–∫—É —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–ª–æ–µ–≤
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ChromaDB –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
"""

import os
import sys
import subprocess
import json
import time
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional
import argparse

class SmartDockerCache:
    """–£–º–Ω—ã–π –∫–µ—à –¥–ª—è Docker —Å–±–æ—Ä–∫–∏"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.cache_dir = self.project_root / ".docker_cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # –°—Ç–∞–±–∏–ª—å–Ω—ã–µ —Å–ª–æ–∏ (–∫–µ—à–∏—Ä—É—é—Ç—Å—è)
        self.stable_layers = [
            "system_deps",
            "python_base",
            "bootstrap_code"
        ]
        
        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–ª–æ–∏ (–ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞—é—Ç—Å—è)
        self.problematic_layers = [
            "ai_deps",
            "db_deps", 
            "monitoring_deps",
            "chromadb_deps"
        ]
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–µ–π—Å—ã –¥–ª—è ChromaDB
        self.chromadb_cases = [
            "chromadb_optimization",
            "vector_embeddings",
            "rag_integration"
        ]
        
        # –ö–µ—à –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self.cache_metadata_file = self.cache_dir / "cache_metadata.json"
        self.cache_metadata = self._load_cache_metadata()
    
    def _load_cache_metadata(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–µ—à–∞"""
        if self.cache_metadata_file.exists():
            try:
                with open(self.cache_metadata_file, 'r') as f:
                    data = json.load(f)
                    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                    if "layer_hashes" not in data:
                        data["layer_hashes"] = {}
                    if "build_times" not in data:
                        data["build_times"] = {}
                    return data
            except:
                pass
        return {
            "last_cleanup": None,
            "cache_hits": 0,
            "cache_misses": 0,
            "build_times": {},
            "layer_hashes": {}
        }
    
    def _save_cache_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫–µ—à–∞"""
        with open(self.cache_metadata_file, 'w') as f:
            json.dump(self.cache_metadata, f, indent=2)
    
    def _get_layer_hash(self, service_name: str, layer_type: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à–∞ —Å–ª–æ—è –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        import hashlib
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        base_files = [
            "Dockerfile.base",
            "requirements-base.txt",
            "bootstrap/config.py"
        ]
        
        # –§–∞–π–ª—ã —Å–µ—Ä–≤–∏—Å–∞
        service_files = [
            f"{service_name}/Dockerfile",
            f"{service_name}/requirements.txt",
            f"{service_name}/app/main.py"
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        files_to_hash = []
        
        for file_path in base_files + service_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                files_to_hash.append(str(full_path))
        
        # –°–æ–∑–¥–∞–µ–º —Ö–µ—à
        content = "|".join(files_to_hash)
        return hashlib.md5(content.encode()).hexdigest()
    
    def _check_cache_validity(self, service_name: str, layer_type: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∫–µ—à–∞"""
        layer_hash = self._get_layer_hash(service_name, layer_type)
        cached_hash = self.cache_metadata["layer_hashes"].get(f"{service_name}_{layer_type}")
        
        return layer_hash == cached_hash
    
    def _update_cache_hash(self, service_name: str, layer_type: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ö–µ—à–∞ –∫–µ—à–∞"""
        layer_hash = self._get_layer_hash(service_name, layer_type)
        self.cache_metadata["layer_hashes"][f"{service_name}_{layer_type}"] = layer_hash
        self._save_cache_metadata()
    
    def build_base_image(self, force: bool = False) -> bool:
        """–°–±–æ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞ —Å —É–º–Ω—ã–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        
        print("üî® –°–±–æ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞...")
        
        if not force and self._check_cache_validity("base", "system_deps"):
            print("‚úÖ –ö–µ—à –±–∞–∑–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞ –∞–∫—Ç—É–∞–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–±–æ—Ä–∫—É")
            self.cache_metadata["cache_hits"] += 1
            self._save_cache_metadata()
            return True
        
        start_time = time.time()
        
        try:
            # –°–±–æ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞
            cmd = [
                "docker", "build",
                "-f", "Dockerfile.base",
                "-t", "eberil/relink-base:latest",
                "--build-arg", "BUILDKIT_INLINE_CACHE=1",
                "."
            ]
            
            if force:
                cmd.extend(["--no-cache"])
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                build_time = time.time() - start_time
                self.cache_metadata["build_times"]["base"] = build_time
                self._update_cache_hash("base", "system_deps")
                self.cache_metadata["cache_misses"] += 1
                self._save_cache_metadata()
                
                print(f"‚úÖ –ë–∞–∑–æ–≤—ã–π –æ–±—Ä–∞–∑ —Å–æ–±—Ä–∞–Ω –∑–∞ {build_time:.2f}—Å")
                return True
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏ –±–∞–∑–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –±–∞–∑–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞: {e}")
            return False
    
    def build_service(self, service_name: str, force: bool = False) -> bool:
        """–°–±–æ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Å —É–º–Ω—ã–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        
        print(f"üî® –°–±–æ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ {service_name}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Å–ª–æ–µ–≤
        cache_valid = True
        for layer in self.stable_layers:
            if not self._check_cache_validity(service_name, layer):
                cache_valid = False
                break
        
        if not force and cache_valid:
            print(f"‚úÖ –ö–µ—à —Å–µ—Ä–≤–∏—Å–∞ {service_name} –∞–∫—Ç—É–∞–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–±–æ—Ä–∫—É")
            self.cache_metadata["cache_hits"] += 1
            self._save_cache_metadata()
            return True
        
        start_time = time.time()
        
        try:
            # –°–±–æ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–∞
            service_dir = self.project_root / service_name
            if not service_dir.exists():
                print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–µ—Ä–≤–∏—Å–∞ {service_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
            
            cmd = [
                "docker", "build",
                "-f", f"{service_name}/Dockerfile",
                "-t", f"eberil/relink-{service_name}:latest",
                "--build-arg", "BUILDKIT_INLINE_CACHE=1",
                "."
            ]
            
            if force:
                cmd.extend(["--no-cache"])
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                build_time = time.time() - start_time
                self.cache_metadata["build_times"][service_name] = build_time
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ö–µ—à–∏ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤
                for layer in self.stable_layers + self.problematic_layers:
                    self._update_cache_hash(service_name, layer)
                
                self.cache_metadata["cache_misses"] += 1
                self._save_cache_metadata()
                
                print(f"‚úÖ –°–µ—Ä–≤–∏—Å {service_name} —Å–æ–±—Ä–∞–Ω –∑–∞ {build_time:.2f}—Å")
                return True
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏ —Å–µ—Ä–≤–∏—Å–∞ {service_name}: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ —Å–µ—Ä–≤–∏—Å–∞ {service_name}: {e}")
            return False
    
    def build_all_services(self, force: bool = False) -> bool:
        """–°–±–æ—Ä–∫–∞ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
        
        print("üöÄ –°–±–æ—Ä–∫–∞ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤...")
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –±–∞–∑–æ–≤—ã–π –æ–±—Ä–∞–∑
        if not self.build_base_image(force):
            return False
        
        # –°–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è —Å–±–æ—Ä–∫–∏
        services = ["router", "backend", "frontend", "monitoring", "testing"]
        
        success_count = 0
        for service in services:
            if self.build_service(service, force):
                success_count += 1
        
        print(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {success_count}/{len(services)} —Å–µ—Ä–≤–∏—Å–æ–≤")
        return success_count == len(services)
    
    def cleanup_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞"""
        
        print("üßπ –û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞...")
        
        try:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –æ–±—Ä–∞–∑—ã
            subprocess.run(["docker", "system", "prune", "-f"], capture_output=True)
            
            # –û—á–∏—â–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–µ—à–∞
            self.cache_metadata = {
                "last_cleanup": time.time(),
                "cache_hits": 0,
                "cache_misses": 0,
                "build_times": {},
                "layer_hashes": {}
            }
            self._save_cache_metadata()
            
            print("‚úÖ –ö–µ—à –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫–µ—à–∞: {e}")
    
    def show_stats(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–µ—à–∞"""
        
        print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞:")
        print(f"  –•–∏—Ç—ã –∫–µ—à–∞: {self.cache_metadata['cache_hits']}")
        print(f"  –ü—Ä–æ–º–∞—Ö–∏ –∫–µ—à–∞: {self.cache_metadata['cache_misses']}")
        
        if self.cache_metadata['cache_hits'] + self.cache_metadata['cache_misses'] > 0:
            hit_rate = self.cache_metadata['cache_hits'] / (self.cache_metadata['cache_hits'] + self.cache_metadata['cache_misses']) * 100
            print(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–µ—à–∞: {hit_rate:.1f}%")
        
        print("\n‚è±Ô∏è –í—Ä–µ–º—è —Å–±–æ—Ä–∫–∏:")
        for service, build_time in self.cache_metadata['build_times'].items():
            print(f"  {service}: {build_time:.2f}—Å")
        
        if self.cache_metadata['last_cleanup']:
            cleanup_time = time.time() - self.cache_metadata['last_cleanup']
            print(f"\nüßπ –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—á–∏—Å—Ç–∫–∞: {cleanup_time/3600:.1f} —á–∞—Å–æ–≤ –Ω–∞–∑–∞–¥")
    
    def chromadb_optimization(self):
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è ChromaDB"""
        
        print("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ChromaDB...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ñ–∞–π–ª—ã ChromaDB
        chromadb_files = [
            "bootstrap/rag_manager.py",
            "bootstrap/rag_service.py",
            "bootstrap/routers/router_router.py"
        ]
        
        files_changed = False
        for file_path in chromadb_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ —Ñ–∞–π–ª—ã ChromaDB
                if not self._check_cache_validity("chromadb", "rag_integration"):
                    files_changed = True
                    break
        
        if files_changed:
            print("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ ChromaDB, –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º —Ä–æ—É—Ç–µ—Ä...")
            return self.build_service("router", force=True)
        else:
            print("‚úÖ ChromaDB —Ñ–∞–π–ª—ã –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å")
            return True
    
    def auto_cleanup_collections(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π ChromaDB"""
        
        print("üßπ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π ChromaDB...")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Ä–æ—É—Ç–µ—Ä
            result = subprocess.run(
                ["docker", "ps", "--filter", "name=relink-router", "--format", "{{.Names}}"],
                capture_output=True, text=True
            )
            
            if "relink-router" in result.stdout:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É —á–µ—Ä–µ–∑ API
                cleanup_cmd = [
                    "curl", "-X", "POST",
                    "http://localhost:8001/api/v1/rag/cleanup"
                ]
                
                subprocess.run(cleanup_cmd, capture_output=True)
                print("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π –∑–∞–ø—É—â–µ–Ω–∞")
            else:
                print("‚ö†Ô∏è –†–æ—É—Ç–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω, –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏: {e}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    parser = argparse.ArgumentParser(description="–£–º–Ω—ã–π –∫–µ—à –¥–ª—è Docker —Å–±–æ—Ä–∫–∏")
    parser.add_argument("--build-all", action="store_true", help="–°–æ–±—Ä–∞—Ç—å –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã")
    parser.add_argument("--build-base", action="store_true", help="–°–æ–±—Ä–∞—Ç—å –±–∞–∑–æ–≤—ã–π –æ–±—Ä–∞–∑")
    parser.add_argument("--build-service", type=str, help="–°–æ–±—Ä–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–µ—Ä–≤–∏—Å")
    parser.add_argument("--force", action="store_true", help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞")
    parser.add_argument("--clean", action="store_true", help="–û—á–∏—Å—Ç–∏—Ç—å –∫–µ—à")
    parser.add_argument("--stats", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É")
    parser.add_argument("--chromadb-optimization", action="store_true", help="–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ChromaDB")
    parser.add_argument("--auto-cleanup", action="store_true", help="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π")
    
    args = parser.parse_args()
    
    cache = SmartDockerCache()
    
    if args.clean:
        cache.cleanup_cache()
    elif args.stats:
        cache.show_stats()
    elif args.chromadb_optimization:
        cache.chromadb_optimization()
    elif args.auto_cleanup:
        cache.auto_cleanup_collections()
    elif args.build_base:
        cache.build_base_image(args.force)
    elif args.build_service:
        cache.build_service(args.build_service, args.force)
    elif args.build_all:
        cache.build_all_services(args.force)
    else:
        parser.print_help()

if __name__ == "__main__":
    main() 