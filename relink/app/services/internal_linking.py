"""
üîó –°–µ—Ä–≤–∏—Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏
"""

import asyncio
import aiohttp
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import os

from ..models import (
    InternalLink, Post, InternalLinkAnalysis, PostAnalysis,
    SEOAnalysisResult, Recommendation, FocusArea, Priority
)

logger = logging.getLogger(__name__)

class InternalLinkingService:
    """–°–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫"""
    
    def __init__(self):
        self.indexed_data: Dict[str, Any] = {}
        self.cache_dir = "cache"
        os.makedirs(self.cache_dir, exist_ok=True)
    
    async def index_domain(self, domain: str) -> Dict[str, Any]:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞"""
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–æ–º–µ–Ω–∞: {domain}")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
            cache_file = os.path.join(self.cache_dir, f"{domain}_index.json")
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.indexed_data[domain] = json.load(f)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {domain}")
                return self.indexed_data[domain]
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
            base_url = f"https://{domain}"
            indexer = DomainIndexer(base_url)
            result = await indexer.index_domain()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
            self.indexed_data[domain] = result
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {domain} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {result['posts_count']} –ø–æ—Å—Ç–æ–≤")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {domain}: {e}")
            raise
    
    async def get_indexing_status(self, domain: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        cache_file = os.path.join(self.cache_dir, f"{domain}_index.json")
        
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return {
                "status": "completed",
                "domain": domain,
                "posts_count": data.get("posts_count", 0),
                "internal_links_count": data.get("internal_links_count", 0),
                "indexed_at": data.get("indexed_at", ""),
                "last_updated": datetime.fromtimestamp(os.path.getmtime(cache_file)).isoformat()
            }
        else:
            return {
                "status": "not_indexed",
                "domain": domain,
                "message": "–î–æ–º–µ–Ω –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω"
            }
    
    async def analyze_domain(
        self, 
        domain: str, 
        include_posts: bool = True,
        include_recommendations: bool = True
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞"""
        logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–º–µ–Ω: {domain}")
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–æ–º–µ–Ω –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω
        if domain not in self.indexed_data:
            await self.index_domain(domain)
        
        data = self.indexed_data[domain]
        
        analysis = {
            "domain": domain,
            "total_posts": data["posts_count"],
            "total_internal_links": data["internal_links_count"],
            "seo_data": data["seo_data"],
            "internal_links_analysis": await self._analyze_internal_links(data),
        }
        
        if include_posts:
            analysis["posts_analysis"] = await self._analyze_posts(data["posts"])
        
        if include_recommendations:
            analysis["recommendations"] = await self._generate_basic_recommendations(data)
        
        return analysis
    
    async def _analyze_internal_links(self, data: Dict[str, Any]) -> InternalLinkAnalysis:
        """–ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫"""
        internal_links = data.get("internal_links", [])
        posts = data.get("posts", [])
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
        all_links = []
        for post in posts:
            all_links.extend(post.get("internal_links", []))
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        link_distribution = {}
        for link in all_links:
            link_type = self._determine_link_type(link)
            link_distribution[link_type] = link_distribution.get(link_type, 0) + 1
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –±–µ–∑ –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
        all_targets = set()
        for link in all_links:
            all_targets.add(link["to_url"])
        
        all_sources = set()
        for link in all_links:
            all_sources.add(link["from_url"])
        
        orphan_pages = list(all_sources - all_targets)
        
        # –°–∞–º—ã–µ —Å—Å—ã–ª–∞–µ–º—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        target_counts = {}
        for link in all_links:
            target = link["to_url"]
            target_counts[target] = target_counts.get(target, 0) + 1
        
        most_linked = sorted(target_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        most_linked_pages = [
            {"url": url, "incoming_links": count} 
            for url, count in most_linked
        ]
        
        return InternalLinkAnalysis(
            total_links=len(all_links),
            unique_targets=len(all_targets),
            orphan_pages=orphan_pages,
            most_linked_pages=most_linked_pages,
            link_distribution=link_distribution
        )
    
    async def _analyze_posts(self, posts: List[Dict[str, Any]]) -> List[PostAnalysis]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç–æ–≤"""
        post_analyses = []
        
        for post in posts:
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º SEO –æ—Ü–µ–Ω–∫—É
            seo_score = await self._calculate_seo_score(post)
            
            # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—ã
            issues = await self._identify_post_issues(post)
            
            analysis = PostAnalysis(
                url=post["url"],
                title=post["title"],
                word_count=post["word_count"],
                internal_links_count=len(post.get("internal_links", [])),
                seo_score=seo_score,
                issues=issues
            )
            post_analyses.append(analysis)
        
        return post_analyses
    
    async def _calculate_seo_score(self, post: Dict[str, Any]) -> float:
        """–†–∞—Å—á–µ—Ç SEO –æ—Ü–µ–Ω–∫–∏ –ø–æ—Å—Ç–∞"""
        score = 0.0
        
        # –û—Ü–µ–Ω–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (0-25 –±–∞–ª–ª–æ–≤)
        title = post.get("title", "")
        if title:
            if 30 <= len(title) <= 60:
                score += 25
            elif 20 <= len(title) <= 70:
                score += 15
            else:
                score += 5
        
        # –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (0-30 –±–∞–ª–ª–æ–≤)
        word_count = post.get("word_count", 0)
        if word_count >= 300:
            score += 30
        elif word_count >= 150:
            score += 20
        elif word_count >= 50:
            score += 10
        
        # –û—Ü–µ–Ω–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ (0-25 –±–∞–ª–ª–æ–≤)
        internal_links = post.get("internal_links", [])
        if 2 <= len(internal_links) <= 5:
            score += 25
        elif len(internal_links) > 5:
            score += 15
        elif len(internal_links) == 1:
            score += 10
        
        # –û—Ü–µ–Ω–∫–∞ meta description (0-20 –±–∞–ª–ª–æ–≤)
        seo_data = post.get("seo_data", {})
        meta_desc = seo_data.get("meta_description", "")
        if meta_desc:
            if 120 <= len(meta_desc) <= 160:
                score += 20
            elif 100 <= len(meta_desc) <= 180:
                score += 15
            else:
                score += 5
        
        return min(score, 100.0)
    
    async def _identify_post_issues(self, post: Dict[str, Any]) -> List[str]:
        """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –≤ –ø–æ—Å—Ç–µ"""
        issues = []
        
        title = post.get("title", "")
        if not title:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫")
        elif len(title) < 30:
            issues.append("–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
        elif len(title) > 60:
            issues.append("–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        word_count = post.get("word_count", 0)
        if word_count < 300:
            issues.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–º–µ–Ω–µ–µ 300 —Å–ª–æ–≤)")
        
        internal_links = post.get("internal_links", [])
        if len(internal_links) == 0:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏")
        elif len(internal_links) > 10:
            issues.append("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫")
        
        seo_data = post.get("seo_data", {})
        if not seo_data.get("meta_description"):
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description")
        
        return issues
    
    async def _determine_link_type(self, link: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Å—ã–ª–∫–∏"""
        anchor_text = link.get("anchor_text", "").lower()
        
        if any(word in anchor_text for word in ["—á–∏—Ç–∞—Ç—å", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "–¥–∞–ª–µ–µ"]):
            return "cta"
        elif any(word in anchor_text for word in ["–≥–ª–∞–≤–Ω–∞—è", "–æ –Ω–∞—Å", "–∫–æ–Ω—Ç–∞–∫—Ç—ã"]):
            return "navigation"
        elif any(word in anchor_text for word in ["—Å–≤—è–∑–∞–Ω–Ω—ã–µ", "–ø–æ—Ö–æ–∂–∏–µ", "—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º"]):
            return "related"
        else:
            return "content"
    
    async def _generate_basic_recommendations(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        posts = data.get("posts", [])
        internal_links = data.get("internal_links", [])
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        short_posts = [p for p in posts if p.get("word_count", 0) < 300]
        if short_posts:
            recommendations.append({
                "type": "content_optimization",
                "priority": "high",
                "title": "–£–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
                "description": f"–ù–∞–π–¥–µ–Ω–æ {len(short_posts)} –ø–æ—Å—Ç–æ–≤ —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –æ–±—ä–µ–º–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
                "action": "–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å—Ç—ã"
            })
        
        # –ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        posts_without_links = [p for p in posts if not p.get("internal_links")]
        if posts_without_links:
            recommendations.append({
                "type": "internal_linking",
                "priority": "medium",
                "title": "–î–æ–±–∞–≤–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏",
                "description": f"–ù–∞–π–¥–µ–Ω–æ {len(posts_without_links)} –ø–æ—Å—Ç–æ–≤ –±–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫",
                "action": "–î–æ–±–∞–≤–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –≤ –ø–æ—Å—Ç—ã"
            })
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        bad_titles = [p for p in posts if len(p.get("title", "")) < 30 or len(p.get("title", "")) > 60]
        if bad_titles:
            recommendations.append({
                "type": "on_page_seo",
                "priority": "medium",
                "title": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏",
                "description": f"–ù–∞–π–¥–µ–Ω–æ {len(bad_titles)} –ø–æ—Å—Ç–æ–≤ —Å –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏",
                "action": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ (30-60 —Å–∏–º–≤–æ–ª–æ–≤)"
            })
        
        return recommendations
    
    async def generate_recommendations(
        self, 
        domain: str, 
        focus_areas: List[FocusArea] = None,
        priority: Priority = Priority.MEDIUM
    ) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è {domain}")
        
        if domain not in self.indexed_data:
            await self.index_domain(domain)
        
        data = self.indexed_data[domain]
        recommendations = []
        
        # –ë–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        basic_recs = await self._generate_basic_recommendations(data)
        recommendations.extend(basic_recs)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±–ª–∞—Å—Ç—è–º —Ñ–æ–∫—É—Å–∞
        if focus_areas:
            for area in focus_areas:
                area_recs = await self._generate_area_recommendations(data, area)
                recommendations.extend(area_recs)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        if priority != Priority.LOW:
            recommendations = [r for r in recommendations if r.get("priority") != "low"]
        
        return recommendations
    
    async def _generate_area_recommendations(self, data: Dict[str, Any], area: FocusArea) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏"""
        recommendations = []
        
        if area == FocusArea.INTERNAL_LINKING:
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
            internal_links = data.get("internal_links", [])
            if len(internal_links) < 10:
                recommendations.append({
                    "type": "internal_linking",
                    "priority": "high",
                    "title": "–£–ª—É—á—à–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫",
                    "description": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –Ω–∞–≤–∏–≥–∞—Ü–∏–∏",
                    "action": "–°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏"
                })
        
        elif area == FocusArea.CONTENT_OPTIMIZATION:
            # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            posts = data.get("posts", [])
            avg_word_count = sum(p.get("word_count", 0) for p in posts) / len(posts) if posts else 0
            
            if avg_word_count < 500:
                recommendations.append({
                    "type": "content_optimization",
                    "priority": "medium",
                    "title": "–£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
                    "description": f"–°—Ä–µ–¥–Ω–∏–π –æ–±—ä–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ({avg_word_count:.0f} —Å–ª–æ–≤) –Ω–∏–∂–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ",
                    "action": "–£–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
                })
        
        elif area == FocusArea.TECHNICAL_SEO:
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            seo_data = data.get("seo_data", {})
            if not seo_data.get("meta_description"):
                recommendations.append({
                    "type": "technical_seo",
                    "priority": "high",
                    "title": "–î–æ–±–∞–≤–∏—Ç—å meta description",
                    "description": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ",
                    "action": "–î–æ–±–∞–≤–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π meta description"
                })
        
        return recommendations
    
    async def get_internal_links(self, domain: str, limit: int = 50) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–æ–º–µ–Ω–∞"""
        if domain not in self.indexed_data:
            await self.index_domain(domain)
        
        data = self.indexed_data[domain]
        return data.get("internal_links", [])[:limit]
    
    async def get_posts(self, domain: str, limit: int = 20) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å—Ç–æ–≤ –¥–æ–º–µ–Ω–∞"""
        if domain not in self.indexed_data:
            await self.index_domain(domain)
        
        data = self.indexed_data[domain]
        return data.get("posts", [])[:limit]
    
    async def analyze_seo_content(
        self, 
        url: str, 
        title: str, 
        content: str, 
        meta_description: str = None
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ SEO –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        analysis = {
            "url": url,
            "title_analysis": await self._analyze_title(title),
            "content_analysis": await self._analyze_content(content),
            "meta_description_analysis": await self._analyze_meta_description(meta_description) if meta_description else None,
            "overall_score": 0.0,
            "recommendations": []
        }
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É
        scores = []
        if analysis["title_analysis"]:
            scores.append(analysis["title_analysis"]["score"])
        if analysis["content_analysis"]:
            scores.append(analysis["content_analysis"]["score"])
        if analysis["meta_description_analysis"]:
            scores.append(analysis["meta_description_analysis"]["score"])
        
        if scores:
            analysis["overall_score"] = sum(scores) / len(scores)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        analysis["recommendations"] = await self._generate_content_recommendations(analysis)
        
        return analysis
    
    async def _analyze_title(self, title: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        score = 0.0
        issues = []
        
        if not title:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫")
        else:
            length = len(title)
            if length < 30:
                score = 30
                issues.append("–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
            elif 30 <= length <= 60:
                score = 100
            elif length <= 70:
                score = 80
            else:
                score = 40
                issues.append("–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        return {
            "score": score,
            "length": len(title),
            "issues": issues,
            "recommendation": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 30-60 —Å–∏–º–≤–æ–ª–æ–≤"
        }
    
    async def _analyze_content(self, content: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        score = 0.0
        issues = []
        
        if not content:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç")
        else:
            word_count = len(content.split())
            if word_count < 300:
                score = 30
                issues.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            elif word_count < 500:
                score = 60
            elif word_count < 1000:
                score = 80
            else:
                score = 100
        
        return {
            "score": score,
            "word_count": len(content.split()) if content else 0,
            "issues": issues,
            "recommendation": "–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –æ–±—ä–µ–º: 500+ —Å–ª–æ–≤"
        }
    
    async def _analyze_meta_description(self, meta_description: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ meta description"""
        score = 0.0
        issues = []
        
        if not meta_description:
            issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç meta description")
        else:
            length = len(meta_description)
            if length < 120:
                score = 40
                issues.append("Meta description —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
            elif 120 <= length <= 160:
                score = 100
            elif length <= 180:
                score = 80
            else:
                score = 60
                issues.append("Meta description —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π")
        
        return {
            "score": score,
            "length": len(meta_description),
            "issues": issues,
            "recommendation": "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: 120-160 —Å–∏–º–≤–æ–ª–æ–≤"
        }
    
    async def _generate_content_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
        recommendations = []
        
        title_analysis = analysis.get("title_analysis")
        if title_analysis and title_analysis.get("issues"):
            recommendations.extend(title_analysis["issues"])
        
        content_analysis = analysis.get("content_analysis")
        if content_analysis and content_analysis.get("issues"):
            recommendations.extend(content_analysis["issues"])
        
        meta_analysis = analysis.get("meta_description_analysis")
        if meta_analysis and meta_analysis.get("issues"):
            recommendations.extend(meta_analysis["issues"])
        
        return recommendations
    
    async def get_dashboard_data(self, domain: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
        if domain not in self.indexed_data:
            await self.index_domain(domain)
        
        data = self.indexed_data[domain]
        posts = data.get("posts", [])
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã
        post_analyses = await self._analyze_posts(posts)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω—é—é SEO –æ—Ü–µ–Ω–∫—É
        avg_seo_score = sum(p.seo_score for p in post_analyses) / len(post_analyses) if post_analyses else 0
        
        # –¢–æ–ø –ø–æ—Å—Ç—ã
        top_posts = sorted(post_analyses, key=lambda x: x.seo_score, reverse=True)[:5]
        
        # –¢–æ–ø —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = await self._generate_basic_recommendations(data)
        top_recommendations = [r for r in recommendations if r.get("priority") == "high"][:3]
        
        return {
            "total_posts": len(posts),
            "total_internal_links": data.get("internal_links_count", 0),
            "average_seo_score": avg_seo_score,
            "top_posts": [p.dict() for p in top_posts],
            "top_recommendations": top_recommendations,
            "recent_activity": [
                {
                    "type": "indexing",
                    "timestamp": data.get("indexed_at", ""),
                    "description": f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(posts)} –ø–æ—Å—Ç–æ–≤"
                }
            ]
        }
    
    async def export_analysis_json(self, domain: str) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON"""
        if domain not in self.indexed_data:
            await self.index_domain(domain)
        
        data = self.indexed_data[domain]
        return json.dumps(data, ensure_ascii=False, indent=2)
    
    async def export_analysis_csv(self, domain: str) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤ CSV"""
        if domain not in self.indexed_data:
            await self.index_domain(domain)
        
        data = self.indexed_data[domain]
        posts = data.get("posts", [])
        
        # –°–æ–∑–¥–∞–µ–º CSV
        csv_lines = ["URL,Title,Word Count,Internal Links,SEO Score"]
        
        for post in posts:
            seo_score = await self._calculate_seo_score(post)
            csv_lines.append(
                f'"{post["url"]}","{post["title"]}",{post["word_count"]},'
                f'{len(post.get("internal_links", []))},{seo_score:.1f}'
            )
        
        return "\n".join(csv_lines)

class DomainIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–æ–º–µ–Ω–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è SEO –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.visited_urls: set = set()
        self.posts: List[Dict[str, Any]] = []
        self.internal_links: List[Dict[str, Any]] = []
        self.seo_data: Dict[str, Any] = {}
        
    async def index_domain(self) -> Dict[str, Any]:
        """–ü–æ–ª–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞"""
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–æ–º–µ–Ω–∞: {self.base_url}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            main_page = await self.fetch_page(self.base_url)
            if not main_page:
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º SEO –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            self.seo_data = await self.extract_seo_data(main_page, self.base_url)
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ—Å—Ç—ã
            post_urls = await self.find_post_urls(main_page)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(post_urls)} –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            
            # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –ø–æ—Å—Ç
            for url in post_urls[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                post_data = await self.index_post(url)
                if post_data:
                    self.posts.append(post_data)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            await self.analyze_internal_links()
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
            report = {
                "domain": self.domain,
                "base_url": self.base_url,
                "indexed_at": datetime.now().isoformat(),
                "seo_data": self.seo_data,
                "posts_count": len(self.posts),
                "posts": self.posts,
                "internal_links_count": len(self.internal_links),
                "internal_links": self.internal_links,
                "visited_urls_count": len(self.visited_urls)
            }
            
            logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(self.posts)} –ø–æ—Å—Ç–æ–≤")
            return report
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
            raise
    
    async def fetch_page(self, url: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                }
                async with session.get(url, headers=headers, timeout=30) as response:
                    if response.status == 200:
                        self.visited_urls.add(url)
                        return await response.text()
                    else:
                        logger.warning(f"–û—à–∏–±–∫–∞ {response.status} –¥–ª—è {url}")
                        return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {url}: {e}")
            return ""
    
    async def extract_seo_data(self, html: str, url: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ SEO –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ú–µ—Ç–∞-—Ç–µ–≥–∏
        title = soup.find('title')
        description = soup.find('meta', attrs={'name': 'description'})
        keywords = soup.find('meta', attrs={'name': 'keywords'})
        
        # Open Graph
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        og_description = soup.find('meta', attrs={'property': 'og:description'})
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        structured_data = []
        for script in soup.find_all('script', type='application/ld+json'):
            try:
                data = json.loads(script.string)
                structured_data.append(data)
            except:
                pass
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        headers = {}
        for i in range(1, 7):
            h_tags = soup.find_all(f'h{i}')
            headers[f'h{i}'] = [h.get_text(strip=True) for h in h_tags]
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = []
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src')
            if src:
                images.append({
                    'src': urljoin(url, src),
                    'alt': img.get('alt', ''),
                    'title': img.get('title', '')
                })
        
        return {
            'url': url,
            'title': title.get_text(strip=True) if title else '',
            'meta_description': description.get('content', '') if description else '',
            'meta_keywords': keywords.get('content', '') if keywords else '',
            'og_title': og_title.get('content', '') if og_title else '',
            'og_description': og_description.get('content', '') if og_description else '',
            'og_image': og_image.get('content', '') if og_image else '',
            'structured_data': structured_data,
            'headers': headers,
            'images': images,
            'word_count': len(soup.get_text().split()),
            'links_count': len(soup.find_all('a'))
        }
    
    async def find_post_urls(self, html: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ø–æ—Å—Ç—ã"""
        soup = BeautifulSoup(html, 'html.parser')
        post_urls = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ—Å—Ç–∞–º–∏
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å—Å—ã–ª–∫–∞
                if urlparse(full_url).netloc == self.domain:
                    # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ—Å—Ç–æ–≤
                    if any(pattern in full_url.lower() for pattern in [
                        '/post/', '/article/', '/blog/', '/news/',
                        '/2024/', '/2023/', '/2022/', '/2021/',
                        '.html', '.php'
                    ]):
                        post_urls.append(full_url)
        
        return list(set(post_urls))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    async def index_post(self, url: str) -> Dict[str, Any]:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ—Å—Ç–∞"""
        logger.info(f"–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–æ—Å—Ç: {url}")
        
        html = await self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º SEO –¥–∞–Ω–Ω—ã–µ
        seo_data = await self.extract_seo_data(html, url)
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content = ""
        content_selectors = [
            'article', '.post-content', '.entry-content', 
            '.content', '.main-content', '#content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(strip=True)
                break
        
        if not content:
            content = soup.get_text(strip=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        publish_date = ""
        date_selectors = [
            '.publish-date', '.post-date', '.entry-date',
            'time[datetime]', '.date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                publish_date = date_elem.get('datetime') or date_elem.get_text(strip=True)
                break
        
        # –ò—â–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –≤ –ø–æ—Å—Ç–µ
        internal_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                full_url = urljoin(url, href)
                if urlparse(full_url).netloc == self.domain:
                    internal_links.append({
                        'from_url': url,
                        'to_url': full_url,
                        'anchor_text': link.get_text(strip=True),
                        'title': link.get('title', '')
                    })
        
        return {
            'url': url,
            'title': seo_data['title'],
            'content': content[:1000] + "..." if len(content) > 1000 else content,
            'publish_date': publish_date,
            'word_count': seo_data['word_count'],
            'internal_links': internal_links,
            'seo_data': seo_data
        }
    
    async def analyze_internal_links(self):
        """–ê–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫"""
        link_map = {}
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
        for post in self.posts:
            for link in post.get('internal_links', []):
                to_url = link['to_url']
                if to_url not in link_map:
                    link_map[to_url] = []
                link_map[to_url].append({
                    'from_url': link['from_url'],
                    'anchor_text': link['anchor_text'],
                    'title': link['title']
                })
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
        for to_url, links in link_map.items():
            self.internal_links.append({
                'target_url': to_url,
                'incoming_links': links,
                'incoming_links_count': len(links)
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
        self.internal_links.sort(key=lambda x: x['incoming_links_count'], reverse=True) 