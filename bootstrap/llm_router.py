"""
üß† –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LLM —Ä–æ—É—Ç–µ—Ä–æ–º –¥–ª—è –≤—Å–µ—Ö –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤
"""

from typing import Optional, Dict, Any
import httpx
import structlog

from .config import get_settings

logger = structlog.get_logger()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LLM —Ä–æ—É—Ç–µ—Ä–∞
_llm_router: Optional['LLMRouter'] = None

class LLMRouter:
    """–ù–∞—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LLM —Ä–æ—É—Ç–µ—Ä–æ–º"""
    
    def __init__(self):
        self.settings = get_settings()
        self.base_url = self.settings.LLM_ROUTER_URL
        self.client = httpx.AsyncClient(timeout=30.0)
        
        logger.info("LLM Router initialized", base_url=self.base_url)
    
    async def route_request(
        self, 
        prompt: str, 
        model: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        try:
            payload = {
                "prompt": prompt,
                "model": model or "auto",
                "context": context or {},
                "service": self.settings.SERVICE_NAME
            }
            
            response = await self.client.post(
                f"{self.base_url}/api/v1/route",
                json=payload
            )
            
            response.raise_for_status()
            result = response.json()
            
            logger.info(
                "Request routed successfully",
                model=result.get("model"),
                service=self.settings.SERVICE_NAME
            )
            
            return result
            
        except Exception as e:
            logger.error(
                "LLM routing error",
                error=str(e),
                prompt=prompt[:100] + "..." if len(prompt) > 100 else prompt
            )
            raise
    
    async def analyze_effectiveness(
        self, 
        request_id: str, 
        result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        
        try:
            payload = {
                "request_id": request_id,
                "result": result,
                "service": self.settings.SERVICE_NAME
            }
            
            response = await self.client.post(
                f"{self.base_url}/api/v1/analyze",
                json=payload
            )
            
            response.raise_for_status()
            analysis = response.json()
            
            logger.info(
                "Effectiveness analyzed",
                request_id=request_id,
                score=analysis.get("effectiveness_score")
            )
            
            return analysis
            
        except Exception as e:
            logger.error(
                "Effectiveness analysis error",
                error=str(e),
                request_id=request_id
            )
            raise
    
    async def get_available_models(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        try:
            response = await self.client.get(f"{self.base_url}/api/v1/models")
            response.raise_for_status()
            
            return response.json()
            
        except Exception as e:
            logger.error("Failed to get available models", error=str(e))
            raise
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        await self.client.aclose()
        logger.info("LLM Router connection closed")

def get_llm_router() -> LLMRouter:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ LLM —Ä–æ—É—Ç–µ—Ä–∞"""
    global _llm_router
    if _llm_router is None:
        _llm_router = LLMRouter()
    return _llm_router

async def close_llm_router():
    """–ó–∞–∫—Ä—ã—Ç–∏–µ LLM —Ä–æ—É—Ç–µ—Ä–∞"""
    global _llm_router
    if _llm_router:
        await _llm_router.close()
        _llm_router = None 