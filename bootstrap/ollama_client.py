"""
ü§ñ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Ollama –¥–ª—è –≤—Å–µ—Ö –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è MacBook M4 —Å Metal GPU acceleration
"""

from typing import Dict, Any, Optional, List, AsyncGenerator
import httpx
import structlog
import platform
import psutil
import asyncio
from dataclasses import dataclass

from .config import get_settings

logger = structlog.get_logger()

@dataclass
class ModelSpec:
    """–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    name: str
    size_gb: float
    recommended_ram_gb: float
    metal_compatible: bool
    quantization: str
    max_tokens: int

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä Ollama –∫–ª–∏–µ–Ω—Ç–∞
_ollama_client: Optional['OllamaClient'] = None

class OllamaClient:
    """–ù–∞—Ç–∏–≤–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Ollama —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è M4"""
    
    def __init__(self):
        self.settings = get_settings()
        self.base_url = self.settings.OLLAMA_URL
        self.client = httpx.AsyncClient(timeout=self.settings.OLLAMA_TIMEOUT)
        self.is_m4_mac = self._detect_m4_mac()
        self.system_memory = self._get_system_memory()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è M4
        if self.is_m4_mac:
            self._optimize_for_m4()
        
        logger.info("Ollama Client initialized", 
                   is_m4_mac=self.is_m4_mac,
                   system_memory_gb=self.system_memory)
    
    def _detect_m4_mac(self) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MacBook M4"""
        try:
            if platform.system() == "Darwin" and platform.machine() == "arm64":
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ M4 —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
                import subprocess
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                return "M4" in result.stdout
            return False
        except:
            return False
    
    def _get_system_memory(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ GB"""
        try:
            return psutil.virtual_memory().total / (1024**3)
        except:
            return 16.0  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 16GB –¥–ª—è M4
    
    def _optimize_for_m4(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è MacBook M4"""
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Metal GPU acceleration
        self.metal_settings = {
            "num_gpu": 1,
            "num_thread": 8,  # M4 –∏–º–µ–µ—Ç 8-10 —è–¥–µ—Ä
            "num_ctx": 4096,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è M4
            "gpu_layers": 35,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU –¥–ª—è –±–æ–ª—å—à–µ–π —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–∏
            "main_gpu": 0,
            "tensor_split": [0.8, 0.2],  # 80% –Ω–∞ GPU, 20% –Ω–∞ CPU
            "rope_freq_base": 10000,
            "rope_freq_scale": 0.5
        }
        
        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –¥–ª—è M4
        if self.system_memory <= 16:
            self.memory_settings = {
                "max_memory": int(self.system_memory * 0.7 * 1024),  # 70% –æ—Ç RAM
                "batch_size": 512,
                "context_size": 2048
            }
        else:
            self.memory_settings = {
                "max_memory": int(self.system_memory * 0.8 * 1024),  # 80% –æ—Ç RAM
                "batch_size": 1024,
                "context_size": 4096
            }
    
    async def generate(
        self, 
        prompt: str, 
        model: str = "qwen2.5:7b-instruct-turbo",
        **kwargs
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è M4"""
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è M4
        if self.is_m4_mac:
            kwargs.update(self.metal_settings)
            kwargs.update(self.memory_settings)
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            **kwargs
        }
        
        try:
            response = await self.client.post(
                f"{self.base_url}/api/generate",
                json=payload
            )
            
            result = response.json()
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è M4
            if self.is_m4_mac and "eval_duration" in result:
                logger.info("M4 generation completed",
                           model=model,
                           eval_duration=result["eval_duration"],
                           tokens_generated=result.get("eval_count", 0))
            
            return result
            
        except Exception as e:
            logger.error("Ollama generation failed", error=str(e), model=model)
            raise
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        try:
            response = await self.client.get(f"{self.base_url}/api/tags")
            models = response.json()["models"]
            
            # –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π –¥–ª—è M4
            if self.is_m4_mac:
                for model in models:
                    model["m4_optimized"] = self._is_model_optimized_for_m4(model)
                    model["recommended_quantization"] = self._get_recommended_quantization(model)
            
            return models
            
        except Exception as e:
            logger.error("Failed to list models", error=str(e))
            return []
    
    def _is_model_optimized_for_m4(self, model: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è M4"""
        model_name = model.get("name", "").lower()
        
        # –ú–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è M4
        m4_optimized_models = [
            "qwen2.5:7b-instruct-turbo",
            "llama3.1:8b-instruct",
            "mistral:7b-instruct",
            "codellama:7b-instruct"
        ]
        
        return any(opt_model in model_name for opt_model in m4_optimized_models)
    
    def _get_recommended_quantization(self, model: Dict[str, Any]) -> str:
        """–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è M4"""
        size_gb = model.get("size", 0) / (1024**3)
        
        if size_gb <= 4:
            return "Q4_K_M"  # –ë—ã—Å—Ç—Ä–æ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ
        elif size_gb <= 8:
            return "Q6_K"    # –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
        else:
            return "Q8_0"    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    
    async def pull_model(self, model: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è M4"""
        
        payload = {"name": model}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è M4
        if self.is_m4_mac:
            payload.update({
                "insecure": True,  # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
                "quiet": False
            })
        
        try:
            response = await self.client.post(
                f"{self.base_url}/api/pull",
                json=payload
            )
            
            result = response.json()
            
            if self.is_m4_mac:
                logger.info("Model pulled for M4", 
                           model=model,
                           size_gb=result.get("size", 0) / (1024**3))
            
            return result
            
        except Exception as e:
            logger.error("Failed to pull model", error=str(e), model=model)
            raise
    
    async def health_check(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ M4"""
        
        try:
            response = await self.client.get(f"{self.base_url}/api/tags")
            
            health_info = {
                "status": "healthy",
                "ollama_connected": True,
                "models_count": len(response.json().get("models", [])),
                "service": self.settings.SERVICE_NAME
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ M4
            if self.is_m4_mac:
                health_info.update({
                    "m4_optimized": True,
                    "system_memory_gb": self.system_memory,
                    "metal_acceleration": True,
                    "recommended_models": [
                        "qwen2.5:7b-instruct-turbo",
                        "llama3.1:8b-instruct",
                        "mistral:7b-instruct"
                    ]
                })
            
            return health_info
            
        except Exception as e:
            logger.error("Ollama health check failed", error=str(e))
            return {
                "status": "unhealthy",
                "error": str(e),
                "service": self.settings.SERVICE_NAME
            }

def get_ollama_client() -> OllamaClient:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
    global _ollama_client
    if _ollama_client is None:
        _ollama_client = OllamaClient()
    return _ollama_client

async def close_ollama_client():
    """–ó–∞–∫—Ä—ã—Ç–∏–µ Ollama –∫–ª–∏–µ–Ω—Ç–∞"""
    global _ollama_client
    if _ollama_client:
        await _ollama_client.close()
        _ollama_client = None 