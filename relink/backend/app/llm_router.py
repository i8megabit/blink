"""
üß† –ï–¥–∏–Ω—ã–π LLM-–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ reLink

–û–±–Ω–æ–≤–ª–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ Ollama.
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å Ollama.
"""

import asyncio
import aiohttp
import json
import logging
import platform
import psutil
import subprocess
import os
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import time
from contextlib import asynccontextmanager

from .config import settings
from .database import get_db
from .models import LLMRequest as DBLLMRequest, LLMResponse as DBLLMResponse, LLMEmbedding
from .cache import cache_manager
from .exceptions import LLMServiceError, OllamaConnectionError
from .monitoring import rag_monitor
from .llm_integration import get_llm_integration_service, LLMIntegrationService

logger = logging.getLogger(__name__)

class LLMServiceType(Enum):
    """–¢–∏–ø—ã LLM-—Å–µ—Ä–≤–∏—Å–æ–≤"""
    SEO_RECOMMENDATIONS = "seo_recommendations"
    DIAGRAM_GENERATION = "diagram_generation"
    CONTENT_ANALYSIS = "content_analysis"
    BENCHMARK_SERVICE = "benchmark_service"
    LLM_TUNING = "llm_tuning"

@dataclass
class SystemSpecs:
    """–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã"""
    platform: str
    architecture: str
    cpu_count: int
    memory_gb: float
    gpu_available: bool
    gpu_type: Optional[str] = None
    apple_silicon: bool = False
    m1_m2_m4: bool = False

@dataclass
class OptimizedConfig:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∏—Å—Ç–µ–º—ã"""
    model: str
    num_gpu: int
    num_thread: int
    batch_size: int
    f16_kv: bool
    temperature: float
    max_tokens: int
    context_length: int
    keep_alive: str
    request_timeout: int
    semaphore_limit: int
    cache_ttl: int

class SystemAnalyzer:
    """
    üîç –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏—Å—Ç–µ–º—ã —Å RAG –∏ LLM
    
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å–ø–æ–ª—å–∑—É—è:
    - –ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    - RAG —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - LLM –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    
    def __init__(self):
        self.specs: Optional[SystemSpecs] = None
        self.optimized_config: Optional[OptimizedConfig] = None
        self.performance_history: List[Dict[str, Any]] = []
        self.knowledge_base: List[Dict[str, Any]] = []
        self._initialize_knowledge_base()
    
    def _initialize_knowledge_base(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.knowledge_base = [
            {
                "system_type": "apple_silicon_m1_m2_m4",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 8,
                    "batch_size": 1024,
                    "f16_kv": True,
                    "context_length": 8192,
                    "semaphore_limit": 2  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
                },
                "performance_notes": "Apple Silicon M1/M2/M4 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å GPU acceleration –∏ –±–æ–ª—å—à–∏–º–∏ batch sizes",
                "memory_optimization": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Unified Memory Architecture –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"
            },
            {
                "system_type": "apple_silicon_generic",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 6,
                    "batch_size": 768,
                    "f16_kv": True,
                    "context_length": 6144,
                    "semaphore_limit": 2
                },
                "performance_notes": "–û–±—â–∏–µ Apple Silicon –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç —Å Metal acceleration",
                "memory_optimization": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU memory"
            },
            {
                "system_type": "nvidia_gpu",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 6,
                    "batch_size": 1024,
                    "f16_kv": True,
                    "context_length": 8192,
                    "semaphore_limit": 2
                },
                "performance_notes": "NVIDIA GPU –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å CUDA acceleration",
                "memory_optimization": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPU memory –¥–ª—è KV cache"
            },
            {
                "system_type": "amd_gpu",
                "optimal_config": {
                    "num_gpu": 1,
                    "num_thread": 6,
                    "batch_size": 768,
                    "f16_kv": True,
                    "context_length": 6144,
                    "semaphore_limit": 2
                },
                "performance_notes": "AMD GPU —Ä–∞–±–æ—Ç–∞–µ—Ç —Å ROCm acceleration",
                "memory_optimization": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM"
            },
            {
                "system_type": "cpu_only_high_memory",
                "optimal_config": {
                    "num_gpu": 0,
                    "num_thread": 8,
                    "batch_size": 512,
                    "f16_kv": False,
                    "context_length": 8192,
                    "semaphore_limit": 2
                },
                "performance_notes": "CPU-only —Å–∏—Å—Ç–µ–º—ã —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º –ø–∞–º—è—Ç–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã",
                "memory_optimization": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç RAM –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"
            },
            {
                "system_type": "cpu_only_low_memory",
                "optimal_config": {
                    "num_gpu": 0,
                    "num_thread": 4,
                    "batch_size": 256,
                    "f16_kv": False,
                    "context_length": 2048,
                    "semaphore_limit": 1
                },
                "performance_notes": "–°–∏—Å—Ç–µ–º—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é —Ç—Ä–µ–±—É—é—Ç –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫",
                "memory_optimization": "–ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"
            }
        ]
    
    async def analyze_system(self) -> SystemSpecs:
        """–ê–Ω–∞–ª–∏–∑ —Å–∏—Å—Ç–µ–º—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π"""
        if self.specs:
            return self.specs
        
        platform_name = platform.system()
        architecture = platform.machine()
        cpu_count = psutil.cpu_count(logical=True)
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Apple Silicon
        apple_silicon = False
        m1_m2_m4 = False
        
        if platform_name == "Darwin" and "arm" in architecture.lower():
            apple_silicon = True
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
            try:
                result = subprocess.run(
                    ["sysctl", "-n", "machdep.cpu.brand_string"], 
                    capture_output=True, text=True
                )
                cpu_brand = result.stdout.lower()
                if any(x in cpu_brand for x in ["m1", "m2", "m3", "m4"]):
                    m1_m2_m4 = True
            except:
                pass
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GPU
        gpu_available = False
        gpu_type = None
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ NVIDIA GPU
            result = subprocess.run(["nvidia-smi"], capture_output=True)
            if result.returncode == 0:
                gpu_available = True
                gpu_type = "nvidia"
            else:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ AMD GPU
                result = subprocess.run(["rocm-smi"], capture_output=True)
                if result.returncode == 0:
                    gpu_available = True
                    gpu_type = "amd"
        except:
            pass
        
        self.specs = SystemSpecs(
            platform=platform_name,
            architecture=architecture,
            cpu_count=cpu_count,
            memory_gb=memory_gb,
            gpu_available=gpu_available,
            gpu_type=gpu_type,
            apple_silicon=apple_silicon,
            m1_m2_m4=m1_m2_m4
        )
        
        logger.info(f"–°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.specs}")
        return self.specs
    
    async def _get_llm_recommendation(self, specs: SystemSpecs, performance_data: List[Dict]) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç LLM –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            llm_service = await get_llm_integration_service()
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
            prompt = f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏ –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Ollama:
            
            –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
            - –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞: {specs.platform}
            - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {specs.architecture}
            - CPU —è–¥–µ—Ä: {specs.cpu_count}
            - –ü–∞–º—è—Ç—å: {specs.memory_gb:.1f} GB
            - GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {specs.gpu_available}
            - GPU —Ç–∏–ø: {specs.gpu_type}
            - Apple Silicon: {specs.apple_silicon}
            - M1/M2/M4: {specs.m1_m2_m4}
            
            –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø–∏—Å–µ–π):
            {performance_data[-10:] if performance_data else "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"}
            
            –ü—Ä–µ–¥–ª–æ–∂–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è Ollama –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
            {{
                "num_gpu": <–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU>,
                "num_thread": <–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤>,
                "batch_size": <—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞>,
                "f16_kv": <–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å f16 –¥–ª—è KV cache>,
                "context_length": <–¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞>,
                "semaphore_limit": <–ª–∏–º–∏—Ç —Å–µ–º–∞—Ñ–æ—Ä–∞>
            }}
            
            –£—á—Ç–∏, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ Ollama.
            """
            
            response = await llm_service.generate_response(
                prompt=prompt,
                max_tokens=500,
                temperature=0.3
            )
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            try:
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
            except:
                pass
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è
            return {
                "num_gpu": 1 if specs.gpu_available else 0,
                "num_thread": min(specs.cpu_count, 8),
                "batch_size": 512,
                "f16_kv": specs.gpu_available,
                "context_length": 4096,
                "semaphore_limit": 2
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è LLM —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            return {
                "num_gpu": 1 if specs.gpu_available else 0,
                "num_thread": min(specs.cpu_count, 4),
                "batch_size": 256,
                "f16_kv": specs.gpu_available,
                "context_length": 2048,
                "semaphore_limit": 2
            }
    
    async def _search_knowledge_base(self, specs: SystemSpecs) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–∏—Å—Ç–µ–º—ã"""
        relevant_configs = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–∏—Å—Ç–µ–º—ã
        if specs.apple_silicon and specs.m1_m2_m4:
            system_type = "apple_silicon_m1_m2_m4"
        elif specs.apple_silicon:
            system_type = "apple_silicon_generic"
        elif specs.gpu_type == "nvidia":
            system_type = "nvidia_gpu"
        elif specs.gpu_type == "amd":
            system_type = "amd_gpu"
        elif specs.memory_gb >= 16:
            system_type = "cpu_only_high_memory"
        else:
            system_type = "cpu_only_low_memory"
        
        # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        for config in self.knowledge_base:
            if config["system_type"] == system_type:
                relevant_configs.append(config)
        
        return relevant_configs
    
    async def optimize_config(self) -> OptimizedConfig:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–∏—Å—Ç–µ–º—ã"""
        if self.optimized_config:
            return self.optimized_config
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É
        specs = await self.analyze_system()
        
        # –ò—â–µ–º –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        knowledge_configs = await self._search_knowledge_base(specs)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –æ—Ç LLM
        llm_recommendation = await self._get_llm_recommendation(specs, self.performance_history)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        optimal_config = {
            "model": "qwen2.5:7b-instruct-turbo",
            "num_gpu": llm_recommendation.get("num_gpu", 1 if specs.gpu_available else 0),
            "num_thread": llm_recommendation.get("num_thread", min(specs.cpu_count, 8)),
            "batch_size": llm_recommendation.get("batch_size", 512),
            "f16_kv": llm_recommendation.get("f16_kv", specs.gpu_available),
            "temperature": 0.7,
            "max_tokens": 2048,
            "context_length": llm_recommendation.get("context_length", 4096),
            "keep_alive": "2h",
            "request_timeout": 300,
            "semaphore_limit": llm_recommendation.get("semaphore_limit", 2),
            "cache_ttl": 3600
        }
        
        self.optimized_config = OptimizedConfig(**optimal_config)
        
        logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.optimized_config}")
        return self.optimized_config
    
    async def record_performance(self, response_time: float, success: bool, tokens_used: int):
        """–ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        performance_record = {
            "timestamp": datetime.utcnow(),
            "response_time": response_time,
            "success": success,
            "tokens_used": tokens_used,
            "system_load": psutil.cpu_percent(),
            "memory_usage": psutil.virtual_memory().percent
        }
        
        self.performance_history.append(performance_record)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-100:]
    
    async def _analyze_and_adapt(self):
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if len(self.performance_history) < 10:
            return
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        recent_performance = self.performance_history[-10:]
        avg_response_time = sum(p["response_time"] for p in recent_performance) / len(recent_performance)
        success_rate = sum(1 for p in recent_performance if p["success"]) / len(recent_performance)
        
        # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É—Ö—É–¥—à–∏–ª–∞—Å—å, –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if avg_response_time > 10.0 or success_rate < 0.9:
            logger.warning(f"–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É—Ö—É–¥—à–∏–ª–∞—Å—å: avg_time={avg_response_time:.2f}s, success_rate={success_rate:.2f}")
            self.optimized_config = None  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–ª—è –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    
    async def get_environment_variables(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Ollama"""
        config = await self.optimize_config()
        
        env_vars = {
            "OLLAMA_HOST": "0.0.0.0:11434",
            "OLLAMA_MODELS": "/root/.ollama/models",
            "OLLAMA_KEEP_ALIVE": config.keep_alive,
            "OLLAMA_ORIGINS": "*",
            "OLLAMA_NUM_PARALLEL": str(config.semaphore_limit),
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è GPU –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        if config.num_gpu > 0:
            if platform.system() == "Darwin":
                env_vars["OLLAMA_GPU_LAYERS"] = str(config.num_gpu)
            else:
                env_vars["CUDA_VISIBLE_DEVICES"] = "0"
        
        return env_vars
    
    async def get_optimization_report(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        config = await self.optimize_config()
        specs = await self.analyze_system()
        
        return {
            "system_specs": {
                "platform": specs.platform,
                "architecture": specs.architecture,
                "cpu_count": specs.cpu_count,
                "memory_gb": specs.memory_gb,
                "gpu_available": specs.gpu_available,
                "gpu_type": specs.gpu_type,
                "apple_silicon": specs.apple_silicon,
                "m1_m2_m4": specs.m1_m2_m4
            },
            "optimized_config": {
                "model": config.model,
                "num_gpu": config.num_gpu,
                "num_thread": config.num_thread,
                "batch_size": config.batch_size,
                "f16_kv": config.f16_kv,
                "temperature": config.temperature,
                "max_tokens": config.max_tokens,
                "context_length": config.context_length,
                "keep_alive": config.keep_alive,
                "request_timeout": config.request_timeout,
                "semaphore_limit": config.semaphore_limit,
                "cache_ttl": config.cache_ttl
            },
            "performance_history": {
                "total_records": len(self.performance_history),
                "avg_response_time": sum(p["response_time"] for p in self.performance_history) / len(self.performance_history) if self.performance_history else 0,
                "success_rate": sum(1 for p in self.performance_history if p["success"]) / len(self.performance_history) if self.performance_history else 1.0
            },
            "knowledge_base_size": len(self.knowledge_base)
        }

@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–∑–∞–ø—Ä–æ—Å–∞"""
    service_type: LLMServiceType
    prompt: str
    context: Optional[Dict[str, Any]] = None
    llm_model: str = "qwen2.5:7b-instruct-turbo"  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è Apple Silicon
    temperature: float = 0.7
    max_tokens: int = 2048
    use_rag: bool = True
    cache_ttl: int = 3600  # 1 —á–∞—Å
    priority: str = "normal"  # critical, high, normal, low, background

@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–æ—Ç–≤–µ—Ç–∞"""
    content: str
    service_type: LLMServiceType
    used_model: str
    tokens_used: int
    response_time: float
    cached: bool = False
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class LLMRouter:
    """
    üöÄ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π LLM-–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä —Å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é LLM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ Ollama.
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç RAG-–æ–±–æ–≥–∞—â–µ–Ω–∏–µ, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    """
    
    def __init__(self):
        self.system_analyzer = SystemAnalyzer()
        self.llm_service: Optional[LLMIntegrationService] = None
        self.optimized_config: Optional[OptimizedConfig] = None
        self._initialized = False
        
        logger.info("LLMRouter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"""
        await self.start()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
        await self.stop()
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ —Ä–æ—É—Ç–µ—Ä–∞"""
        if self._initialized:
            return
        
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM —Å–µ—Ä–≤–∏—Å
            self.llm_service = await get_llm_integration_service()
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            self.optimized_config = await self.system_analyzer.optimize_config()
            
            self._initialized = True
            logger.info("LLMRouter –∑–∞–ø—É—â–µ–Ω —Å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ LLMRouter: {e}")
            raise
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ä–æ—É—Ç–µ—Ä–∞"""
        self._initialized = False
        logger.info("LLMRouter –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def _generate_cache_key(self, request: LLMRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        key_parts = [
            request.service_type.value,
            request.prompt,
            request.llm_model,
            str(request.temperature),
            str(request.max_tokens),
            str(request.use_rag)
        ]
        
        key_string = "|".join(key_parts)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    async def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            cached_data = await cache_manager.get(f"llm_response:{cache_key}")
            if cached_data:
                return LLMResponse(**cached_data)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫—ç—à–∞: {e}")
        
        return None
    
    async def _cache_response(self, cache_key: str, response: LLMResponse, ttl: int):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        try:
            response_data = {
                "content": response.content,
                "service_type": response.service_type.value,
                "used_model": response.used_model,
                "tokens_used": response.tokens_used,
                "response_time": response.response_time,
                "cached": True,
                "error": response.error,
                "metadata": response.metadata
            }
            
            await cache_manager.set(f"llm_response:{cache_key}", response_data, ttl)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    async def _generate_rag_context(self, request: LLMRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            if not request.use_rag:
                return ""
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            embedding = await self.llm_service.get_embedding(request.prompt)
            
            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            relevant_docs = await self.llm_service.search_knowledge_base(
                request.prompt, 
                limit=3
            )
            
            if relevant_docs:
                context = "\n".join(relevant_docs)
                logger.info(f"RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è {request.service_type.value}")
                return context
            
            return ""
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return ""
    
    async def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            return await self.llm_service.get_embedding(text)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return []
    
    async def _search_knowledge_base(
        self, 
        embedding: List[float], 
        service_type: LLMServiceType,
        limit: int = 3
    ) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –ø–æ–∏—Å–∫–∞
            query = f"service_type:{service_type.value}"
            relevant_docs = await self.llm_service.search_knowledge_base(query, limit)
            
            return [{"content": doc, "relevance": 0.8} for doc in relevant_docs]
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    async def _make_ollama_request(self, request: LLMRequest) -> LLMResponse:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É"""
        start_time = time.time()
        
        try:
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ö–æ–¥—è—â–∏–π –∑–∞–ø—Ä–æ—Å
            logger.info(f"ü§ñ LLM –ó–ê–ü–†–û–° [{request.service_type.value}]")
            logger.info(f"üìù –ü—Ä–æ–º–ø—Ç: {request.prompt[:200]}{'...' if len(request.prompt) > 200 else ''}")
            logger.info(f"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: –º–æ–¥–µ–ª—å={request.llm_model}, —Ç–æ–∫–µ–Ω—ã={request.max_tokens}, temp={request.temperature}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç
            logger.info("üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
            rag_context = await self._generate_rag_context(request)
            
            if rag_context:
                logger.info(f"üìö RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω: {len(rag_context)} —Å–∏–º–≤–æ–ª–æ–≤")
                logger.info(f"üìñ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç: {rag_context[:300]}{'...' if len(rag_context) > 300 else ''}")
                
                final_prompt = f"""
                –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
                {rag_context}
                
                –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
                {request.prompt}
                
                –û—Ç–≤–µ—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:
                """
            else:
                logger.info("‚ö†Ô∏è RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π –ø—Ä–æ–º–ø—Ç")
                final_prompt = request.prompt
            
            logger.info(f"üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama...")
            logger.info(f"üì§ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {final_prompt[:300]}{'...' if len(final_prompt) > 300 else ''}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
            response = await self.llm_service.process_llm_request(
                prompt=final_prompt,
                llm_model=request.llm_model,
                priority=request.priority,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                use_rag=request.use_rag,
                metadata={
                    "service_type": request.service_type.value,
                    "context": request.context
                }
            )
            
            response_time = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –æ—Ç Ollama
            logger.info(f"‚úÖ OLLAMA –û–¢–í–ï–¢ [{request.service_type.value}]")
            logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response_time:.2f}s")
            logger.info(f"üß† –ú–æ–¥–µ–ª—å: {response.model_used}")
            logger.info(f"üî¢ –¢–æ–∫–µ–Ω—ã: {response.tokens_used}")
            logger.info(f"üìÑ –û—Ç–≤–µ—Ç: {response.response[:500]}{'...' if len(response.response) > 500 else ''}")
            logger.info(f"üîç RAG —É—Å–∏–ª–µ–Ω: {response.rag_enhanced}")
            logger.info(f"üíæ –ö—ç—à —Ö–∏—Ç: {response.cache_hit}")
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            await self.system_analyzer.record_performance(
                response_time, 
                True, 
                response.tokens_used
            )
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∞—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            llm_response = LLMResponse(
                content=response.response,
                service_type=request.service_type,
                used_model=response.model_used,
                tokens_used=response.tokens_used,
                response_time=response_time,
                cached=False,
                metadata={
                    "rag_enhanced": response.rag_enhanced,
                    "cache_hit": response.cache_hit,
                    "original_request_id": response.request_id
                }
            )
            
            logger.info(f"üéØ –ó–∞–ø—Ä–æ—Å {request.service_type.value} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {response_time:.2f}s")
            return llm_response
            
        except Exception as e:
            response_time = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            logger.error(f"‚ùå –û–®–ò–ë–ö–ê LLM [{request.service_type.value}]")
            logger.error(f"‚è±Ô∏è –í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {response_time:.2f}s")
            logger.error(f"üö® –û—à–∏–±–∫–∞: {str(e)}")
            logger.error(f"üìù –ü—Ä–æ–º–ø—Ç: {request.prompt[:200]}{'...' if len(request.prompt) > 200 else ''}")
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
            await self.system_analyzer.record_performance(response_time, False, 0)
            
            return LLMResponse(
                content="",
                service_type=request.service_type,
                used_model=request.llm_model,
                tokens_used=0,
                response_time=response_time,
                error=str(e)
            )
    
    async def process_request(self, request: LLMRequest) -> LLMResponse:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ LLM –∑–∞–ø—Ä–æ—Å–∞"""
        if not self._initialized:
            raise RuntimeError("LLMRouter –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = self._generate_cache_key(request)
        cached_response = await self._get_cached_response(cache_key)
        
        if cached_response:
            logger.info(f"–ö—ç—à-—Ö–∏—Ç –¥–ª—è {request.service_type.value}")
            return cached_response
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
        response = await self._make_ollama_request(request)
        
        # –ö—ç—à–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
        if not response.error:
            await self._cache_response(cache_key, response, request.cache_ttl)
        
        return response
    
    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–æ—É—Ç–µ—Ä–∞"""
        if not self._initialized:
            return {"error": "LLMRouter –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"}
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        llm_metrics = await self.llm_service.get_metrics()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimization_report = await self.system_analyzer.get_optimization_report()
        
        return {
            "llm_metrics": llm_metrics,
            "optimization_report": optimization_report,
            "initialized": self._initialized
        }
    
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Ä–æ—É—Ç–µ—Ä–∞"""
        if not self._initialized:
            return False
        
        try:
            health_status = await self.llm_service.health_check()
            return health_status.get("status") == "healthy"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
system_analyzer = SystemAnalyzer()
_llm_router: Optional[LLMRouter] = None

async def get_llm_router() -> LLMRouter:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ LLM —Ä–æ—É—Ç–µ—Ä–∞"""
    global _llm_router
    
    if _llm_router is None:
        _llm_router = LLMRouter()
        await _llm_router.start()
    
    return _llm_router

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è llm_router –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
llm_router = LLMRouter()

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ—É–Ω–∫—Ü–∏—è–º
async def generate_seo_recommendations(prompt: str, context: Optional[Dict] = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    router = await get_llm_router()
    
    request = LLMRequest(
        service_type=LLMServiceType.SEO_RECOMMENDATIONS,
        prompt=prompt,
        context=context,
        priority="high"
    )
    
    response = await router.process_request(request)
    
    if response.error:
        raise LLMServiceError(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SEO —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {response.error}")
    
    return response.content

async def generate_diagram(prompt: str, diagram_type: str = "architecture") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã"""
    router = await get_llm_router()
    
    request = LLMRequest(
        service_type=LLMServiceType.DIAGRAM_GENERATION,
        prompt=f"–°–æ–∑–¥–∞–π {diagram_type} –¥–∏–∞–≥—Ä–∞–º–º—É: {prompt}",
        priority="normal"
    )
    
    response = await router.process_request(request)
    
    if response.error:
        raise LLMServiceError(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–≥—Ä–∞–º–º—ã: {response.error}")
    
    return response.content

async def analyze_content(content: str, analysis_type: str = "general") -> str:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    router = await get_llm_router()
    
    request = LLMRequest(
        service_type=LLMServiceType.CONTENT_ANALYSIS,
        prompt=f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–Ω—Ç ({analysis_type}): {content}",
        priority="normal"
    )
    
    response = await router.process_request(request)
    
    if response.error:
        raise LLMServiceError(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {response.error}")
    
    return response.content

async def run_benchmark(benchmark_type: str, parameters: Dict[str, Any]) -> str:
    """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    router = await get_llm_router()
    
    request = LLMRequest(
        service_type=LLMServiceType.BENCHMARK_SERVICE,
        prompt=f"–ó–∞–ø—É—Å—Ç–∏ {benchmark_type} –±–µ–Ω—á–º–∞—Ä–∫ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {parameters}",
        priority="low"
    )
    
    response = await router.process_request(request)
    
    if response.error:
        raise LLMServiceError(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {response.error}")
    
    return response.content

async def tune_llm_model(model_config: Dict[str, Any], tuning_params: Dict[str, Any]) -> str:
    """–¢—é–Ω–∏–Ω–≥ LLM –º–æ–¥–µ–ª–∏"""
    router = await get_llm_router()
    
    request = LLMRequest(
        service_type=LLMServiceType.LLM_TUNING,
        prompt=f"–ù–∞—Å—Ç—Ä–æ–π –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π {model_config} –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ {tuning_params}",
        priority="background"
    )
    
    response = await router.process_request(request)
    
    if response.error:
        raise LLMServiceError(f"–û—à–∏–±–∫–∞ —Ç—é–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏: {response.error}")
    
    return response.content 