#!/usr/bin/env python3
"""
üöÄ LLM BENCHMARK SYSTEM –¥–ª—è SEO-–ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏
–°–∏—Å—Ç–µ–º–∞ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π Ollama –Ω–∞ Apple M4
"""

import asyncio
import json
import time
import httpx
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional
import statistics

@dataclass
class BenchmarkMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
    model_name: str
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_quality_score: float
    memory_usage_mb: float
    tokens_per_second: float
    context_utilization: float
    hallucination_rate: float
    relevance_score: float
    anchor_quality: float
    reasoning_quality: float
    timestamp: str

@dataclass
class TestCase:
    """–¢–µ—Å—Ç–æ–≤—ã–π –∫–µ–π—Å –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    name: str
    domain: str
    test_text: str
    expected_links_count: int
    difficulty_level: str  # easy, medium, hard
    category: str  # tech, health, business, etc.

class LLMBenchmark:
    """–°–∏—Å—Ç–µ–º–∞ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ LLM –º–æ–¥–µ–ª–µ–π."""
    
    def __init__(self, backend_url: str = "http://localhost:8000"):
        self.backend_url = backend_url
        self.test_cases = self._create_test_cases()
        self.results = {}
    
    async def _switch_model(self, model_name: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤ –±—ç–∫–µ–Ω–¥–µ."""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.backend_url}/api/v1/benchmark_model",
                    params={"model_name": model_name}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("status") == "success":
                        print(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∞ –Ω–∞ {model_name}")
                        return True
                    else:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è: {result.get('message')}")
                        return False
                else:
                    print(f"‚ùå HTTP –æ—à–∏–±–∫–∞: {response.status_code}")
                    return False
                    
        except Exception as e:
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def _create_test_cases(self) -> List[TestCase]:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤."""
        return [
            TestCase(
                name="simple_tech_linking",
                domain="tech-blog.com",
                test_text="–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–≥–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã.",
                expected_links_count=2,
                difficulty_level="easy",
                category="tech"
            ),
            TestCase(
                name="complex_seo_analysis",
                domain="seo-agency.ru",
                test_text="–ü–æ–∏—Å–∫–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π —Å–∞–π—Ç–∞ –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö. –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, –ø–æ–¥–±–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å —Å–∏–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏.",
                expected_links_count=4,
                difficulty_level="hard",
                category="seo"
            ),
            TestCase(
                name="medical_content",
                domain="med-portal.com",
                test_text="–ó–¥–æ—Ä–æ–≤—å–µ —Å–µ—Ä–¥–µ—á–Ω–æ-—Å–æ—Å—É–¥–∏—Å—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã –Ω–∞–ø—Ä—è–º—É—é —Å–≤—è–∑–∞–Ω–æ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –∫–∞—Ä–¥–∏–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ —Å–Ω–∏–∂–∞—é—Ç —Ä–∏—Å–∫ —Ä–∞–∑–≤–∏—Ç–∏—è –∞—Ç–µ—Ä–æ—Å–∫–ª–µ—Ä–æ–∑–∞ –∏ –ø–æ–º–æ–≥–∞—é—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∞—Ä—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ.",
                expected_links_count=3,
                difficulty_level="medium",
                category="health"
            ),
            TestCase(
                name="business_consulting",
                domain="biz-consult.ru",
                test_text="–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ —Ç—Ä–µ–±—É–µ—Ç –≥–∏–±–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. Agile-–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –±—ã—Å—Ç—Ä–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —Ä—ã–Ω–∫–∞. –¶–∏—Ñ—Ä–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –¥–ª—è –≤—ã–∂–∏–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π.",
                expected_links_count=5,
                difficulty_level="hard",
                category="business"
            ),
            TestCase(
                name="ecommerce_optimization",
                domain="shop-expert.com",
                test_text="–ö–æ–Ω–≤–µ—Ä—Å–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤. UX-–¥–∏–∑–∞–π–Ω, —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ —É—Å–ø–µ—Ö–µ e-commerce –ø—Ä–æ–µ–∫—Ç–∞.",
                expected_links_count=3,
                difficulty_level="medium",
                category="ecommerce"
            )
        ]
    
    async def benchmark_model(self, model_name: str, iterations: int = 5) -> BenchmarkMetrics:
        """–ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        print(f"üî• –ë–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥ –º–æ–¥–µ–ª–∏: {model_name}")
        print("=" * 50)
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –≤ –±—ç–∫–µ–Ω–¥–µ
        await self._switch_model(model_name)
        
        response_times = []
        quality_scores = []
        successful_requests = 0
        failed_requests = 0
        
        async with httpx.AsyncClient(timeout=300.0) as client:
            
            for iteration in range(iterations):
                print(f"   –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration + 1}/{iterations}")
                
                for test_case in self.test_cases:
                    try:
                        start_time = time.time()
                        
                        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ API
                        response = await client.post(
                            f"{self.backend_url}/api/v1/recommend",
                            json={"text": test_case.test_text}
                        )
                        
                        end_time = time.time()
                        response_time = end_time - start_time
                        
                        if response.status_code == 200:
                            result = response.json()
                            links = result.get('links', [])
                            
                            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                            quality_score = self._calculate_quality_score(
                                links, test_case, response_time
                            )
                            
                            response_times.append(response_time)
                            quality_scores.append(quality_score)
                            successful_requests += 1
                            
                            print(f"     ‚úÖ {test_case.name}: {len(links)} —Å—Å—ã–ª–æ–∫, {response_time:.2f}—Å, –∫–∞—á–µ—Å—Ç–≤–æ: {quality_score:.2f}")
                            
                        else:
                            failed_requests += 1
                            print(f"     ‚ùå {test_case.name}: –û—à–∏–±–∫–∞ {response.status_code}")
                            
                    except Exception as e:
                        failed_requests += 1
                        print(f"     üí• {test_case.name}: –ò—Å–∫–ª—é—á–µ–Ω–∏–µ - {e}")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            tokens_per_second = self._estimate_tokens_per_second(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = tokens_per_second = 0
        
        avg_quality_score = statistics.mean(quality_scores) if quality_scores else 0
        
        return BenchmarkMetrics(
            model_name=model_name,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            total_requests=successful_requests + failed_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            avg_quality_score=avg_quality_score,
            memory_usage_mb=0,  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
            tokens_per_second=tokens_per_second,
            context_utilization=0.8,  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            hallucination_rate=self._estimate_hallucination_rate(quality_scores),
            relevance_score=avg_quality_score * 0.9,
            anchor_quality=avg_quality_score * 0.85,
            reasoning_quality=avg_quality_score * 0.95,
            timestamp=datetime.now().isoformat()
        )
    
    def _calculate_quality_score(self, links: List[str], test_case: TestCase, response_time: float) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞."""
        score = 0.0
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫ (30% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        link_count_score = min(len(links) / test_case.expected_links_count, 1.0) * 30
        score += link_count_score
        
        # –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (20% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        time_score = max(0, (10 - response_time) / 10) * 20
        score += time_score
        
        # –ù–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫ (30% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        has_links_score = 30 if links else 0
        score += has_links_score
        
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —Å—Å—ã–ª–æ–∫ (20% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        unique_score = (len(set(links)) / len(links) if links else 0) * 20
        score += unique_score
        
        return min(score, 100.0)
    
    def _estimate_tokens_per_second(self, response_times: List[float]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥—É."""
        if not response_times:
            return 0
        avg_time = statistics.mean(response_times)
        estimated_tokens = 100  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        return estimated_tokens / avg_time if avg_time > 0 else 0
    
    def _estimate_hallucination_rate(self, quality_scores: List[float]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
        if not quality_scores:
            return 1.0
        avg_quality = statistics.mean(quality_scores)
        return max(0, (100 - avg_quality) / 100)
    
    async def compare_models(self, models: List[str], iterations: int = 3) -> Dict:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π."""
        print("üèÜ –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´ A/B –ë–ï–ù–ß–ú–ê–†–ö–ò–ù–ì–ê")
        print("=" * 60)
        
        results = {}
        
        for model in models:
            print(f"\nüöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model}")
            results[model] = await self.benchmark_model(model, iterations)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        self._generate_comparison_report(results)
        
        return results
    
    def _generate_comparison_report(self, results: Dict[str, BenchmarkMetrics]):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π."""
        print("\n" + "=" * 60)
        print("üèÜ –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ë–ï–ù–ß–ú–ê–†–ö–ê")
        print("=" * 60)
        
        # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"{'–ú–æ–¥–µ–ª—å':<25} {'–í—Ä–µ–º—è (—Å)':<10} {'–£—Å–ø–µ—Ö %':<10} {'–ö–∞—á–µ—Å—Ç–≤–æ':<10} {'TPS':<8}")
        print("-" * 65)
        
        for model_name, metrics in results.items():
            success_rate = (metrics.successful_requests / metrics.total_requests * 100) if metrics.total_requests > 0 else 0
            print(f"{model_name:<25} {metrics.avg_response_time:<10.2f} {success_rate:<10.1f} {metrics.avg_quality_score:<10.1f} {metrics.tokens_per_second:<8.1f}")
        
        # –ü–æ–±–µ–¥–∏—Ç–µ–ª—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        print("\nüèÖ –õ–ò–î–ï–†–´ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
        
        fastest = min(results.items(), key=lambda x: x[1].avg_response_time)
        print(f"‚ö° –°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è: {fastest[0]} ({fastest[1].avg_response_time:.2f}—Å)")
        
        highest_quality = max(results.items(), key=lambda x: x[1].avg_quality_score)
        print(f"üéØ –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {highest_quality[0]} ({highest_quality[1].avg_quality_score:.1f})")
        
        most_reliable = max(results.items(), key=lambda x: x[1].successful_requests / x[1].total_requests if x[1].total_requests > 0 else 0)
        print(f"üõ°Ô∏è –°–∞–º–∞—è –Ω–∞–¥–µ–∂–Ω–∞—è: {most_reliable[0]} ({most_reliable[1].successful_requests}/{most_reliable[1].total_requests})")
        
        # –û–±—â–∏–π –ø–æ–±–µ–¥–∏—Ç–µ–ª—å (–∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
        best_overall = self._calculate_overall_winner(results)
        print(f"\nüëë –û–ë–©–ò–ô –ü–û–ë–ï–î–ò–¢–ï–õ–¨: {best_overall}")
    
    def _calculate_overall_winner(self, results: Dict[str, BenchmarkMetrics]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–±—â–µ–≥–æ –ø–æ–±–µ–¥–∏—Ç–µ–ª—è –ø–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ."""
        scores = {}
        
        for model_name, metrics in results.items():
            # –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤–µ—Å–∞)
            speed_score = 1 / (metrics.avg_response_time + 0.1)  # –ß–µ–º –º–µ–Ω—å—à–µ –≤—Ä–µ–º—è, —Ç–µ–º –±–æ–ª—å—à–µ –æ—á–∫–æ–≤
            quality_score = metrics.avg_quality_score
            reliability_score = (metrics.successful_requests / metrics.total_requests * 100) if metrics.total_requests > 0 else 0
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            overall_score = (speed_score * 0.3 + quality_score * 0.5 + reliability_score * 0.2)
            scores[model_name] = overall_score
        
        return max(scores.items(), key=lambda x: x[1])[0]

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    benchmark = LLMBenchmark()
    
    # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    models_to_test = [
        "qwen2.5:7b-turbo",           # –ù–∞—à–∞ —Ç–µ–∫—É—â–∞—è —Ç—É—Ä–±–æ –º–æ–¥–µ–ª—å
        "qwen2.5:7b-instruct-turbo", # –ù–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è instruct –º–æ–¥–µ–ª—å
        "qwen2.5:7b-instruct",       # –ë–∞–∑–æ–≤–∞—è instruct –º–æ–¥–µ–ª—å
    ]
    
    results = await benchmark.compare_models(models_to_test, iterations=3)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open(f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", "w", encoding="utf-8") as f:
        json.dump({k: asdict(v) for k, v in results.items()}, f, ensure_ascii=False, indent=2)
    
    print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ benchmark_results_*.json")

if __name__ == "__main__":
    asyncio.run(main()) 