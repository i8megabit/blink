"""FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""

from __future__ import annotations

import asyncio
import os
import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from collections import defaultdict
from dataclasses import dataclass

import httpx
import nltk
import numpy as np
import chromadb
import ollama
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import Integer, Text, JSON, select, DateTime, ARRAY, Float, String, Index, ForeignKey, func
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, sessionmaker, relationship
from datetime import datetime

# –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
RUSSIAN_STOP_WORDS = set(stopwords.words('russian'))

@dataclass
class SemanticEntity:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—É—â–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM."""
    entity_type: str
    value: str
    confidence: float
    context: str

@dataclass 
class ThematicCluster:
    """–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä —Å—Ç–∞—Ç–µ–π."""
    cluster_id: str
    theme: str
    keywords: List[str]
    articles_count: int
    semantic_density: float

class WebSocketManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, websocket: WebSocket, client_id: str):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞."""
        await websocket.accept()
        self.active_connections[client_id] = websocket
        print(f"üîå WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω: {client_id}")
    
    def disconnect(self, client_id: str):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            print(f"üîå WebSocket –æ—Ç–∫–ª—é—á–µ–Ω: {client_id}")
    
    async def send_progress(self, client_id: str, message: dict):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–ª–∏–µ–Ω—Ç—É."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json(message)
                print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω {client_id}: {message}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ {client_id}: {e}")
    
    async def send_error(self, client_id: str, error: str, details: str = ""):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ—à–∏–±–∫–∏ –∫–ª–∏–µ–Ω—Ç—É."""
        await self.send_progress(client_id, {
            "type": "error",
            "message": error,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_step(self, client_id: str, step: str, current: int, total: int, details: str = ""):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ."""
        await self.send_progress(client_id, {
            "type": "progress",
            "step": step,
            "current": current,
            "total": total,
            "percentage": round((current / total) * 100) if total > 0 else 0,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })
    
    async def send_ollama_info(self, client_id: str, info: dict):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞–±–æ—Ç–µ Ollama."""
        await self.send_progress(client_id, {
            "type": "ollama",
            "info": info,
            "timestamp": datetime.now().isoformat()
        })


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä WebSocket
websocket_manager = WebSocketManager()

app = FastAPI()

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class Base(DeclarativeBase):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–µ–π."""


class Domain(Base):
    """–ú–æ–¥–µ–ª—å –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è."""
    
    __tablename__ = "domains"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(255), unique=True, index=True)
    display_name: Mapped[str] = mapped_column(String(500))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    language: Mapped[str] = mapped_column(String(10), default="ru")
    category: Mapped[str] = mapped_column(String(100), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_active: Mapped[bool] = mapped_column(default=True)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_posts: Mapped[int] = mapped_column(Integer, default=0)
    total_analyses: Mapped[int] = mapped_column(Integer, default=0)
    last_analysis_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    posts: Mapped[List["WordPressPost"]] = relationship("WordPressPost", back_populates="domain_ref", cascade="all, delete-orphan")
    analyses: Mapped[List["AnalysisHistory"]] = relationship("AnalysisHistory", back_populates="domain_ref", cascade="all, delete-orphan")
    themes: Mapped[List["ThematicGroup"]] = relationship("ThematicGroup", back_populates="domain_ref", cascade="all, delete-orphan")


class ThematicGroup(Base):
    """–ú–æ–¥–µ–ª—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏."""
    
    __tablename__ = "thematic_groups"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    name: Mapped[str] = mapped_column(String(200))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    keywords: Mapped[list[str]] = mapped_column(JSON)
    semantic_signature: Mapped[str] = mapped_column(Text)  # TF-IDF –ø–æ–¥–ø–∏—Å—å —Ç–µ–º—ã
    articles_count: Mapped[int] = mapped_column(Integer, default=0)
    avg_semantic_density: Mapped[float] = mapped_column(Float, default=0.0)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="themes")
    posts: Mapped[List["WordPressPost"]] = relationship("WordPressPost", back_populates="thematic_group")
    
    __table_args__ = (
        Index("idx_thematic_groups_domain_semantic", "domain_id", "avg_semantic_density"),
    )


class WordPressPost(Base):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å—Ç–∞—Ç–µ–π WordPress —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–ª—è–º–∏."""

    __tablename__ = "wordpress_posts"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    thematic_group_id: Mapped[int] = mapped_column(Integer, ForeignKey("thematic_groups.id"), nullable=True, index=True)
    wp_post_id: Mapped[int] = mapped_column(Integer)
    
    # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –ø–æ–ª—è
    title: Mapped[str] = mapped_column(Text)
    content: Mapped[str] = mapped_column(Text)
    excerpt: Mapped[str] = mapped_column(Text, nullable=True)
    link: Mapped[str] = mapped_column(Text, index=True)
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –¥–ª—è LLM
    semantic_summary: Mapped[str] = mapped_column(Text, nullable=True)  # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è LLM
    key_concepts: Mapped[list[str]] = mapped_column(JSON, default=list)  # –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
    entity_mentions: Mapped[list[dict]] = mapped_column(JSON, default=list)  # –£–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
    content_type: Mapped[str] = mapped_column(String(50), nullable=True)  # —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–≥–∞–π–¥, –æ–±–∑–æ—Ä, –Ω–æ–≤–æ—Å—Ç—å)
    difficulty_level: Mapped[str] = mapped_column(String(20), nullable=True)  # —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    target_audience: Mapped[str] = mapped_column(String(100), nullable=True)  # —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    content_quality_score: Mapped[float] = mapped_column(Float, default=0.0)
    semantic_richness: Mapped[float] = mapped_column(Float, default=0.0)  # –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏–∫–∏
    linkability_score: Mapped[float] = mapped_column(Float, default=0.0)  # –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ —Å—Ç–∞—Ç—É—Å—ã
    published_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_analyzed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="posts")
    thematic_group: Mapped["ThematicGroup"] = relationship("ThematicGroup", back_populates="posts")
    embeddings: Mapped[List["ArticleEmbedding"]] = relationship("ArticleEmbedding", back_populates="post", cascade="all, delete-orphan")
    
    __table_args__ = (
        Index("idx_wp_posts_domain_theme", "domain_id", "thematic_group_id"),
        Index("idx_wp_posts_semantic_scores", "semantic_richness", "linkability_score"),
        Index("idx_wp_posts_published", "published_at"),
    )


class ArticleEmbedding(Base):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏."""

    __tablename__ = "article_embeddings"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    
    # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embedding_type: Mapped[str] = mapped_column(String(50))  # 'title', 'content', 'summary', 'full'
    vector_model: Mapped[str] = mapped_column(String(100))  # –º–æ–¥–µ–ª—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    embedding_vector: Mapped[str] = mapped_column(Text)  # JSON –≤–µ–∫—Ç–æ—Ä–∞
    dimension: Mapped[int] = mapped_column(Integer)  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context_window: Mapped[int] = mapped_column(Integer, nullable=True)  # —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞
    preprocessing_info: Mapped[dict] = mapped_column(JSON, default=dict)  # –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ
    quality_metrics: Mapped[dict] = mapped_column(JSON, default=dict)  # –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    post: Mapped["WordPressPost"] = relationship("WordPressPost", back_populates="embeddings")
    
    __table_args__ = (
        Index("idx_embeddings_post_type", "post_id", "embedding_type"),
    )


class SemanticConnection(Base):
    """–ú–æ–¥–µ–ª—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π."""
    
    __tablename__ = "semantic_connections"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    source_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    target_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    
    # –¢–∏–ø—ã —Å–≤—è–∑–µ–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
    connection_type: Mapped[str] = mapped_column(String(50))  # 'semantic', 'topical', 'hierarchical', 'sequential', 'complementary'
    strength: Mapped[float] = mapped_column(Float)  # —Å–∏–ª–∞ —Å–≤—è–∑–∏ (0.0 - 1.0)
    confidence: Mapped[float] = mapped_column(Float)  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–≤—è–∑–∏
    
    # –ù–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    usage_count: Mapped[int] = mapped_column(Integer, default=0)  # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–≤—è–∑—å –±—ã–ª–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞
    success_rate: Mapped[float] = mapped_column(Float, default=0.0)  # —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è
    evolution_score: Mapped[float] = mapped_column(Float, default=0.0)  # —ç–≤–æ–ª—é—Ü–∏—è —Å–≤—è–∑–∏ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
    connection_context: Mapped[str] = mapped_column(Text, nullable=True)  # –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–≤—è–∑–∏
    suggested_anchor: Mapped[str] = mapped_column(String(200), nullable=True)  # –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π –∞–Ω–∫–æ—Ä
    alternative_anchors: Mapped[list[str]] = mapped_column(JSON, default=list)  # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∞–Ω–∫–æ—Ä—ã
    bidirectional: Mapped[bool] = mapped_column(default=False)  # –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å–≤—è–∑—å
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
    semantic_tags: Mapped[list[str]] = mapped_column(JSON, default=list)  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏
    theme_intersection: Mapped[str] = mapped_column(String(200), nullable=True)  # –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ç–µ–º
    
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    validated_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    last_recommended_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    
    __table_args__ = (
        Index("idx_semantic_connections_strength", "strength"),
        Index("idx_semantic_connections_usage", "usage_count", "success_rate"),
        Index("idx_semantic_connections_evolution", "evolution_score"),
    )


class LinkRecommendation(Base):
    """–ú–æ–¥–µ–ª—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –∏ —ç–≤–æ–ª—é—Ü–∏–µ–π."""
    
    __tablename__ = "link_recommendations"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    source_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    target_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    anchor_text: Mapped[str] = mapped_column(String(300))
    reasoning: Mapped[str] = mapped_column(Text)
    quality_score: Mapped[float] = mapped_column(Float, default=0.0)  # –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    
    # –ù–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    generation_count: Mapped[int] = mapped_column(Integer, default=1)  # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞—Å—å
    improvement_iterations: Mapped[int] = mapped_column(Integer, default=0)  # –∏—Ç–µ—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è
    status: Mapped[str] = mapped_column(String(50), default='active')  # active, deprecated, improved
    
    # –°–≤—è–∑—å —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é
    semantic_connection_id: Mapped[int] = mapped_column(Integer, ForeignKey("semantic_connections.id"), nullable=True)
    
    # –≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    previous_version_id: Mapped[int] = mapped_column(Integer, ForeignKey("link_recommendations.id"), nullable=True)
    improvement_reason: Mapped[str] = mapped_column(Text, nullable=True)
    
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")
    source_post: Mapped["WordPressPost"] = relationship("WordPressPost", foreign_keys=[source_post_id])
    target_post: Mapped["WordPressPost"] = relationship("WordPressPost", foreign_keys=[target_post_id])
    semantic_connection: Mapped["SemanticConnection"] = relationship("SemanticConnection")
    previous_version: Mapped["LinkRecommendation"] = relationship("LinkRecommendation", remote_side=[id])
    
    __table_args__ = (
        Index("idx_link_recommendations_quality", "quality_score"),
        Index("idx_link_recommendations_status", "status"),
        Index("idx_link_recommendations_generation", "generation_count"),
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å–≤—è–∑–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫-—Ü–µ–ª—å –≤ —Ä–∞–º–∫–∞—Ö –¥–æ–º–µ–Ω–∞
        Index("idx_link_recommendations_unique", "domain_id", "source_post_id", "target_post_id"),
    )


class ThematicClusterAnalysis(Base):
    """–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –∏—Ö —ç–≤–æ–ª—é—Ü–∏–∏."""
    
    __tablename__ = "thematic_cluster_analysis"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    
    # –ö–ª–∞—Å—Ç–µ—Ä
    cluster_name: Mapped[str] = mapped_column(String(200))
    cluster_keywords: Mapped[list[str]] = mapped_column(JSON)
    cluster_description: Mapped[str] = mapped_column(Text)
    
    # –°—Ç–∞—Ç—å–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    article_ids: Mapped[list[int]] = mapped_column(JSON)  # ID —Å—Ç–∞—Ç–µ–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    article_count: Mapped[int] = mapped_column(Integer)
    
    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    coherence_score: Mapped[float] = mapped_column(Float)  # —Å–≤—è–∑–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∞
    diversity_score: Mapped[float] = mapped_column(Float)  # —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    linkability_potential: Mapped[float] = mapped_column(Float)  # –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –ª–∏–Ω–∫–æ–≤–∫–∏
    
    # –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
    related_clusters: Mapped[dict] = mapped_column(JSON, default=dict)  # —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –∏—Ö –≤–µ—Å–∞
    cross_cluster_opportunities: Mapped[list[dict]] = mapped_column(JSON, default=list)  # –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö —Å–≤—è–∑–µ–π
    
    # –≠–≤–æ–ª—é—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
    evolution_stage: Mapped[str] = mapped_column(String(50), default='emerging')  # emerging, mature, declining
    growth_trend: Mapped[float] = mapped_column(Float, default=0.0)  # —Ç—Ä–µ–Ω–¥ —Ä–æ—Å—Ç–∞
    
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")
    
    __table_args__ = (
        Index("idx_thematic_cluster_coherence", "coherence_score"),
        Index("idx_thematic_cluster_linkability", "linkability_potential"),
        Index("idx_thematic_cluster_evolution", "evolution_stage", "growth_trend"),
    )


class CumulativeInsight(Base):
    """–ú–æ–¥–µ–ª—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    
    __tablename__ = "cumulative_insights"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    
    # –¢–∏–ø –∏–Ω—Å–∞–π—Ç–∞
    insight_type: Mapped[str] = mapped_column(String(100))  # 'pattern', 'gap', 'opportunity', 'trend'
    insight_category: Mapped[str] = mapped_column(String(100))  # 'semantic', 'structural', 'thematic'
    
    # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–∞
    title: Mapped[str] = mapped_column(String(300))
    description: Mapped[str] = mapped_column(Text)
    evidence: Mapped[dict] = mapped_column(JSON)  # –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
    impact_score: Mapped[float] = mapped_column(Float)  # –≤–∞–∂–Ω–æ—Å—Ç—å –∏–Ω—Å–∞–π—Ç–∞
    confidence_level: Mapped[float] = mapped_column(Float)  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∏–Ω—Å–∞–π—Ç–µ
    actionability: Mapped[float] = mapped_column(Float)  # –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å
    
    # –°–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
    related_posts: Mapped[list[int]] = mapped_column(JSON, default=list)
    related_clusters: Mapped[list[int]] = mapped_column(JSON, default=list)
    related_connections: Mapped[list[int]] = mapped_column(JSON, default=list)
    
    # –°—Ç–∞—Ç—É—Å –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
    status: Mapped[str] = mapped_column(String(50), default='discovered')  # discovered, validated, applied
    applied_count: Mapped[int] = mapped_column(Integer, default=0)
    
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    validated_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")
    
    __table_args__ = (
        Index("idx_cumulative_insights_impact", "impact_score"),
        Index("idx_cumulative_insights_type", "insight_type", "insight_category"),
        Index("idx_cumulative_insights_status", "status"),
    )


class AnalysisHistory(Base):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏."""

    __tablename__ = "analysis_history"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    posts_analyzed: Mapped[int] = mapped_column(Integer)
    connections_found: Mapped[int] = mapped_column(Integer)
    recommendations_generated: Mapped[int] = mapped_column(Integer)
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    recommendations: Mapped[list[dict]] = mapped_column(JSON)
    thematic_analysis: Mapped[dict] = mapped_column(JSON, default=dict)  # –∞–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏–∫
    semantic_metrics: Mapped[dict] = mapped_column(JSON, default=dict)  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    quality_assessment: Mapped[dict] = mapped_column(JSON, default=dict)  # –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    
    # LLM –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    llm_model_used: Mapped[str] = mapped_column(String(100))
    llm_context_size: Mapped[int] = mapped_column(Integer, nullable=True)
    processing_time_seconds: Mapped[float] = mapped_column(Float, nullable=True)
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    completed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="analyses")
    
    __table_args__ = (
        Index("idx_analysis_history_domain_date", "domain_id", "created_at"),
    )


class Recommendation(Base):
    """–ú–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)."""

    __tablename__ = "recommendations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    text: Mapped[str] = mapped_column(Text)
    links: Mapped[list[str]] = mapped_column(JSON)


class RecommendRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""

    text: str


class WPRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ WordPress-—Å–∞–π—Ç–∞."""

    domain: str
    client_id: Optional[str] = None
    comprehensive: Optional[bool] = False  # –§–ª–∞–≥ –¥–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏


OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
# –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è SEO –∑–∞–¥–∞—á: qwen2.5:7b - –æ—Ç–ª–∏—á–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞/—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏/—Ä–µ—Å—É—Ä—Å–æ–≤
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:7b")

DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://seo_user:seo_pass@localhost/seo_db",
)

engine = create_async_engine(DATABASE_URL)
AsyncSessionLocal = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã
chroma_client = None
tfidf_vectorizer = None

def initialize_rag_system():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é RAG-—Å–∏—Å—Ç–µ–º—É —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º."""
    global chroma_client, tfidf_vectorizer
    try:
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π RAG-—Å–∏—Å—Ç–µ–º—ã...")
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        tfidf_vectorizer = TfidfVectorizer(
            max_features=2000,  # —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            ngram_range=(1, 3),  # –¥–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã
            min_df=1,
            max_df=0.85,
            stop_words=list(RUSSIAN_STOP_WORDS),  # —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            analyzer='word',
            lowercase=True,
            token_pattern=r'\b[–∞-—è—ë]{2,}\b|\b[a-z]{2,}\b'  # —Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        chroma_client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=chromadb.Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
        chroma_client = None
        tfidf_vectorizer = None


class AdvancedRAGManager:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π RAG-–º–µ–Ω–µ–¥–∂–µ—Ä —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º."""
    
    def __init__(self):
        self.domain_collections = {}
        self.thematic_clusters = {}
        self.semantic_cache = {}
    
    async def create_semantic_knowledge_base(
        self, 
        domain: str, 
        posts: List[Dict],
        client_id: Optional[str] = None
    ) -> bool:
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π."""
        if not chroma_client:
            print("‚ùå RAG-—Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return False
            
        try:
            if client_id:
                await websocket_manager.send_step(client_id, "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", 1, 5, "–ê–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏–∫ –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π...")
            
            print(f"üß† –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è {domain}...")
            
            # –û—á–∏—â–∞–µ–º –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_name = domain.replace(".", "_").replace("-", "_")
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                chroma_client.delete_collection(name=collection_name)
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}")
            except:
                pass
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            collection = chroma_client.create_collection(
                name=collection_name,
                metadata={
                    "domain": domain,
                    "created_at": datetime.now().isoformat(),
                    "rag_version": "2.0",
                    "semantic_analysis": True
                }
            )
            
            if client_id:
                await websocket_manager.send_step(client_id, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", 2, 5, "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π...")
            
            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π
            enriched_posts = await self._enrich_posts_with_semantics(posts, domain)
            
            if client_id:
                await websocket_manager.send_step(client_id, "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", 3, 5, "–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤...")
            
            documents = []
            metadatas = []
            ids = []
            
            for i, post in enumerate(enriched_posts):
                # –°–æ–∑–¥–∞–µ–º –±–æ–≥–∞—Ç—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
                semantic_text = self._create_llm_friendly_context(post)
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, —á–∏—Å–ª–∞ –∏ –±—É–ª–µ–≤—ã –∑–Ω–∞—á–µ–Ω–∏—è
                key_concepts = post.get('key_concepts', [])
                key_concepts_str = ', '.join(key_concepts[:5]) if key_concepts else ""  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                
                documents.append(semantic_text)
                metadatas.append({
                    "title": (post.get('title', '') or '')[:300],  # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É
                    "link": post.get('link', '') or '',
                    "content_snippet": (post.get('content', '') or '')[:500],
                    "domain": domain,
                    "post_index": i,
                    "semantic_summary": (post.get('semantic_summary', '') or '')[:500],
                    "key_concepts_str": key_concepts_str,  # –°—Ç—Ä–æ–∫–∞ –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–∞
                    "key_concepts_count": len(key_concepts),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∫ —á–∏—Å–ª–æ
                    "content_type": post.get('content_type', 'article'),
                    "difficulty_level": post.get('difficulty_level', 'medium'),
                    "linkability_score": float(post.get('linkability_score', 0.5))  # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —ç—Ç–æ float
                })
                ids.append(f"{collection_name}_{i}")
            
            if not documents:
                print(f"‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {domain}")
                return False
            
            if client_id:
                await websocket_manager.send_step(client_id, "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", 4, 5, "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º...")
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
            vectorizer = TfidfVectorizer(
                max_features=1000,
                ngram_range=(1, 3),
                min_df=1,
                max_df=0.8,
                stop_words=list(RUSSIAN_STOP_WORDS),
                analyzer='word'
            )
            
            tfidf_matrix = vectorizer.fit_transform(documents)
            dense_embeddings = tfidf_matrix.toarray().tolist()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids,
                embeddings=dense_embeddings
            )
            
            if client_id:
                await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 5, 5, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
            
            self.domain_collections[domain] = collection_name
            
            print(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {len(documents)} —Å—Ç–∞—Ç–µ–π –¥–ª—è {domain}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–∞–∑—ã: {e}")
            return False
    
    async def _enrich_posts_with_semantics(self, posts: List[Dict], domain: str) -> List[Dict]:
        """–û–±–æ–≥–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
        enriched = []
        
        for post in posts:
            try:
                title = post.get('title', '')
                content = post.get('content', '')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
                key_concepts = self._extract_key_concepts(title + ' ' + content)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_type = self._classify_content_type(title, content)
                
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                difficulty = self._assess_difficulty(content)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
                linkability = self._calculate_linkability_score(title, content, key_concepts)
                
                # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ
                semantic_summary = self._create_semantic_summary(title, content, key_concepts)
                
                enriched_post = {
                    **post,
                    'key_concepts': key_concepts,
                    'content_type': content_type,
                    'difficulty_level': difficulty,
                    'linkability_score': linkability,
                    'semantic_summary': semantic_summary
                }
                
                enriched.append(enriched_post)
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ {post.get('title', 'unknown')}: {e}")
                enriched.append(post)  # –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
        
        return enriched
    
    def _extract_key_concepts(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
        words = word_tokenize(text.lower())
        words = [w for w in words if w.isalpha() and len(w) > 3 and w not in RUSSIAN_STOP_WORDS]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        return sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)[:10]
    
    def _classify_content_type(self, title: str, content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        title_lower = title.lower()
        content_lower = content.lower()
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if any(word in title_lower for word in ['–∫–∞–∫', '–≥–∞–π–¥', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è']):
            return 'guide'
        elif any(word in title_lower for word in ['–æ–±–∑–æ—Ä', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ç–µ—Å—Ç']):
            return 'review'
        elif any(word in title_lower for word in ['–Ω–æ–≤–æ—Å—Ç–∏', '–∞–Ω–æ–Ω—Å', '—Ä–µ–ª–∏–∑']):
            return 'news'
        elif len(content) < 1000:
            return 'short_article'
        else:
            return 'article'
    
    def _assess_difficulty(self, content: str) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        words = word_tokenize(content)
        avg_word_length = sum(len(w) for w in words) / len(words) if words else 0
        
        if avg_word_length < 5:
            return 'easy'
        elif avg_word_length < 7:
            return 'medium'
        else:
            return 'advanced'
    
    def _calculate_linkability_score(self, title: str, content: str, concepts: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""
        score = 0.0
        
        # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        score += min(len(content) / 2000, 0.4)  # –¥–æ 0.4 –∑–∞ –¥–ª–∏–Ω—É
        
        # –°–∫–æ—Ä –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        score += min(len(concepts) / 20, 0.3)  # –¥–æ 0.3 –∑–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        
        # –°–∫–æ—Ä –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        if any(word in title.lower() for word in ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–ø–æ—á–µ–º—É']):
            score += 0.2
            
        # –°–∫–æ—Ä –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (–ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        if content.count('.') > 5:  # –º–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            score += 0.1
            
        return min(score, 1.0)
    
    def _create_semantic_summary(self, title: str, content: str, concepts: List[str]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–ª—è LLM."""
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        sentences = content.split('.')[:3]
        summary = '. '.join(sentences).strip()
        
        if concepts:
            summary += f" –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: {', '.join(concepts[:5])}."
            
        return summary[:500]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    
    def _create_llm_friendly_context(self, post: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç LLM-–¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏."""
        title = post.get('title', '')
        summary = post.get('semantic_summary', '')
        concepts = post.get('key_concepts', [])
        content_type = post.get('content_type', 'article')
        difficulty = post.get('difficulty_level', 'medium')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context_parts = [
            f"–ó–ê–ì–û–õ–û–í–û–ö: {title}",
            f"–¢–ò–ü: {content_type}",
            f"–°–õ–û–ñ–ù–û–°–¢–¨: {difficulty}",
            f"–û–ü–ò–°–ê–ù–ò–ï: {summary}"
        ]
        
        if concepts:
            context_parts.append(f"–ö–õ–Æ–ß–ï–í–´–ï_–¢–ï–ú–´: {', '.join(concepts[:7])}")
        
        return ' | '.join(context_parts)
    
    async def get_semantic_recommendations(
        self,
        domain: str,
        limit: int = 25,
        min_linkability: float = 0.2  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    ) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        if domain not in self.domain_collections:
            return []
        
        try:
            collection = chroma_client.get_collection(name=self.domain_collections[domain])
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            results = collection.get(
                limit=limit * 2,  # –±–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                include=['metadatas', 'documents']
            )
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
            filtered_articles = []
            for i, metadata in enumerate(results['metadatas']):
                linkability_score = metadata.get('linkability_score', 0.0)
                if linkability_score >= min_linkability:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º key_concepts –∏–∑ —Å—Ç—Ä–æ–∫–∏
                    key_concepts_str = metadata.get('key_concepts_str', '')
                    key_concepts = [kc.strip() for kc in key_concepts_str.split(',') if kc.strip()] if key_concepts_str else []
                    
                    filtered_articles.append({
                        'title': metadata['title'],
                        'link': metadata['link'],
                        'content': metadata.get('content_snippet', ''),
                        'semantic_summary': metadata.get('semantic_summary', ''),
                        'key_concepts': key_concepts,
                        'content_type': metadata.get('content_type', 'article'),
                        'difficulty_level': metadata.get('difficulty_level', 'medium'),
                        'linkability_score': linkability_score,
                        'domain': metadata['domain']
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
            filtered_articles.sort(key=lambda x: x['linkability_score'], reverse=True)
            
            print(f"üéØ –ü–æ–ª—É—á–µ–Ω–æ {len(filtered_articles)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
            return filtered_articles[:limit]
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return []


class CumulativeIntelligenceManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π."""
    
    def __init__(self):
        self.connection_cache = {}
        self.cluster_cache = {}
        self.insight_cache = {}
    
    async def analyze_existing_connections(self, domain: str, session: AsyncSession) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–≤—è–∑–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        domain_result = await session.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = domain_result.scalar_one_or_none()
        if not domain_obj:
            return {"existing_connections": 0, "existing_recommendations": 0}
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
        connections_result = await session.execute(
            select(SemanticConnection)
            .join(WordPressPost, SemanticConnection.source_post_id == WordPressPost.id)
            .where(WordPressPost.domain_id == domain_obj.id)
        )
        existing_connections = connections_result.scalars().all()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations_result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_obj.id)
            .where(LinkRecommendation.status == 'active')
        )
        existing_recommendations = recommendations_result.scalars().all()
        
        return {
            "existing_connections": len(existing_connections),
            "existing_recommendations": len(existing_recommendations),
            "connections": existing_connections,
            "recommendations": existing_recommendations,
            "domain_id": domain_obj.id
        }
    
    async def discover_thematic_clusters(self, domain_id: int, posts: list, session: AsyncSession) -> list:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã."""
        from sklearn.cluster import KMeans
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np
        
        if len(posts) < 3:
            return []
        
        # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        texts = [f"{post['title']} {post.get('content', '')[:500]}" for post in posts]
        vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            stop_words=list(RUSSIAN_STOP_WORDS)
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            n_clusters = min(max(2, len(posts) // 5), 8)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
            clusters = []
            feature_names = vectorizer.get_feature_names_out()
            
            for cluster_id in range(n_clusters):
                cluster_posts = [posts[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
                if len(cluster_posts) < 2:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                cluster_center = kmeans.cluster_centers_[cluster_id]
                top_features_idx = cluster_center.argsort()[-10:][::-1]
                cluster_keywords = [feature_names[i] for i in top_features_idx]
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
                coherence_score = float(np.mean([cluster_center[i] for i in top_features_idx[:5]]))
                diversity_score = float(len(set(cluster_keywords)) / len(cluster_keywords))
                linkability_potential = coherence_score * diversity_score * len(cluster_posts) / len(posts)
                
                cluster_analysis = ThematicClusterAnalysis(
                    domain_id=domain_id,
                    cluster_name=f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id + 1}: {', '.join(cluster_keywords[:3])}",
                    cluster_keywords=cluster_keywords,
                    cluster_description=f"–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(cluster_posts)} —Å—Ç–∞—Ç–µ–π",
                    article_ids=[post.get('id', 0) for post in cluster_posts],
                    article_count=len(cluster_posts),
                    coherence_score=coherence_score,
                    diversity_score=diversity_score,
                    linkability_potential=linkability_potential,
                    evolution_stage='emerging'
                )
                
                session.add(cluster_analysis)
                clusters.append({
                    'analysis': cluster_analysis,
                    'posts': cluster_posts,
                    'keywords': cluster_keywords
                })
            
            await session.commit()
            return clusters
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return []
    
    async def generate_cumulative_insights(self, domain_id: int, analysis_data: dict, session: AsyncSession):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞."""
        insights = []
        
        # –ò–Ω—Å–∞–π—Ç –æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Å–≤—è–∑–µ–π
        if analysis_data['existing_connections'] > 0:
            connection_density = analysis_data['existing_connections'] / max(analysis_data.get('total_posts', 1), 1)
            
            if connection_density < 0.1:
                insight = CumulativeInsight(
                    domain_id=domain_id,
                    insight_type='gap',
                    insight_category='structural',
                    title='–ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–≤—è–∑–µ–π',
                    description=f'–î–æ–º–µ–Ω –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ {analysis_data["existing_connections"]} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫—É.',
                    evidence={'connection_density': connection_density, 'total_connections': analysis_data['existing_connections']},
                    impact_score=0.8,
                    confidence_level=0.9,
                    actionability=0.9
                )
                session.add(insight)
                insights.append(insight)
        
        # –ò–Ω—Å–∞–π—Ç –æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏
        clusters = analysis_data.get('clusters', [])
        if len(clusters) > 1:
            avg_linkability = sum(c['analysis'].linkability_potential for c in clusters) / len(clusters)
            
            if avg_linkability > 0.3:
                insight = CumulativeInsight(
                    domain_id=domain_id,
                    insight_type='opportunity',
                    insight_category='thematic',
                    title='–í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–µ–∂—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π',
                    description=f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(clusters)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏.',
                    evidence={'clusters_count': len(clusters), 'avg_linkability': avg_linkability},
                    impact_score=0.7,
                    confidence_level=0.8,
                    actionability=0.8
                )
                session.add(insight)
                insights.append(insight)
        
        await session.commit()
        return insights
    
    async def deduplicate_and_evolve_recommendations(
        self, 
        new_recommendations: list, 
        domain_id: int, 
        session: AsyncSession
    ) -> list:
        """–î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ—Ç –∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        existing_result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_id)
            .where(LinkRecommendation.status == 'active')
        )
        existing_recommendations = {
            (rec.source_post_id, rec.target_post_id): rec 
            for rec in existing_result.scalars().all()
        }
        
        evolved_recommendations = []
        
        for new_rec in new_recommendations:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Å—Ç—ã –≤ –ë–î
            source_result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_id)
                .where(WordPressPost.link == new_rec['from'])
            )
            source_post = source_result.scalar_one_or_none()
            
            target_result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_id)
                .where(WordPressPost.link == new_rec['to'])
            )
            target_post = target_result.scalar_one_or_none()
            
            if not source_post or not target_post:
                continue
            
            key = (source_post.id, target_post.id)
            
            if key in existing_recommendations:
                # –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                existing_rec = existing_recommendations[key]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–ª—É—á—à–∏–ª–∞—Å—å –ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
                new_quality = self._calculate_quality_score(new_rec)
                if new_quality > existing_rec.quality_score:
                    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                    improved_rec = LinkRecommendation(
                        domain_id=domain_id,
                        source_post_id=source_post.id,
                        target_post_id=target_post.id,
                        anchor_text=new_rec['anchor'],
                        reasoning=new_rec['comment'],
                        quality_score=new_quality,
                        generation_count=existing_rec.generation_count + 1,
                        improvement_iterations=existing_rec.improvement_iterations + 1,
                        previous_version_id=existing_rec.id,
                        improvement_reason=f"–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {new_quality:.2f} > {existing_rec.quality_score:.2f}"
                    )
                    
                    # –ü–æ–º–µ—á–∞–µ–º —Å—Ç–∞—Ä—É—é –∫–∞–∫ —É–ª—É—á—à–µ–Ω–Ω—É—é
                    existing_rec.status = 'improved'
                    
                    session.add(improved_rec)
                    evolved_recommendations.append(new_rec)
                else:
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    existing_rec.generation_count += 1
                    existing_rec.updated_at = datetime.utcnow()
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                quality_score = self._calculate_quality_score(new_rec)
                link_rec = LinkRecommendation(
                    domain_id=domain_id,
                    source_post_id=source_post.id,
                    target_post_id=target_post.id,
                    anchor_text=new_rec['anchor'],
                    reasoning=new_rec['comment'],
                    quality_score=quality_score
                )
                
                session.add(link_rec)
                evolved_recommendations.append(new_rec)
        
        await session.commit()
        return evolved_recommendations
    
    def _calculate_quality_score(self, recommendation: dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        score = 0.0
        
        # –û—Ü–µ–Ω–∫–∞ –∞–Ω–∫–æ—Ä–∞
        anchor = recommendation.get('anchor', '')
        if len(anchor) > 5:
            score += 0.3
        if any(word in anchor.lower() for word in ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ']):
            score += 0.2
        
        # –û—Ü–µ–Ω–∫–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
        comment = recommendation.get('comment', '')
        if len(comment) > 20:
            score += 0.3
        if len(comment) > 50:
            score += 0.2
        
        return min(score, 1.0)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã
rag_manager = AdvancedRAGManager()
cumulative_intelligence = CumulativeIntelligenceManager()


async def generate_links(text: str) -> list[str]:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—ã—Ö —Å—Å—ã–ª–æ–∫."""
    prompt = (
        "–ü—Ä–µ–¥–ª–æ–∂–∏ –¥–æ –ø—è—Ç–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        "–ö–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É –≤—ã–≤–µ–¥–∏ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ /article/–Ω–∞–∑–≤–∞–Ω–∏–µ-—Å—Ç–∞—Ç—å–∏, "
        "–æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–ª–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è. "
        f"–¢–µ–∫—Å—Ç: {text}"
    )
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.post(
            OLLAMA_URL,
            json={"model": OLLAMA_MODEL, "prompt": prompt, "stream": False},
            timeout=60,
        )
    response.raise_for_status()
    data = response.json()
    lines = [line.strip("- \n") for line in data.get("response", "").splitlines()]
    links = [line for line in lines if line]
    return links[:5]


async def fetch_and_store_wp_posts(domain: str, client_id: Optional[str] = None) -> tuple[list[dict[str, str]], dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ WordPress —Å —É–º–Ω–æ–π –¥–µ–ª—å—Ç–∞-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π."""
    print(f"üß† –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ {domain}")
    
    if client_id:
        await websocket_manager.send_step(client_id, "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π", 1, 5, "–ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –¥–æ–º–µ–Ω
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = result.scalar_one_or_none()
        
        if not domain_obj:
            domain_obj = Domain(
                name=domain,
                display_name=domain,
                description=f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –¥–æ–º–µ–Ω –¥–ª—è {domain}",
                language="ru"
            )
            session.add(domain_obj)
            await session.commit()
            await session.refresh(domain_obj)
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –¥–æ–º–µ–Ω: {domain}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        existing_posts_result = await session.execute(
            select(WordPressPost).where(WordPressPost.domain_id == domain_obj.id)
        )
        existing_posts = {post.wp_post_id: post for post in existing_posts_result.scalars().all()}
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(existing_posts)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")
    
    if client_id:
        await websocket_manager.send_step(client_id, "–ó–∞–≥—Ä—É–∑–∫–∞ —Å WordPress", 2, 5, "–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    url = f"https://{domain}/wp-json/wp/v2/posts?per_page=50"
    async with httpx.AsyncClient(timeout=120.0) as client:
        response = await client.get(url)
    if response.status_code >= 400:
        raise HTTPException(status_code=400, detail="–°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ WordPress")
    data = response.json()
    if not isinstance(data, list):
        raise HTTPException(status_code=400, detail="–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç WordPress")
    
    if client_id:
        await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π", 3, 5, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(data)} —Å—Ç–∞—Ç–µ–π...")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–ª—å—Ç–∞-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    delta_stats = {
        'new_posts': 0,
        'updated_posts': 0,
        'unchanged_posts': 0,
        'removed_posts': 0,
        'total_posts': len(data)
    }
    
    posts = []
    processed_wp_ids = set()
    seen_urls = set()
    seen_titles = set()
    
    async with AsyncSessionLocal() as session:
        for item in data:
            try:
                wp_post_id = item["id"]
                processed_wp_ids.add(wp_post_id)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = item.get("content", {}).get("rendered", "")
                excerpt = item.get("excerpt", {}).get("rendered", "")
                
                # –û—á–∏—â–∞–µ–º HTML
                clean_content = BeautifulSoup(content, 'html.parser').get_text()
                clean_excerpt = BeautifulSoup(excerpt, 'html.parser').get_text() if excerpt else ""
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –Ω–∞—à–µ–º—É –¥–æ–º–µ–Ω—É
                post_link = item["link"]
                if domain.lower() not in post_link.lower():
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞—Ç—å—é —Å —á—É–∂–æ–≥–æ –¥–æ–º–µ–Ω–∞: {post_link}")
                    continue
                
                # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
                if post_link in seen_urls:
                    print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç URL –ø—Ä–æ–ø—É—â–µ–Ω: {post_link}")
                    continue
                seen_urls.add(post_link)
                
                # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                title = item["title"]["rendered"]
                title_normalized = title.lower().strip()
                if title_normalized in seen_titles:
                    print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω: {title}")
                    continue
                seen_titles.add(title_normalized)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ—Å—Ç
                existing_post = existing_posts.get(wp_post_id)
                post_modified = datetime.fromisoformat(item.get("modified", datetime.now().isoformat()).replace('Z', '+00:00'))
                
                if existing_post:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ –ø–æ—Å—Ç
                    if (existing_post.title == title and 
                        existing_post.content == clean_content and
                        existing_post.link == post_link):
                        # –ü–æ—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
                        delta_stats['unchanged_posts'] += 1
                        print(f"‚ö° –ü–æ—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è: {title}")
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –ø–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                        posts.append({
                            "title": title, 
                            "link": post_link,
                            "content": clean_content[:800].strip()
                        })
                        continue
                    else:
                        # –ü–æ—Å—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è - –æ–±–Ω–æ–≤–ª—è–µ–º
                        existing_post.title = title
                        existing_post.content = clean_content
                        existing_post.excerpt = clean_excerpt
                        existing_post.link = post_link
                        existing_post.updated_at = datetime.utcnow()
                        existing_post.last_analyzed_at = None  # –°–±—Ä–æ—Å –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                        
                        delta_stats['updated_posts'] += 1
                        print(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω –ø–æ—Å—Ç: {title}")
                else:
                    # –ù–æ–≤—ã–π –ø–æ—Å—Ç - —Å–æ–∑–¥–∞–µ–º
                    wp_post = WordPressPost(
                        domain_id=domain_obj.id,
                        wp_post_id=wp_post_id,
                        title=title,
                        content=clean_content,
                        excerpt=clean_excerpt,
                        link=post_link,
                        published_at=datetime.fromisoformat(item.get("date", datetime.now().isoformat()).replace('Z', '+00:00')) if item.get("date") else None,
                        last_analyzed_at=None  # –ù–æ–≤—ã–π –ø–æ—Å—Ç —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞
                    )
                    session.add(wp_post)
                    
                    delta_stats['new_posts'] += 1
                    print(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ—Å—Ç: {title}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                posts.append({
                    "title": title, 
                    "link": post_link,
                    "content": clean_content[:800].strip()
                })
                
            except Exception as exc:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞ {item.get('id', 'unknown')}: {exc}")
                continue
        
        # –£–¥–∞–ª—è–µ–º –ø–æ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –Ω–∞ —Å–∞–π—Ç–µ
        for wp_post_id, existing_post in existing_posts.items():
            if wp_post_id not in processed_wp_ids:
                await session.delete(existing_post)
                delta_stats['removed_posts'] += 1
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –ø–æ—Å—Ç: {existing_post.title}")
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –≤ –ë–î
        posts_count_result = await session.execute(
            select(func.count(WordPressPost.id))
            .where(WordPressPost.domain_id == domain_obj.id)
        )
        actual_posts_count = posts_count_result.scalar()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–º–µ–Ω–∞
        domain_obj.total_posts = actual_posts_count
        domain_obj.updated_at = datetime.utcnow()
        
        print(f"üìä –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–º–µ–Ω–∞: {actual_posts_count} –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")
        
        await session.commit()
        
        if client_id:
            await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 4, 5, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")
            
        print(f"üß† –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(f"   ‚ûï –ù–æ–≤—ã–µ –ø–æ—Å—Ç—ã: {delta_stats['new_posts']}")
        print(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ: {delta_stats['updated_posts']}")
        print(f"   ‚ö° –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {delta_stats['unchanged_posts']}")
        print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω–Ω—ã–µ: {delta_stats['removed_posts']}")
        print(f"   üìä –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")
        
        if client_id:
            await websocket_manager.send_step(client_id, "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞", 5, 5, 
                f"–ù–æ–≤—ã—Ö: {delta_stats['new_posts']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {delta_stats['updated_posts']}, –£–¥–∞–ª–µ–Ω–æ: {delta_stats['removed_posts']}")
    
    return posts, delta_stats


async def generate_comprehensive_domain_recommendations(domain: str, client_id: Optional[str] = None) -> list[dict[str, str]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π."""
    print(f"üß† –ó–∞–ø—É—Å–∫ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")
    
    analysis_start_time = datetime.now()
    all_recommendations = []
    
    try:
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", 1, 12, "–ò–∑—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π...")
        
        async with AsyncSessionLocal() as session:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–≤—è–∑–∏
            existing_analysis = await cumulative_intelligence.analyze_existing_connections(domain, session)
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ {existing_analysis['existing_connections']} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π")
            print(f"üìã –ù–∞–π–¥–µ–Ω–æ {existing_analysis['existing_recommendations']} –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
            
            domain_obj = await session.get(Domain, existing_analysis['domain_id'])
            if not domain_obj:
                error_msg = f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"
                print(error_msg)
                if client_id:
                    await websocket_manager.send_error(client_id, error_msg)
                return [], 0.0
            
            # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –ø–æ—Å—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_obj.id)
                .order_by(WordPressPost.linkability_score.desc())
            )
            all_posts = result.scalars().all()
        
        if not all_posts:
            error_msg = "‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg)
            return [], 0.0
        
        print(f"üìä –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {len(all_posts)} —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î")
        
        # –®–∞–≥ 2: –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", 2, 12, f"–ê–Ω–∞–ª–∏–∑ {len(all_posts)} —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–∞–º...")
        
        full_dataset = []
        for post in all_posts:
            full_dataset.append({
                "id": post.id,
                "title": post.title,
                "link": post.link,
                "content": post.content,
                "semantic_summary": post.semantic_summary or "",
                "key_concepts": post.key_concepts or [],
                "content_type": post.content_type or "article",
                "difficulty_level": post.difficulty_level or "medium",
                "linkability_score": post.linkability_score or 0.5,
                "semantic_richness": post.semantic_richness or 0.5
            })
        
        async with AsyncSessionLocal() as session:
            # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            clusters = await cumulative_intelligence.discover_thematic_clusters(
                domain_obj.id, full_dataset, session
            )
            print(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(clusters)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            
            existing_analysis['clusters'] = clusters
            existing_analysis['total_posts'] = len(all_posts)
        
        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤
        if client_id:
            await websocket_manager.send_step(client_id, "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤", 3, 12, "–ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É–ª—É—á—à–µ–Ω–∏—è...")
        
        async with AsyncSessionLocal() as session:
            insights = await cumulative_intelligence.generate_cumulative_insights(
                domain_obj.id, existing_analysis, session
            )
            print(f"üí° –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(insights)} –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤")
        
        # –®–∞–≥ 4: –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", 4, 12, "–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
        
        success = await rag_manager.create_semantic_knowledge_base(domain, full_dataset, client_id)
        if not success:
            error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg)
            return [], 0.0
        
        # –®–∞–≥ 5: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥
        if client_id:
            await websocket_manager.send_step(client_id, "–£–º–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥", 5, 12, "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º...")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –¥–ª—è –±–æ–ª–µ–µ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        cluster_batches = []
        for cluster in clusters:
            cluster_posts = cluster['posts']
            if len(cluster_posts) >= 2:
                cluster_batches.append({
                    'posts': cluster_posts,
                    'cluster_info': cluster['analysis'],
                    'keywords': cluster['keywords']
                })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –±–∞—Ç—á–µ–π (–º–∞–∫—Å–∏–º—É–º 5)
        if len(clusters) > 1:
            cross_cluster_count = 0
            max_cross_clusters = 5  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            
            for i, cluster1 in enumerate(clusters):
                if cross_cluster_count >= max_cross_clusters:
                    break
                for cluster2 in clusters[i+1:]:
                    if cross_cluster_count >= max_cross_clusters:
                        break
                    # –ú–∏–∫—Å —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    mixed_posts = cluster1['posts'][:2] + cluster2['posts'][:2]
                    cluster_batches.append({
                        'posts': mixed_posts,
                        'cluster_info': None,  # –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                        'keywords': cluster1['keywords'] + cluster2['keywords'],
                        'cross_cluster': True
                    })
                    cross_cluster_count += 1
        
        print(f"üß† –°–æ–∑–¥–∞–Ω–æ {len(cluster_batches)} –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –±–∞—Ç—á–µ–π")
        
        # –®–∞–≥ 6-9: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ –±–∞—Ç—á–∏
        for batch_idx, batch_info in enumerate(cluster_batches, 1):
            if client_id:
                batch_type = "–ú–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π" if batch_info.get('cross_cluster') else "–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π"
                await websocket_manager.send_step(
                    client_id, 
                    f"{batch_type} –∞–Ω–∞–ª–∏–∑ {batch_idx}/{len(cluster_batches)}", 
                    5 + batch_idx, 
                    12, 
                    f"–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–º..."
                )
            
            batch_recommendations = await process_cumulative_batch_with_ollama(
                domain, batch_info, existing_analysis, batch_idx, len(cluster_batches), client_id
            )
            
            all_recommendations.extend(batch_recommendations)
            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –±–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω–æ {len(batch_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
        
        # –®–∞–≥ 10: –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —ç–≤–æ–ª—é—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", 10, 12, "–≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
        
        async with AsyncSessionLocal() as session:
            evolved_recommendations = await cumulative_intelligence.deduplicate_and_evolve_recommendations(
                all_recommendations, domain_obj.id, session
            )
            print(f"üß¨ –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–æ {len(evolved_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
        
        # –®–∞–≥ 11: –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ", 11, 12, "–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏...")
        
        final_recommendations = rank_recommendations_with_cumulative_intelligence(
            evolved_recommendations, existing_analysis, insights
        )
        
        # –®–∞–≥ 12: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        total_analysis_time = (datetime.now() - analysis_start_time).total_seconds()
        
        if client_id:
            await websocket_manager.send_step(
                client_id, 
                "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞", 
                12, 
                12, 
                f"–ì–æ—Ç–æ–≤–æ! {len(final_recommendations)} —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            )
        
        print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(final_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∑–∞ {total_analysis_time:.1f}—Å")
        return final_recommendations, total_analysis_time
        
    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}"
        print(error_msg)
        if client_id:
            await websocket_manager.send_error(client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", str(e))
        
        error_time = (datetime.now() - analysis_start_time).total_seconds()
        return [], error_time


async def process_batch_with_ollama(
    domain: str, 
    batch: List[Dict], 
    full_dataset: List[Dict], 
    batch_idx: int, 
    total_batches: int,
    client_id: Optional[str] = None
) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ Ollama —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –±–∞—Ç—á–∞
    batch_context = f"""–ê–ù–ê–õ–ò–ó –ë–ê–¢–ß–ê {batch_idx}/{total_batches} –î–û–ú–ï–ù–ê {domain}

–°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ({len(batch)}):
"""
    
    for i, article in enumerate(batch, 1):
        key_concepts_str = ', '.join(article['key_concepts'][:4]) if article['key_concepts'] else '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'
        
        batch_context += f"""
{i}. {article['title']}
   URL: {article['link']}
   –¢–∏–ø: {article['content_type']} | –°–≤—è–∑–Ω–æ—Å—Ç—å: {article['linkability_score']:.2f}
   –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {key_concepts_str}
   –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {article['semantic_summary'][:150]}
   –ö–æ–Ω—Ç–µ–Ω—Ç: {article['content'][:200]}...

"""
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç–µ–π
    batch_context += f"""–î–û–°–¢–£–ü–ù–´–ï –¶–ï–õ–ò ({len(full_dataset)} —Å—Ç–∞—Ç–µ–π):
"""
    
    targets_added = 0
    for article in full_dataset[:15]:  # –¢–æ–ø-15 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        if article not in batch and targets_added < 10:  # –ú–∞–∫—Å–∏–º—É–º 10 —Ü–µ–ª–µ–π
            batch_context += f"‚Ä¢ {article['title'][:50]} | {article['link'][:60]}\n"
            targets_added += 1
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    comprehensive_prompt = f"""{batch_context}

–ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –ª–æ–≥–∏—á–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏

–ö–†–ò–¢–ï–†–ò–ò:
‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å
‚Ä¢ –ü–æ–ª—å–∑–∞ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è  
‚Ä¢ SEO-—Ü–µ–Ω–Ω–æ—Å—Ç—å

–ü–†–ê–í–ò–õ–ê –∞–Ω–∫–æ—Ä–æ–≤:
‚Ä¢ –û–ø–∏—Å—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
‚Ä¢ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: "—Å–∞–π—Ç", "—Ä–µ—Å—É—Ä—Å", "–ø–æ—Ä—Ç–∞–ª"
‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã: "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–ø–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä"

–ù–∞–π–¥–∏ –í–°–ï –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –¥–ª—è —Å—Ç–∞—Ç–µ–π —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞.

–§–û–†–ú–ê–¢: –ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä | –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–û–¢–í–ï–¢:"""
    
    try:
        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "processing_batch",
                "batch": f"{batch_idx}/{total_batches}",
                "articles_in_batch": len(batch),
                "total_context_size": len(comprehensive_prompt),
                "model": OLLAMA_MODEL
            })
        
        print(f"ü§ñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –±–∞—Ç—á {batch_idx} —á–µ—Ä–µ–∑ Ollama (—Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(comprehensive_prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=300.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º-–∞—É—Ç –¥–æ 5 –º–∏–Ω—É—Ç
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": comprehensive_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,    # –ù–µ–º–Ω–æ–≥–æ –ø–æ–≤—ã—à–∞–µ–º –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_ctx": 4096,       # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        "num_predict": 800,    # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                        "top_p": 0.8,
                        "top_k": 40,
                        "repeat_penalty": 1.05,
                        "num_thread": 6        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
                    }
                },
                timeout=300  # 5 –º–∏–Ω—É—Ç –Ω–∞ –±–∞—Ç—á
            )
        
        request_time = (datetime.now() - start_time).total_seconds()
        
        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "batch_completed",
                "batch": f"{batch_idx}/{total_batches}",
                "processing_time": f"{request_time:.1f}s",
                "response_length": len(response.text) if response.status_code == 200 else 0
            })
        
        if response.status_code != 200:
            print(f"‚ùå Ollama –æ—à–∏–±–∫–∞ –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}: –∫–æ–¥ {response.status_code}")
            return []
        
        data = response.json()
        content = data.get("response", "")
        
        print(f"üìù –ë–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞
        batch_recommendations = parse_ollama_recommendations(content, domain, full_dataset)
        
        return batch_recommendations
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_idx}: {e}")
        return []


async def process_cumulative_batch_with_ollama(
    domain: str, 
    batch_info: dict, 
    existing_analysis: dict, 
    batch_idx: int, 
    total_batches: int,
    client_id: Optional[str] = None
) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π."""
    
    posts = batch_info['posts']
    keywords = batch_info.get('keywords', [])
    is_cross_cluster = batch_info.get('cross_cluster', False)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
    batch_context = f"""–ö–£–ú–£–õ–Ø–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –î–û–ú–ï–ù–ê {domain}

–ö–û–ù–¢–ï–ö–°–¢ –ù–ê–ö–û–ü–õ–ï–ù–ù–´–• –ó–ù–ê–ù–ò–ô:
‚Ä¢ –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π: {existing_analysis['existing_connections']}
‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {existing_analysis['existing_recommendations']}
‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(existing_analysis.get('clusters', []))}

–°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ({len(posts)}):
"""
    
    for i, post in enumerate(posts, 1):
        post_concepts = ', '.join(post.get('key_concepts', [])[:3])
        batch_context += f"""
{i}. {post['title']}
   URL: {post['link']}
   –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {post_concepts}
   –¢–∏–ø: {post.get('content_type', 'article')}
   –°–≤—è–∑–Ω–æ—Å—Ç—å: {post.get('linkability_score', 0.5):.2f}
   –ö–æ–Ω—Ç–µ–Ω—Ç: {post.get('content', '')[:150]}...

"""
    
    if is_cross_cluster:
        batch_context += f"""
üîó –ú–ï–ñ–ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó
–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è: {', '.join(keywords[:8])}
–ó–ê–î–ê–ß–ê: –ù–∞–π—Ç–∏ –≥–ª—É–±–æ–∫–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏
"""
    else:
        batch_context += f"""
üéØ –í–ù–£–¢–†–ò–ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó  
–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords[:6])}
–ó–ê–î–ê–ß–ê: –ù–∞–π—Ç–∏ –ª–æ–≥–∏—á–Ω—ã–µ —Å–≤—è–∑–∏ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
"""
    
    # –£–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    cumulative_prompt = f"""{batch_context}

–ü–†–ò–ù–¶–ò–ü–´ –ö–£–ú–£–õ–Ø–¢–ò–í–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:
‚úÖ –ò–∑–±–µ–≥–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö {existing_analysis['existing_recommendations']} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å –ù–û–í–´–ï —Å–≤—è–∑–∏, –¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
‚úÖ –£—á–∏—Ç—ã–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ, –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏

–ö–†–ò–¢–ï–†–ò–ò –ö–ê–ß–ï–°–¢–í–ê:
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å
‚Ä¢ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π
‚Ä¢ SEO-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å

–§–û–†–ú–ê–¢: –ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä | –≥–ª—É–±–æ–∫–æ–µ_–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–û–¢–í–ï–¢:"""
    
    try:
        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "processing_cumulative_batch",
                "batch": f"{batch_idx}/{total_batches}",
                "batch_type": "–º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π" if is_cross_cluster else "–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π",
                "articles_count": len(posts),
                "context_size": len(cumulative_prompt),
                "existing_connections": existing_analysis['existing_connections']
            })
        
        print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞ {batch_idx} ({'–º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π' if is_cross_cluster else '–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π'})")
        
        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=240.0) as client:
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": cumulative_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.4,    # –ß—É—Ç—å –≤—ã—à–µ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_ctx": 4096,       
                        "num_predict": 600,    
                        "top_p": 0.85,
                        "top_k": 50,
                        "repeat_penalty": 1.1,
                        "num_thread": 6
                    }
                },
                timeout=240
            )
        
        request_time = (datetime.now() - start_time).total_seconds()
        
        if response.status_code != 200:
            print(f"‚ùå Ollama –æ—à–∏–±–∫–∞ –¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –±–∞—Ç—á–∞ {batch_idx}: –∫–æ–¥ {response.status_code}")
            return []
        
        data = response.json()
        content = data.get("response", "")
        
        print(f"üìù –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        batch_recommendations = parse_ollama_recommendations(content, domain, posts)
        
        return batch_recommendations
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–∞—Ç—á–∞ {batch_idx}: {e}")
        return []


def rank_recommendations_with_cumulative_intelligence(
    recommendations: List[Dict], 
    existing_analysis: dict, 
    insights: List
) -> List[Dict]:
    """–†–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞."""
    
    def cumulative_quality_score(rec):
        score = 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        anchor = rec.get('anchor', '')
        comment = rec.get('comment', '')
        
        # –î–ª–∏–Ω–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∫–æ—Ä–∞
        if len(anchor) > 5:
            score += 0.2
        if len(anchor) > 15:
            score += 0.1
        
        # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∞–Ω–∫–æ—Ä–µ
        quality_words = ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–æ–±–∑–æ—Ä', '–≥–∞–π–¥', '–∞–Ω–∞–ª–∏–∑']
        anchor_quality = sum(1 for word in quality_words if word in anchor.lower()) * 0.15
        score += anchor_quality
        
        # –ì–ª—É–±–∏–Ω–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
        if len(comment) > 30:
            score += 0.2
        if len(comment) > 60:
            score += 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏
        semantic_words = ['—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏', '—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏', '–ª–æ–≥–∏—á–µ—Å–∫–∏', '–¥–æ–ø–æ–ª–Ω—è–µ—Ç', '—É–≥–ª—É–±–ª—è–µ—Ç', '—Ä–∞—Å—à–∏—Ä—è–µ—Ç']
        semantic_bonus = sum(1 for word in semantic_words if word in comment.lower()) * 0.1
        score += semantic_bonus
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–æ–≤–∏–∑–Ω—É (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö)
        # –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞, —Ç–∞–∫ –∫–∞–∫ —Ç–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ë–î
        score += 0.1  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–æ–≤—ã–µ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        
        return min(score, 1.0)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É
    ranked_recommendations = sorted(recommendations, key=cumulative_quality_score, reverse=True)
    
    print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(ranked_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    
    return ranked_recommendations[:30]  # –¢–æ–ø-30 —Å–∞–º—ã—Ö —Ü–µ–Ω–Ω—ã—Ö


def deduplicate_and_rank_recommendations(recommendations: List[Dict], domain: str) -> List[Dict]:
    """–î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ—Ç –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—É—Å—Ç–∞—Ä–µ–≤—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è)."""
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –ø–∞—Ä–µ –∏—Å—Ç–æ—á–Ω–∏–∫->—Ü–µ–ª—å
    seen_pairs = set()
    unique_recommendations = []
    
    for rec in recommendations:
        pair_key = (rec['from'], rec['to'])
        if pair_key not in seen_pairs:
            seen_pairs.add(pair_key)
            unique_recommendations.append(rec)
    
    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∞–Ω–∫–æ—Ä–∞ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
    def quality_score(rec):
        anchor_score = len(rec['anchor']) * 0.1  # –î–ª–∏–Ω–∞ –∞–Ω–∫–æ—Ä–∞
        comment_score = len(rec['comment']) * 0.05  # –î–ª–∏–Ω–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∞–Ω–∫–æ—Ä–µ
        quality_words = ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–æ–±–∑–æ—Ä', '–≥–∞–π–¥']
        anchor_quality = sum(1 for word in quality_words if word in rec['anchor'].lower()) * 2
        
        return anchor_score + comment_score + anchor_quality
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
    ranked_recommendations = sorted(unique_recommendations, key=quality_score, reverse=True)
    
    print(f"üéØ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: {len(recommendations)} -> {len(unique_recommendations)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    
    return ranked_recommendations[:50]  # –¢–æ–ø-50 —Å–∞–º—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö


async def generate_rag_recommendations(domain: str, client_id: Optional[str] = None) -> list[dict[str, str]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è RAG-–ø–æ–¥—Ö–æ–¥ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î."""
    print(f"üöÄ –ó–∞–ø—É—Å–∫ RAG-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞
    analysis_start_time = datetime.now()
    request_time = 0.0
    
    try:
        # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π", 1, 7, "–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        
        async with AsyncSessionLocal() as session:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω
            domain_result = await session.execute(
                select(Domain).where(Domain.name == domain)
            )
            domain_obj = domain_result.scalar_one_or_none()
            
            if not domain_obj:
                error_msg = f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"
                print(error_msg)
                if client_id:
                    await websocket_manager.send_error(client_id, error_msg)
                return [], 0.0
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_obj.id)
                .order_by(WordPressPost.linkability_score.desc(), WordPressPost.published_at.desc())
                .limit(100)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            )
            db_posts = result.scalars().all()
        
        if not db_posts:
            error_msg = "‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è RAG-–∞–Ω–∞–ª–∏–∑–∞"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg, f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}")
            return [], 0.0
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(db_posts)} —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è RAG
        posts_data = []
        for post in db_posts:
            posts_data.append({
                "title": post.title,
                "link": post.link,
                "content": post.content[:1000]  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
            })
        
        # –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã
        if client_id:
            await websocket_manager.send_step(client_id, "–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã", 2, 7, "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
        
        success = await rag_manager.create_semantic_knowledge_base(domain, posts_data, client_id)
        if not success:
            error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg, "–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return [], 0.0
        
        # –®–∞–≥ 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–∞ —Å—Ç–∞—Ç–µ–π
        if client_id:
            await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", 3, 7, "–í—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        
        articles = await rag_manager.get_semantic_recommendations(domain, limit=12)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if not articles:
            error_msg = "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å—Ç–∞—Ç—å–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg, "–ü—É—Å—Ç–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
            return [], 0.0
        
        print(f"üìã –í—ã–±—Ä–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        if client_id:
            await websocket_manager.send_step(client_id, "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ò–ò", 4, 7, "–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è Ollama...")
        
        # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∏ –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        articles_context = ""
        for i, article in enumerate(articles, 1):
            title = article['title'][:80]
            content_snippet = article['content'][:200] if article.get('content') else ""  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–Ω–∏–ø–ø–µ—Ç
            key_concepts = article.get('key_concepts', [])[:5]  # –¢–æ–ø-5 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
            content_type = article.get('content_type', 'article')
            linkability = article.get('linkability_score', 0.5)
            
            articles_context += f"""–°—Ç–∞—Ç—å—è {i}: {title}
URL: {article['link']}
–¢–∏–ø: {content_type} | –°–≤—è–∑–Ω–æ—Å—Ç—å: {linkability:.2f}
–ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {', '.join(key_concepts) if key_concepts else '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'}
–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {content_snippet}...

"""
        
        # –£–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç - –ø—É—Å—Ç—å –ò–ò —Å–∞–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        qwen_optimized_prompt = f"""–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏ —Å–∞–π—Ç–∞ {domain}

–í–ê–ñ–ù–û: –°–æ–∑–¥–∞—é—Ç—Å—è –í–ù–£–¢–†–ï–ù–ù–ò–ï —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –û–î–ù–û–ì–û —Å–∞–π—Ç–∞ {domain}

–î–æ—Å—Ç—É–ø–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
{articles_context}

–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –∏ —Å–æ–∑–¥–∞—Ç—å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫

–ö–†–ò–¢–ï–†–ò–ò –∫–∞—á–µ—Å—Ç–≤–∞:
‚úÖ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏
‚úÖ –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è  
‚úÖ SEO-—Ü–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–∞–π—Ç–∞
‚úÖ –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∫–æ—Ä–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

–ü–†–ê–í–ò–õ–ê –¥–ª—è –∞–Ω–∫–æ—Ä–æ–≤:
- –û–ø–∏—Å—ã–≤–∞—Ç—å –°–û–î–ï–†–ñ–ê–ù–ò–ï —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: "–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç", "–ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç", "–≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"
- –ü—Ä–∏–º–µ—Ä—ã –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–∫–æ—Ä–æ–≤: "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞", "–¥–µ—Ç–∞–ª—å–Ω—ã–π –æ–±–∑–æ—Ä"
- –ê–Ω–∫–æ—Ä –¥–æ–ª–∂–µ–Ω –æ—Ä–≥–∞–Ω–∏—á–Ω–æ –≤–ø–∏—Å—ã–≤–∞—Ç—å—Å—è –≤ —Ç–µ–∫—Å—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞

–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –°–æ–∑–¥–∞–π —Å—Ç–æ–ª—å–∫–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, —Å–∫–æ–ª—å–∫–æ –Ω–∞–π–¥–µ—à—å –ª–æ–≥–∏—á–Ω—ã—Ö –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏. –ú–∏–Ω–∏–º—É–º 5, –º–∞–∫—Å–∏–º—É–º –æ–ø—Ä–µ–¥–µ–ª–∏ —Å–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

–§–û–†–ú–ê–¢:
–ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä_—Ç–µ–∫—Å—Ç | –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ_—Å–≤—è–∑–∏

–ü–†–ò–ú–ï–†:
{articles[0]['link']} -> {articles[1]['link'] if len(articles) > 1 else articles[0]['link']} | –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ç–µ–º–µ | —Å—Ç–∞—Ç—å–∏ –¥–æ–ø–æ–ª–Ω—è—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏

–û–¢–í–ï–¢:"""

        # –®–∞–≥ 5: –ó–∞–ø—Ä–æ—Å –∫ Ollama
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–ø—Ä–æ—Å –∫ Ollama", 5, 7, "–û—Ç–ø—Ä–∞–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ò–ò...")
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏ –∑–∞–ø—Ä–æ—Å–∞
            await websocket_manager.send_ollama_info(client_id, {
                "status": "starting",
                "model": OLLAMA_MODEL,
                "model_info": "qwen2.5:7b - –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π",
                "articles_count": len(articles),
                "prompt_length": len(qwen_optimized_prompt),
                "timeout": 120,
                "settings": "temperature=0.3, ctx=6144, predict=600, threads=6",
                "expected_recommendations": "–º–∏–Ω–∏–º—É–º 5, –º–∞–∫—Å–∏–º—É–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ò–ò"
            })
        
        print("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è qwen2.5...")
        print(f"üìù –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(qwen_optimized_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã qwen2.5:7b
        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=120.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 2 –º–∏–Ω—É—Ç
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": qwen_optimized_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,    # –ù–µ–º–Ω–æ–≥–æ –ø–æ–≤—ã—à–∞–µ–º –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_ctx": 6144,       # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π
                        "num_predict": 600,    # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        "top_p": 0.8,         # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º top_p
                        "top_k": 40,          # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—ã–±–æ—Ä
                        "repeat_penalty": 1.05, # –°–Ω–∏–∂–∞–µ–º repeat_penalty
                        "seed": 42,           # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–µ—Ä–Ω–æ
                        "stop": ["```", "–ö–û–ù–ï–¶", "---", "\n\n\n"],
                        "num_thread": 6      # –ë–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    }
                },
                timeout=120  # –î—É–±–ª–∏—Ä—É–µ–º —Ç–∞–π–º-–∞—É—Ç
            )
        
        request_time = (datetime.now() - start_time).total_seconds()
        
        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "completed",
                "response_code": response.status_code,
                "request_time": f"{request_time:.1f}s",
                "response_length": len(response.text) if response.status_code == 200 else 0
            })
        
        if response.status_code != 200:
            error_msg = f"‚ùå Ollama –≤–µ—Ä–Ω—É–ª–∞ –∫–æ–¥ {response.status_code}"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg, f"HTTP —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            return [], 0.0
        
        data = response.json()
        content = data.get("response", "")
        print(f"üìù –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print("üîç –û–¢–õ–ê–î–ö–ê: –û—Ç–≤–µ—Ç Ollama:")
        print("="*50)
        print(content)
        print("="*50)
        
        # –®–∞–≥ 6: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if client_id:
            await websocket_manager.send_step(client_id, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞", 6, 7, "–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç –ò–ò...")
        
        recommendations = parse_ollama_recommendations(content, domain, articles)
        
        print(f"üìä –û–¢–õ–ê–î–ö–ê: –ü–∞—Ä—Å–µ—Ä –Ω–∞—à–µ–ª {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏–∑ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        
        # –®–∞–≥ 7: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ", 7, 7, f"–ì–æ—Ç–æ–≤–æ! –ü–æ–ª—É—á–µ–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞
        total_analysis_time = (datetime.now() - analysis_start_time).total_seconds()
        print(f"‚úÖ RAG-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∑–∞ {total_analysis_time:.1f}—Å")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –≤—Ä–µ–º—è –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        return recommendations[:25], total_analysis_time  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 25 –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        
    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ RAG-–∞–Ω–∞–ª–∏–∑–∞: {e}"
        print(error_msg)
        if client_id:
            await websocket_manager.send_error(client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞", str(e))
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∏ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ –æ—à–∏–±–∫–∏
        error_time = (datetime.now() - analysis_start_time).total_seconds()
        return [], error_time


def parse_ollama_recommendations(text: str, domain: str, articles: List[Dict]) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Ollama —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–æ–º–µ–Ω–∞ - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è."""
    recommendations = []
    
    # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö URL –¥–ª—è –¥–æ–º–µ–Ω–∞
    valid_urls = set()
    for article in articles:
        url = article['link']
        if domain.lower() in url.lower():
            valid_urls.add(url)
    
    print(f"üîç –û–¢–õ–ê–î–ö–ê: –í–∞–ª–∏–¥–Ω—ã–µ URL –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}: {len(valid_urls)}")
    
    lines = text.splitlines()
    print(f"üîç –û–¢–õ–ê–î–ö–ê: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(lines)} —Å—Ç—Ä–æ–∫ –æ—Ç–≤–µ—Ç–∞")
    
    for i, line in enumerate(lines, 1):
        line = line.strip()
        print(f"   –°—Ç—Ä–æ–∫–∞ {i}: {line[:100]}...")
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç Ollama
        if ('**–ò—Å—Ç–æ—á–Ω–∏–∫:**' in line and '**–¶–µ–ª—å:**' in line) or ('->' in line and '|' in line):
            print(f"      ‚úì –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ —Å—Ç—Ä–æ–∫–µ {i}")
            try:
                source = ""
                target = ""
                anchor = ""
                comment = ""
                
                # –§–æ—Ä–º–∞—Ç 1: **–ò—Å—Ç–æ—á–Ω–∏–∫:** URL **–¶–µ–ª—å:** URL | –∞–Ω–∫–æ—Ä | –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                if '**–ò—Å—Ç–æ—á–Ω–∏–∫:**' in line and '**–¶–µ–ª—å:**' in line:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                    source_match = line.split('**–ò—Å—Ç–æ—á–Ω–∏–∫:**')[1].split('**–¶–µ–ª—å:**')[0].strip()
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ —Å–∫–æ–±–æ–∫ –∏–ª–∏ –±–µ—Ä–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    if '(' in source_match and ')' in source_match:
                        source = source_match.split('(')[1].split(')')[0].strip()
                    else:
                        source = source_match.strip()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–ª—å –∏ –∞–Ω–∫–æ—Ä
                    target_part = line.split('**–¶–µ–ª—å:**')[1]
                    if '|' in target_part:
                        target_and_anchor = target_part.split('|')
                        target_raw = target_and_anchor[0].strip()
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL —Ü–µ–ª–∏
                        if '(' in target_raw and ')' in target_raw:
                            target = target_raw.split('(')[1].split(')')[0].strip()
                        else:
                            target = target_raw.strip()
                        
                        # –ê–Ω–∫–æ—Ä –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                        if len(target_and_anchor) >= 2:
                            anchor = target_and_anchor[1].strip().strip('"')
                        if len(target_and_anchor) >= 3:
                            comment = target_and_anchor[2].strip()
                    
                # –§–æ—Ä–º–∞—Ç 2: URL -> URL | –∞–Ω–∫–æ—Ä | –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
                elif '->' in line and '|' in line:
                    parts = line.split('|', 2)
                    if len(parts) >= 3:
                        link_part = parts[0].strip()
                        anchor = parts[1].strip().strip('"')
                        comment = parts[2].strip()
                        
                        if '->' in link_part:
                            source_target = link_part.split('->', 1)
                            if len(source_target) == 2:
                                source = source_target[0].strip()
                                target = source_target[1].strip()
                
                print(f"      - –ò—Å—Ç–æ—á–Ω–∏–∫: {source[:60]}...")
                print(f"      - –¶–µ–ª—å: {target[:60]}...")
                print(f"      - –ê–Ω–∫–æ—Ä: {anchor}")
                print(f"      - –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: {comment[:50]}...")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
                if not source or not target or not anchor:
                    print(f"      ‚ùå –ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                    continue
                
                if len(anchor) < 3 or len(comment) < 5:
                    print(f"      ‚ùå –ö–∞—á–µ—Å—Ç–≤–æ: –∞–Ω–∫–æ—Ä {len(anchor)} —Å–∏–º–≤–æ–ª–æ–≤, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π {len(comment)} —Å–∏–º–≤–æ–ª–æ–≤")
                    continue
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∞–Ω–∫–æ—Ä—ã –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
                bad_anchor_patterns = [
                    '–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç', '–ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç', '—Å–∞–π—Ç', '–≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞',
                    '–¥–æ–º–µ–Ω', '—Ä–µ—Å—É—Ä—Å', '–ø–æ—Ä—Ç–∞–ª', '–≤–µ–±-—Å–∞–π—Ç', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ä–µ—Å—É—Ä—Å'
                ]
                anchor_lower = anchor.lower()
                if any(pattern in anchor_lower for pattern in bad_anchor_patterns):
                    print(f"      ‚ùå –ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –∞–Ω–∫–æ—Ä –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Å—ã–ª–∫–∏: {anchor}")
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å URL
                source_valid = domain.lower() in source.lower() and source != target
                target_valid = domain.lower() in target.lower()
                
                print(f"      - –ò—Å—Ç–æ—á–Ω–∏–∫ –≤–∞–ª–∏–¥–µ–Ω: {source_valid}")
                print(f"      - –¶–µ–ª—å –≤–∞–ª–∏–¥–Ω–∞: {target_valid}")
                
                if source_valid and target_valid:
                    recommendations.append({
                        "from": source,
                        "to": target,
                        "anchor": anchor,
                        "comment": comment
                    })
                    print(f"      ‚úÖ –ü–†–ò–ù–Ø–¢–ê —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è #{len(recommendations)}")
                else:
                    print(f"      ‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–∞: –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ URL –∏–ª–∏ –¥–æ–º–µ–Ω")
                    
            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {i}: {e}")
                continue
        else:
            if line and not line.startswith('#') and len(line) > 10:
                print(f"      - –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç—Ä–æ–∫—É: {line[:50]}...")
    
    print(f"üìä –§–ò–ù–ê–õ: –ù–∞–π–¥–µ–Ω–æ {len(recommendations)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    return recommendations


@app.on_event("startup")
async def on_startup() -> None:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ."""
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG-—Å–∏—Å—Ç–µ–º—É
    initialize_rag_system()


@app.post("/api/v1/test")
async def test(req: RecommendRequest) -> dict[str, str]:
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint."""
    return {"message": f"–ü–æ–ª—É—á–µ–Ω —Ç–µ–∫—Å—Ç: {req.text[:50]}..."}


@app.post("/api/v1/recommend")
async def recommend(req: RecommendRequest) -> dict[str, list[str]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""
    links = await generate_links(req.text)

    async with AsyncSessionLocal() as session:
        rec = Recommendation(text=req.text, links=links)
        session.add(rec)
        await session.commit()
    return {"links": links}


@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    """WebSocket —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
    await websocket_manager.connect(websocket, client_id)
    try:
        while True:
            # –û–∂–∏–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ (–ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è)
            data = await websocket.receive_text()
            if data == "ping":
                await websocket.send_text("pong")
    except WebSocketDisconnect:
        websocket_manager.disconnect(client_id)


@app.post("/api/v1/wp_index")
async def wp_index_domain(req: WPRequest) -> dict[str, object]:
    """–£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
    try:
        if req.client_id:
            await websocket_manager.send_step(
                req.client_id, 
                "–ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", 
                0, 
                5, 
                f"–£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ {req.domain}"
            )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —É–º–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
        posts, delta_stats = await fetch_and_store_wp_posts(req.domain, req.client_id)
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        success = await rag_manager.create_semantic_knowledge_base(req.domain, posts, req.client_id)
        
        if req.client_id:
            await websocket_manager.send_progress(req.client_id, {
                "type": "complete",
                "message": "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!",
                "delta_stats": delta_stats,
                "posts_count": len(posts),
                "indexed": success,
                "timestamp": datetime.now().isoformat()
            })
        
        return {
            "success": True,
            "domain": req.domain,
            "posts_count": len(posts),
            "delta_stats": delta_stats,
            "indexed": success
        }
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}"
        print(f"‚ùå {error_msg}")
        
        if req.client_id:
            await websocket_manager.send_error(req.client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", error_msg)
        
        raise HTTPException(status_code=500, detail=error_msg)


@app.post("/api/v1/wp_generate_recommendations")  
async def wp_generate_recommendations(req: WPRequest) -> dict[str, list[dict[str, str]]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    try:
        if req.client_id:
            await websocket_manager.send_step(
                req.client_id, 
                "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π", 
                0, 
                9, 
                f"–ê–Ω–∞–ª–∏–∑ –¥–æ–º–µ–Ω–∞ {req.domain} –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
        rag_result = await generate_comprehensive_domain_recommendations(req.domain, req.client_id)
        
        if isinstance(rag_result, tuple) and len(rag_result) == 2:
            recs, total_analysis_time = rag_result
        else:
            recs = rag_result if isinstance(rag_result, list) else []
            total_analysis_time = 0.0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        async with AsyncSessionLocal() as session:
            domain_result = await session.execute(
                select(Domain).where(Domain.name == req.domain)
            )
            domain_obj = domain_result.scalar_one_or_none()
            
            if domain_obj:
                domain_obj.total_analyses += 1
                domain_obj.last_analysis_at = datetime.utcnow()
                
                analysis = AnalysisHistory(
                    domain_id=domain_obj.id,
                    posts_analyzed=domain_obj.total_posts,
                    connections_found=len(recs),
                    recommendations_generated=len(recs),
                    recommendations=recs,
                    thematic_analysis={
                        "analysis_type": "recommendations_generation",
                        "domain_indexed": True,
                        "smart_delta_indexing": True
                    },
                    semantic_metrics={
                        "recommendations_generated": len(recs),
                        "processing_time": total_analysis_time
                    },
                    quality_assessment={
                        "methodology": "comprehensive_analysis",
                        "completeness": "exhaustive"
                    },
                    llm_model_used=OLLAMA_MODEL,
                    processing_time_seconds=total_analysis_time,
                    completed_at=datetime.utcnow()
                )
                session.add(analysis)
                await session.commit()
        
        if req.client_id:
            await websocket_manager.send_progress(req.client_id, {
                "type": "complete",
                "message": "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!",
                "recommendations_count": len(recs),
                "timestamp": datetime.now().isoformat()
            })
        
        return {"recommendations": recs}
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {str(e)}"
        print(f"‚ùå {error_msg}")
        
        if req.client_id:
            await websocket_manager.send_error(req.client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", error_msg)
        
        raise HTTPException(status_code=500, detail=error_msg)


@app.post("/api/v1/wp_comprehensive")
async def wp_comprehensive_analysis(req: WPRequest) -> dict[str, list[dict[str, str]]]:
    """–ü–æ–ª–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ —Å –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–º –∞–Ω–∞–ª–∏–∑–æ–º –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π."""
    try:
        if req.client_id:
            await websocket_manager.send_step(
                req.client_id, 
                "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", 
                0, 
                10, 
                f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–µ–º—É –∞–Ω–∞–ª–∏–∑—É –¥–æ–º–µ–Ω–∞ {req.domain}"
            )
        
        # –≠—Ç–∞–ø 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã
        if req.client_id:
            await websocket_manager.send_step(req.client_id, "–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö", 1, 10, "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–µ–π –≤ –ë–î...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å—Ç–∞—Ç—å–∏ –≤ –ë–î
        async with AsyncSessionLocal() as session:
            domain_result = await session.execute(
                select(Domain).where(Domain.name == req.domain)
            )
            domain_obj = domain_result.scalar_one_or_none()
            
            posts_count = 0
            if domain_obj:
                posts_result = await session.execute(
                    select(WordPressPost).where(WordPressPost.domain_id == domain_obj.id)
                )
                posts_count = len(posts_result.scalars().all())
        
        if posts_count == 0:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å—Ç—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            if req.client_id:
                await websocket_manager.send_step(req.client_id, "–ó–∞–≥—Ä—É–∑–∫–∞ WordPress", 2, 10, "–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞...")
            
            posts_data = await fetch_and_store_wp_posts(req.domain, req.client_id)
            if isinstance(posts_data, tuple):
                posts, delta_stats = posts_data
            else:
                posts = posts_data
            posts_count = len(posts)
        else:
            if req.client_id:
                await websocket_manager.send_step(req.client_id, "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–µ—à–∞", 2, 10, f"–ù–∞–π–¥–µ–Ω–æ {posts_count} —Å—Ç–∞—Ç–µ–π –≤ –ë–î")
        
        print(f"üèóÔ∏è –ù–∞—á–∏–Ω–∞—é –ø–æ–ª–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é {posts_count} —Å—Ç–∞—Ç–µ–π –¥–æ–º–µ–Ω–∞ {req.domain}")
        
        # –≠—Ç–∞–ø—ã 3-10: –ü–æ–ª–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        rag_result = await generate_comprehensive_domain_recommendations(req.domain, req.client_id)
        
        if isinstance(rag_result, tuple) and len(rag_result) == 2:
            recs, total_analysis_time = rag_result
        else:
            recs = rag_result if isinstance(rag_result, list) else []
            total_analysis_time = 0.0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–∞
        async with AsyncSessionLocal() as session:
            domain_result = await session.execute(
                select(Domain).where(Domain.name == req.domain)
            )
            domain_obj = domain_result.scalar_one_or_none()
            
            if domain_obj:
                domain_obj.total_analyses += 1
                domain_obj.last_analysis_at = datetime.utcnow()
                
                analysis = AnalysisHistory(
                    domain_id=domain_obj.id,
                    posts_analyzed=posts_count,
                    connections_found=len(recs),
                    recommendations_generated=len(recs),
                    recommendations=recs,
                    thematic_analysis={
                        "analysis_type": "comprehensive_domain_indexing",
                        "total_posts_indexed": posts_count,
                        "batch_processing": True,
                        "comprehensive_analysis": True,
                        "coverage": "exhaustive"
                    },
                    semantic_metrics={
                        "indexing_depth": "maximum",
                        "context_utilization": "full_database",
                        "batch_analysis": True,
                        "semantic_links_found": len(recs)
                    },
                    quality_assessment={
                        "methodology": "comprehensive_domain_indexing",
                        "completeness": "exhaustive",
                        "quality_ranking": "applied",
                        "deduplication": "performed"
                    },
                    llm_model_used=f"{OLLAMA_MODEL} (batch_processing)",
                    processing_time_seconds=total_analysis_time,
                    completed_at=datetime.utcnow()
                )
                session.add(analysis)
                await session.commit()
        
        if req.client_id:
            await websocket_manager.send_progress(req.client_id, {
                "type": "complete",
                "message": "–ü–æ–ª–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!",
                "recommendations_count": len(recs),
                "posts_count": posts_count,
                "analysis_type": "comprehensive",
                "timestamp": datetime.now().isoformat()
            })
        
        return {"recommendations": recs}
        
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}"
        print(f"‚ùå {error_msg}")
        
        if req.client_id:
            await websocket_manager.send_error(req.client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", error_msg)
        
        raise HTTPException(status_code=500, detail=error_msg)


@app.get("/api/v1/health")
async def health() -> dict[str, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏."""
    return {"status": "ok"}


@app.get("/api/v1/ollama_status")
async def ollama_status() -> dict[str, object]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama –∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    try:
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ Ollama: {OLLAMA_URL}")
        
        status_info = {
            "ollama_url": OLLAMA_URL,
            "model_name": OLLAMA_MODEL,
            "server_available": False,
            "model_loaded": False,
            "model_info": None,
            "error": None,
            "ready_for_work": False,
            "last_check": datetime.now().isoformat()
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞ Ollama
        async with httpx.AsyncClient(timeout=10.0) as client:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞
                health_response = await client.get(f"{OLLAMA_URL.replace('/api/generate', '')}/api/tags")
                
                if health_response.status_code == 200:
                    status_info["server_available"] = True
                    models_data = health_response.json()
                    available_models = [model["name"] for model in models_data.get("models", [])]
                    status_info["available_models"] = available_models
                    
                    print(f"‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å
                    if OLLAMA_MODEL in available_models:
                        status_info["model_loaded"] = True
                        
                        # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
                        try:
                            model_info_response = await client.post(
                                f"{OLLAMA_URL.replace('/api/generate', '')}/api/show",
                                json={"name": OLLAMA_MODEL}
                            )
                            if model_info_response.status_code == 200:
                                model_details = model_info_response.json()
                                status_info["model_info"] = {
                                    "name": model_details.get("modelfile", "").split("\n")[0] if model_details.get("modelfile") else OLLAMA_MODEL,
                                    "size": model_details.get("size", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"),
                                    "modified_at": model_details.get("modified_at", ""),
                                    "parameters": model_details.get("parameters", {}),
                                    "template": model_details.get("template", "")[:100] + "..." if model_details.get("template") else ""
                                }
                        except Exception as model_info_error:
                            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏: {model_info_error}")
                            status_info["model_info"] = {"error": str(model_info_error)}
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
                        try:
                            test_response = await client.post(
                                OLLAMA_URL,
                                json={
                                    "model": OLLAMA_MODEL,
                                    "prompt": "–¢–µ—Å—Ç",
                                    "stream": False,
                                    "options": {"num_predict": 1}
                                },
                                timeout=15.0
                            )
                            
                            if test_response.status_code == 200:
                                status_info["ready_for_work"] = True
                                test_data = test_response.json()
                                status_info["test_response_time"] = test_data.get("total_duration", 0) / 1000000  # –Ω–∞–Ω–æ—Å–µ–∫—É–Ω–¥—ã –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                                print(f"‚úÖ –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                            else:
                                status_info["error"] = f"–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ –≤–µ—Ä–Ω—É–ª –∫–æ–¥ {test_response.status_code}"
                                print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ—É—Å–ø–µ—à–µ–Ω: {test_response.status_code}")
                                
                        except Exception as test_error:
                            status_info["error"] = f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(test_error)}"
                            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {test_error}")
                    else:
                        status_info["error"] = f"–ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Ä–µ–¥–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö: {available_models}"
                        print(f"‚ùå –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                        
                        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∫–æ–º–∞–Ω–¥—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
                        status_info["suggested_command"] = f"ollama pull {OLLAMA_MODEL}"
                        
                else:
                    status_info["error"] = f"Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–∫–æ–¥ {health_response.status_code})"
                    print(f"‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {health_response.status_code}")
                    
            except httpx.TimeoutException:
                status_info["error"] = "–¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É"
                print("‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama")
            except httpx.ConnectError:
                status_info["error"] = "–ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä ollama –∑–∞–ø—É—â–µ–Ω"
                print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama")
            except Exception as connection_error:
                status_info["error"] = f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {str(connection_error)}"
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {connection_error}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π —Å—Ç–∞—Ç—É—Å
        if status_info["ready_for_work"]:
            status_info["status"] = "ready"
            status_info["message"] = f"‚úÖ Ollama –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—å—é {OLLAMA_MODEL}"
        elif status_info["model_loaded"]:
            status_info["status"] = "model_loaded_but_not_ready"
            status_info["message"] = f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {OLLAMA_MODEL} –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã"
        elif status_info["server_available"]:
            status_info["status"] = "server_available_model_missing"
            status_info["message"] = f"‚ö†Ô∏è Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –º–æ–¥–µ–ª—å {OLLAMA_MODEL} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        else:
            status_info["status"] = "server_unavailable"
            status_info["message"] = "‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
        
        return status_info
        
    except Exception as e:
        error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ Ollama: {str(e)}"
        print(f"‚ùå {error_msg}")
        return {
            "ollama_url": OLLAMA_URL,
            "model_name": OLLAMA_MODEL,
            "server_available": False,
            "model_loaded": False,
            "ready_for_work": False,
            "status": "error",
            "message": "‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞",
            "error": error_msg,
            "last_check": datetime.now().isoformat()
        }


@app.get("/api/v1/recommendations")
async def list_recommendations() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Recommendation).order_by(Recommendation.id.desc())
        )
        items = [
            {"id": rec.id, "text": rec.text, "links": rec.links}
            for rec in result.scalars()
        ]
    return items


@app.get("/api/v1/domains")
async def list_domains() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤ —Å –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Domain).order_by(Domain.updated_at.desc())
        )
        items = []
        for domain in result.scalars().all():
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏
            actual_posts_count_result = await session.execute(
                select(func.count(WordPressPost.id))
                .where(WordPressPost.domain_id == domain.id)
            )
            actual_posts_count = actual_posts_count_result.scalar()
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ
            if domain.total_posts != actual_posts_count:
                domain.total_posts = actual_posts_count
                await session.commit()
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç –≤ —Å–µ—Å—Å–∏–∏
                await session.refresh(domain)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            latest_analysis_result = await session.execute(
                select(AnalysisHistory)
                .where(AnalysisHistory.domain_id == domain.id)
                .order_by(AnalysisHistory.created_at.desc())
                .limit(1)
            )
            latest_analysis = latest_analysis_result.scalar_one_or_none()
            
            # –ü–æ–ª—É—á–∞–µ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            active_recommendations_result = await session.execute(
                select(LinkRecommendation)
                .where(LinkRecommendation.domain_id == domain.id)
                .where(LinkRecommendation.status == 'active')
            )
            active_recommendations_count = len(active_recommendations_result.scalars().all())
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Å–∞–π—Ç–æ–≤
            insights_result = await session.execute(
                select(CumulativeInsight)
                .where(CumulativeInsight.domain_id == domain.id)
                .where(CumulativeInsight.status == 'discovered')
            )
            insights_count = len(insights_result.scalars().all())
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            clusters_result = await session.execute(
                select(ThematicClusterAnalysis)
                .where(ThematicClusterAnalysis.domain_id == domain.id)
            )
            clusters_count = len(clusters_result.scalars().all())
            
            items.append({
                "id": domain.id,
                "name": domain.name,
                "display_name": domain.display_name,
                "description": domain.description,
                "total_posts": domain.total_posts,
                "total_analyses": domain.total_analyses,
                "last_analysis_at": domain.last_analysis_at.isoformat() if domain.last_analysis_at else None,
                "created_at": domain.created_at.isoformat(),
                "updated_at": domain.updated_at.isoformat(),
                "is_indexed": domain.total_posts > 0,
                "latest_recommendations": latest_analysis.recommendations if latest_analysis else [],
                "latest_recommendations_count": len(latest_analysis.recommendations) if latest_analysis else 0,
                "latest_recommendations_date": latest_analysis.created_at.isoformat() if latest_analysis else None,
                # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                "cumulative_recommendations": active_recommendations_count,
                "cumulative_insights": insights_count,
                "thematic_clusters": clusters_count,
                "intelligence_level": min(1.0, (active_recommendations_count * 0.1 + insights_count * 0.2 + clusters_count * 0.15))
            })
    return items


@app.get("/api/v1/analysis_history")
async def list_analysis_history() -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤ WordPress —Å–∞–π—Ç–æ–≤ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(AnalysisHistory, Domain)
            .join(Domain, AnalysisHistory.domain_id == Domain.id)
            .order_by(AnalysisHistory.created_at.desc())
        )
        items = []
        for analysis, domain in result:
            items.append({
                "id": analysis.id,
                "domain": domain.name,
                "domain_display_name": domain.display_name,
                "posts_analyzed": analysis.posts_analyzed,
                "connections_found": analysis.connections_found,
                "recommendations_generated": analysis.recommendations_generated,
                "thematic_analysis": analysis.thematic_analysis,
                "semantic_metrics": analysis.semantic_metrics,
                "quality_assessment": analysis.quality_assessment,
                "llm_model_used": analysis.llm_model_used,
                "processing_time_seconds": analysis.processing_time_seconds,
                "created_at": analysis.created_at.isoformat(),
                "completed_at": analysis.completed_at.isoformat() if analysis.completed_at else None,
                "recommendations": analysis.recommendations,
                "summary": f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ {domain.name}: {analysis.posts_analyzed} —Å—Ç–∞—Ç–µ–π, {analysis.recommendations_generated} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            })
    return items


@app.get("/api/v1/analysis_history/{analysis_id}")
async def get_analysis_details(analysis_id: int) -> dict[str, object]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(AnalysisHistory, Domain)
            .join(Domain, AnalysisHistory.domain_id == Domain.id)
            .where(AnalysisHistory.id == analysis_id)
        )
        analysis_data = result.first()
        if not analysis_data:
            raise HTTPException(status_code=404, detail="–ê–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        analysis, domain = analysis_data
        
        return {
            "id": analysis.id,
            "domain": domain.name,
            "domain_info": {
                "name": domain.name,
                "display_name": domain.display_name,
                "description": domain.description,
                "language": domain.language,
                "category": domain.category,
                "total_posts": domain.total_posts,
                "total_analyses": domain.total_analyses
            },
            "posts_analyzed": analysis.posts_analyzed,
            "connections_found": analysis.connections_found,
            "recommendations_generated": analysis.recommendations_generated,
            "thematic_analysis": analysis.thematic_analysis,
            "semantic_metrics": analysis.semantic_metrics,
            "quality_assessment": analysis.quality_assessment,
            "llm_model_used": analysis.llm_model_used,
            "processing_time_seconds": analysis.processing_time_seconds,
            "created_at": analysis.created_at.isoformat(),
            "completed_at": analysis.completed_at.isoformat() if analysis.completed_at else None,
            "recommendations": analysis.recommendations
        }


@app.get("/api/v1/cumulative_insights/{domain_id}")
async def get_cumulative_insights(domain_id: int) -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(CumulativeInsight)
            .where(CumulativeInsight.domain_id == domain_id)
            .order_by(CumulativeInsight.impact_score.desc())
        )
        insights = []
        for insight in result.scalars().all():
            insights.append({
                "id": insight.id,
                "type": insight.insight_type,
                "category": insight.insight_category,
                "title": insight.title,
                "description": insight.description,
                "evidence": insight.evidence,
                "impact_score": insight.impact_score,
                "confidence_level": insight.confidence_level,
                "actionability": insight.actionability,
                "status": insight.status,
                "applied_count": insight.applied_count,
                "created_at": insight.created_at.isoformat()
            })
    return insights


@app.get("/api/v1/thematic_clusters/{domain_id}")
async def get_thematic_clusters(domain_id: int) -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(ThematicClusterAnalysis)
            .where(ThematicClusterAnalysis.domain_id == domain_id)
            .order_by(ThematicClusterAnalysis.linkability_potential.desc())
        )
        clusters = []
        for cluster in result.scalars().all():
            clusters.append({
                "id": cluster.id,
                "name": cluster.cluster_name,
                "description": cluster.cluster_description,
                "keywords": cluster.cluster_keywords,
                "article_count": cluster.article_count,
                "coherence_score": cluster.coherence_score,
                "diversity_score": cluster.diversity_score,
                "linkability_potential": cluster.linkability_potential,
                "evolution_stage": cluster.evolution_stage,
                "growth_trend": cluster.growth_trend,
                "created_at": cluster.created_at.isoformat()
            })
    return clusters


@app.get("/api/v1/cumulative_recommendations/{domain_id}")
async def get_cumulative_recommendations(domain_id: int) -> list[dict[str, object]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_id)
            .where(LinkRecommendation.status == 'active')
            .order_by(LinkRecommendation.quality_score.desc())
        )
        recommendations = []
        for rec in result.scalars().all():
            recommendations.append({
                "id": rec.id,
                "anchor_text": rec.anchor_text,
                "reasoning": rec.reasoning,
                "quality_score": rec.quality_score,
                "generation_count": rec.generation_count,
                "improvement_iterations": rec.improvement_iterations,
                "status": rec.status,
                "created_at": rec.created_at.isoformat(),
                "updated_at": rec.updated_at.isoformat()
            })
    return recommendations


@app.delete("/api/v1/clear_data")
async def clear_all_data() -> dict[str, str]:
    """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)."""
    try:
        from sqlalchemy import text
        
        async with AsyncSessionLocal() as session:
            # –û—á–∏—â–∞–µ–º –Ω–æ–≤—ã–µ –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            await session.execute(text("DELETE FROM cumulative_insights"))
            await session.execute(text("DELETE FROM thematic_cluster_analysis"))
            await session.execute(text("DELETE FROM link_recommendations"))
            await session.execute(text("DELETE FROM semantic_connections"))
            
            # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã
            await session.execute(text("DELETE FROM analysis_history"))
            await session.execute(text("DELETE FROM article_embeddings")) 
            await session.execute(text("DELETE FROM wordpress_posts"))
            await session.execute(text("DELETE FROM recommendations"))
            
            # –°–±—Ä–æ—Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–∞–≤—Ç–æ–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç)
            await session.execute(text("ALTER SEQUENCE cumulative_insights_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE thematic_cluster_analysis_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE link_recommendations_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE semantic_connections_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE analysis_history_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE article_embeddings_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE wordpress_posts_id_seq RESTART WITH 1"))
            await session.execute(text("ALTER SEQUENCE recommendations_id_seq RESTART WITH 1"))
            
            await session.commit()
        
        # –û—á–∏—â–∞–µ–º ChromaDB
        try:
            if chroma_client:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏ —É–¥–∞–ª—è–µ–º –∏—Ö
                collections = chroma_client.list_collections()
                for collection in collections:
                    chroma_client.delete_collection(name=collection.name)
                    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ ChromaDB –∫–æ–ª–ª–µ–∫—Ü–∏—è: {collection.name}")
                
                # –û—á–∏—â–∞–µ–º –∫–µ—à–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
                rag_manager.domain_collections.clear()
                cumulative_intelligence.connection_cache.clear()
                cumulative_intelligence.cluster_cache.clear()
                cumulative_intelligence.insight_cache.clear()
                print("üóëÔ∏è –û—á–∏—â–µ–Ω—ã –∫–µ—à–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤")
        except Exception as chroma_error:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ ChromaDB: {chroma_error}")
        
        print("üßπ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ—á–∏—â–µ–Ω—ã")
        return {"status": "ok", "message": "–í—Å–µ –¥–∞–Ω–Ω—ã–µ –∏ –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ—á–∏—â–µ–Ω—ã"}
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {str(e)}")


@app.get("/")
async def root():
    """–†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥."""
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url="http://localhost:3000")
