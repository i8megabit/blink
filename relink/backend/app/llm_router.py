"""
üß† –ï–¥–∏–Ω—ã–π LLM-–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ reLink

–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–º RAG-–ø–æ–¥—Ö–æ–¥–µ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º –≤ —Å–µ—Ä–≤–∏—Å–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å Ollama.
"""

import asyncio
import aiohttp
import json
import logging
from typing import Dict, List, Optional, Any, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import time
from contextlib import asynccontextmanager

from .config import settings
from .database import get_db
from .models import LLMRequest, LLMResponse, LLMEmbedding
from .cache import cache_manager
from .exceptions import LLMServiceError, OllamaConnectionError

logger = logging.getLogger(__name__)

class LLMServiceType(Enum):
    """–¢–∏–ø—ã LLM-—Å–µ—Ä–≤–∏—Å–æ–≤"""
    SEO_RECOMMENDATIONS = "seo_recommendations"
    DIAGRAM_GENERATION = "diagram_generation"
    CONTENT_ANALYSIS = "content_analysis"
    BENCHMARK_SERVICE = "benchmark_service"
    LLM_TUNING = "llm_tuning"

@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–∑–∞–ø—Ä–æ—Å–∞"""
    service_type: LLMServiceType
    prompt: str
    context: Optional[Dict[str, Any]] = None
    model: str = "qwen2.5:7b-instruct-turbo"  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è Apple Silicon
    temperature: float = 0.7
    max_tokens: int = 2048
    use_rag: bool = True
    cache_ttl: int = 3600  # 1 —á–∞—Å
    priority: int = 1  # 1-10, –≥–¥–µ 10 - –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ LLM-–æ—Ç–≤–µ—Ç–∞"""
    content: str
    service_type: LLMServiceType
    model_used: str
    tokens_used: int
    response_time: float
    cached: bool = False
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class LLMRouter:
    """
    üß† –ï–¥–∏–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä LLM —Å RAG-–ø–æ–¥—Ö–æ–¥–æ–º
    
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —É—Å–ø–µ—à–Ω–æ–º –æ–ø—ã—Ç–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
    - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
    - RAG —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ fallback
    """
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(5)  # –ú–∞–∫—Å–∏–º—É–º 5 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.request_queue = asyncio.Queue()
        self.processing = False
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "cached_responses": 0,
            "avg_response_time": 0.0
        }
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä"""
        await self.start()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        await self.stop()
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        if not self.session:
            timeout = aiohttp.ClientTimeout(total=300)  # 5 –º–∏–Ω—É—Ç
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers={"Content-Type": "application/json"}
            )
            logger.info("üöÄ LLM Router started")
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        if self.session:
            await self.session.close()
            self.session = None
            logger.info("üõë LLM Router stopped")
    
    def _generate_cache_key(self, request: LLMRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        content = f"{request.service_type.value}:{request.prompt}:{request.model}:{request.temperature}"
        if request.context:
            content += f":{json.dumps(request.context, sort_keys=True)}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    async def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            cached_data = await cache_manager.get(f"llm:{cache_key}")
            if cached_data:
                logger.info(f"üì¶ Cache hit for {cache_key[:16]}...")
                return LLMResponse(**cached_data, cached=True)
        except Exception as e:
            logger.warning(f"Cache error: {e}")
        return None
    
    async def _cache_response(self, cache_key: str, response: LLMResponse, ttl: int):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞"""
        try:
            cache_data = {
                "content": response.content,
                "service_type": response.service_type.value,
                "model_used": response.model_used,
                "tokens_used": response.tokens_used,
                "response_time": response.response_time,
                "metadata": response.metadata
            }
            await cache_manager.set(f"llm:{cache_key}", cache_data, ttl)
            logger.info(f"üíæ Cached response for {cache_key[:16]}...")
        except Exception as e:
            logger.warning(f"Cache error: {e}")
    
    async def _generate_rag_context(self, request: LLMRequest) -> str:
        """
        üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è RAG-–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —É—Å–ø–µ—à–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
        - –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        - –û–±–æ–≥–∞—â–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        - –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤
        """
        if not request.use_rag:
            return request.prompt
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            embedding = await self._get_embedding(request.prompt)
            
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
            relevant_knowledge = await self._search_knowledge_base(
                embedding, 
                request.service_type,
                limit=3
            )
            
            if relevant_knowledge:
                context_parts = [request.prompt]
                context_parts.append("\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
                for knowledge in relevant_knowledge:
                    context_parts.append(f"- {knowledge['content']}")
                
                enhanced_prompt = "\n".join(context_parts)
                logger.info(f"üß† RAG enhanced prompt for {request.service_type.value}")
                return enhanced_prompt
            
        except Exception as e:
            logger.warning(f"RAG error: {e}")
        
        return request.prompt
    
    async def _get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            async with self.semaphore:
                async with self.session.post(
                    f"{settings.OLLAMA_URL}/api/embeddings",
                    json={"model": "qwen2.5:7b-instruct-turbo", "prompt": text}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("embedding", [])
                    else:
                        logger.error(f"Embedding error: {response.status}")
                        return []
        except Exception as e:
            logger.error(f"Embedding request failed: {e}")
            return []
    
    async def _search_knowledge_base(
        self, 
        embedding: List[float], 
        service_type: LLMServiceType,
        limit: int = 3
    ) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î (Chroma, Pinecone, etc.)
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return []
        except Exception as e:
            logger.error(f"Knowledge base search failed: {e}")
            return []
    
    async def _make_ollama_request(self, request: LLMRequest) -> LLMResponse:
        """
        üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ Apple Silicon M4
        
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:
        - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
        - –¢–∞–π–º–∞—É—Ç—ã –∏ retry
        - Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        start_time = time.time()
        
        try:
            async with self.semaphore:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ —Å RAG
                enhanced_prompt = await self._generate_rag_context(request)
                
                # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama —Å Apple Silicon –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                ollama_request = {
                    "model": request.model,
                    "prompt": enhanced_prompt,
                    "stream": False,
                    "options": {
                        "temperature": request.temperature,
                        "num_predict": request.max_tokens,
                        # üöÄ APPLE SILICON M4 –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò
                        "num_gpu": 1,                    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
                        "num_thread": 8,                 # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è M4
                        "num_ctx": 4096,                 # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                        "batch_size": 512,               # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        "f16_kv": True,                  # 16-–±–∏—Ç–Ω—ã–µ –∫–ª—é—á–∏-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                        "use_mmap": True,                # Memory mapping –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                        "use_mlock": True,               # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –ø–∞–º—è—Ç–∏
                        "rope_freq_base": 10000,         # RoPE –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞
                        "rope_freq_scale": 0.5,          # RoPE –º–∞—Å—à—Ç–∞–± —á–∞—Å—Ç–æ—Ç—ã
                        "top_p": 0.9,                    # Top-p sampling
                        "top_k": 40,                     # Top-k sampling
                        "repeat_penalty": 1.1,           # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                        "seed": 42                       # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
                    }
                }
                
                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
                async with self.session.post(
                    f"{settings.OLLAMA_URL}/api/generate",
                    json=ollama_request
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        response_time = time.time() - start_time
                        
                        return LLMResponse(
                            content=data.get("response", ""),
                            service_type=request.service_type,
                            model_used=request.model,
                            tokens_used=data.get("eval_count", 0),
                            response_time=response_time,
                            metadata={
                                "prompt_tokens": data.get("prompt_eval_count", 0),
                                "total_duration": data.get("total_duration", 0),
                                "apple_silicon_optimized": True,
                                "gpu_used": True,
                                "batch_size": 512,
                                "context_length": 4096
                            }
                        )
                    else:
                        error_text = await response.text()
                        raise OllamaConnectionError(f"Ollama error {response.status}: {error_text}")
                        
        except asyncio.TimeoutError:
            raise LLMServiceError("Request timeout")
        except Exception as e:
            raise LLMServiceError(f"Request failed: {e}")
    
    async def process_request(self, request: LLMRequest) -> LLMResponse:
        """
        üéØ –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM-–∑–∞–ø—Ä–æ—Å–æ–≤
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π pipeline:
        1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        2. RAG-–æ–±–æ–≥–∞—â–µ–Ω–∏–µ
        3. –ó–∞–ø—Ä–æ—Å –∫ Ollama
        4. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        5. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        """
        self.stats["total_requests"] += 1
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞
            cache_key = self._generate_cache_key(request)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
            if request.cache_ttl > 0:
                cached_response = await self._get_cached_response(cache_key)
                if cached_response:
                    self.stats["cached_responses"] += 1
                    return cached_response
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama
            response = await self._make_ollama_request(request)
            
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if request.cache_ttl > 0:
                await self._cache_response(cache_key, response, request.cache_ttl)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats["successful_requests"] += 1
            self.stats["avg_response_time"] = (
                (self.stats["avg_response_time"] * (self.stats["successful_requests"] - 1) + 
                 response.response_time) / self.stats["successful_requests"]
            )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            logger.info(
                f"‚úÖ {request.service_type.value} completed in {response.response_time:.2f}s "
                f"(tokens: {response.tokens_used})"
            )
            
            return response
            
        except Exception as e:
            self.stats["failed_requests"] += 1
            logger.error(f"‚ùå {request.service_type.value} failed: {e}")
            
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            return LLMResponse(
                content=f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                service_type=request.service_type,
                model_used=request.model,
                tokens_used=0,
                response_time=time.time() - start_time,
                error=str(e)
            )
    
    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        return {
            **self.stats,
            "active_connections": self.semaphore._value,
            "queue_size": self.request_queue.qsize() if hasattr(self.request_queue, 'qsize') else 0
        }
    
    async def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Ollama"""
        try:
            async with self.session.get(f"{settings.OLLAMA_URL}/api/tags") as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞
llm_router = LLMRouter()

# –£—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def generate_seo_recommendations(prompt: str, context: Optional[Dict] = None) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SEO-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.SEO_RECOMMENDATIONS,
        prompt=prompt,
        context=context,
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.6,  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è SEO –∑–∞–¥–∞—á
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def generate_diagram(prompt: str, diagram_type: str = "architecture") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SVG –¥–∏–∞–≥—Ä–∞–º–º—ã —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.DIAGRAM_GENERATION,
        prompt=f"–°–æ–∑–¥–∞–π SVG –¥–∏–∞–≥—Ä–∞–º–º—É —Ç–∏–ø–∞ '{diagram_type}': {prompt}",
        context={"diagram_type": diagram_type},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.8,  # –í—ã—Å–æ–∫–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –¥–∏–∞–≥—Ä–∞–º–º
        max_tokens=4096   # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è SVG
    )
    response = await llm_router.process_request(request)
    return response.content

async def analyze_content(content: str, analysis_type: str = "general") -> str:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.CONTENT_ANALYSIS,
        prompt=f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞: {analysis_type}): {content}",
        context={"analysis_type": analysis_type},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.5,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def run_benchmark(benchmark_type: str, parameters: Dict[str, Any]) -> str:
    """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.BENCHMARK_SERVICE,
        prompt=f"–í—ã–ø–æ–ª–Ω–∏ –±–µ–Ω—á–º–∞—Ä–∫ —Ç–∏–ø–∞ '{benchmark_type}' —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {json.dumps(parameters)}",
        context={"benchmark_type": benchmark_type, "parameters": parameters},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content

async def tune_llm_model(model_config: Dict[str, Any], tuning_params: Dict[str, Any]) -> str:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
    request = LLMRequest(
        service_type=LLMServiceType.LLM_TUNING,
        prompt=f"–ù–∞—Å—Ç—Ä–æ–π –º–æ–¥–µ–ª—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π: {json.dumps(model_config)} –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {json.dumps(tuning_params)}",
        context={"model_config": model_config, "tuning_params": tuning_params},
        model="qwen2.5:7b-instruct-turbo",
        temperature=0.4,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        max_tokens=2048
    )
    response = await llm_router.process_request(request)
    return response.content 