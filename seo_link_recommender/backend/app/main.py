"""FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""

from __future__ import annotations

import asyncio
import json
import os
import re
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple

import chromadb
import httpx
import nltk
import numpy as np
import ollama
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import (
    DateTime, Float, ForeignKey, Index, Integer, JSON, String, Text,
    func, select
)
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column, relationship

# –ó–∞–≥—Ä—É–∑–∫–∞ NLTK –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

# –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
RUSSIAN_STOP_WORDS = set(stopwords.words('russian'))

# üîí –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –°–ï–ú–ê–§–û–† –î–õ–Ø –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –ù–ê–ì–†–£–ó–ö–ò –ù–ê OLLAMA
# –ú–∞–∫—Å–∏–º—É–º 1 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ Ollama –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º—ã
OLLAMA_SEMAPHORE = asyncio.Semaphore(1)

@dataclass
class SemanticEntity:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—É—â–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ LLM."""
    entity_type: str
    value: str
    confidence: float
    context: str

@dataclass
class ThematicCluster:
    """–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä —Å—Ç–∞—Ç–µ–π."""
    cluster_id: str
    theme: str
    keywords: List[str]
    articles_count: int
    semantic_density: float

@dataclass
class AIThought:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—ã—Å–ª—å –ò–ò —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
    thought_id: str
    stage: str  # analyzing, connecting, evaluating, optimizing
    content: str
    confidence: float
    semantic_weight: float
    related_concepts: List[str]
    reasoning_chain: List[str]
    timestamp: datetime
    
@dataclass
class SemanticConnection:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏."""
    source_concept: str
    target_concept: str
    connection_type: str  # semantic, causal, hierarchical, temporal
    strength: float
    evidence: List[str]
    context_keywords: Set[str]

class WebSocketManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""

    def __init__(self) -> None:
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, client_id: str) -> None:
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞."""
        await websocket.accept()
        self.active_connections[client_id] = websocket
        print(f"üîå WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω: {client_id}")

    def disconnect(self, client_id: str) -> None:
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            print(f"üîå WebSocket –æ—Ç–∫–ª—é—á–µ–Ω: {client_id}")

    async def send_progress(self, client_id: str, message: dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–ª–∏–µ–Ω—Ç—É."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json(message)
                print(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω {client_id}: {message}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ {client_id}: {e}")

    async def send_error(self, client_id: str, error: str, details: str = "") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ—à–∏–±–∫–∏ –∫–ª–∏–µ–Ω—Ç—É."""
        await self.send_progress(client_id, {
            "type": "error",
            "message": error,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })

    async def send_step(self, client_id: str, step: str, current: int, total: int, details: str = "") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ."""
        await self.send_progress(client_id, {
            "type": "progress",
            "step": step,
            "current": current,
            "total": total,
            "percentage": round((current / total) * 100) if total > 0 else 0,
            "details": details,
            "timestamp": datetime.now().isoformat()
        })

    async def send_ollama_info(self, client_id: str, info: dict) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞–±–æ—Ç–µ Ollama."""
        await self.send_progress(client_id, {
            "type": "ollama",
            "info": info,
            "timestamp": datetime.now().isoformat()
        })

    async def send_ai_thinking(self, client_id: str, thought: str, thinking_stage: str = "analyzing", emoji: str = "ü§î") -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ '–º—ã—Å–ª–µ–π' –ò–ò –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json({
                    "type": "ai_thinking",
                    "thought": thought,
                    "thinking_stage": thinking_stage,
                    "emoji": emoji,
                    "timestamp": datetime.now().isoformat()
                })
                print(f"üß† –ú—ã—Å–ª—å –ò–ò –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ {client_id}: {thought[:50]}...")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –º—ã—Å–ª–∏ {client_id}: {e}")
    
    async def send_enhanced_ai_thinking(self, client_id: str, ai_thought: AIThought) -> None:
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º—ã—Å–ª–µ–π –ò–ò —Å –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json({
                    "type": "enhanced_ai_thinking",
                    "thought_id": ai_thought.thought_id,
                    "stage": ai_thought.stage,
                    "content": ai_thought.content,
                    "confidence": ai_thought.confidence,
                    "semantic_weight": ai_thought.semantic_weight,
                    "related_concepts": ai_thought.related_concepts,
                    "reasoning_chain": ai_thought.reasoning_chain,
                    "timestamp": ai_thought.timestamp.isoformat()
                })
                print(f"üî¨ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –º—ã—Å–ª—å –ò–ò –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ {client_id}: {ai_thought.stage}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –º—ã—Å–ª–∏ {client_id}: {e}")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
chroma_client: Optional[Any] = None
tfidf_vectorizer: Optional[Any] = None

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã (–±—É–¥—É—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤)
websocket_manager: Optional[WebSocketManager] = None
thought_generator: Optional['IntelligentThoughtGenerator'] = None
rag_manager: Optional['AdvancedRAGManager'] = None
cumulative_manager: Optional['CumulativeIntelligenceManager'] = None

app = FastAPI()

# –î–æ–±–∞–≤–ª—è–µ–º CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
static_dir = Path(__file__).parent.parent.parent / "frontend"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
static_dir = Path(__file__).parent.parent.parent / "frontend"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")


class Base(DeclarativeBase):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–µ–π."""


class Domain(Base):
    """–ú–æ–¥–µ–ª—å –¥–æ–º–µ–Ω–æ–≤ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è."""

    __tablename__ = "domains"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(255), unique=True, index=True)
    display_name: Mapped[str] = mapped_column(String(500))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    language: Mapped[str] = mapped_column(String(10), default="ru")
    category: Mapped[str] = mapped_column(String(100), nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_active: Mapped[bool] = mapped_column(default=True)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_posts: Mapped[int] = mapped_column(Integer, default=0)
    total_analyses: Mapped[int] = mapped_column(Integer, default=0)
    last_analysis_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    posts: Mapped[List["WordPressPost"]] = relationship("WordPressPost", back_populates="domain_ref", cascade="all, delete-orphan")
    analyses: Mapped[List["AnalysisHistory"]] = relationship("AnalysisHistory", back_populates="domain_ref", cascade="all, delete-orphan")
    themes: Mapped[List["ThematicGroup"]] = relationship("ThematicGroup", back_populates="domain_ref", cascade="all, delete-orphan")


class ThematicGroup(Base):
    """–ú–æ–¥–µ–ª—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏."""

    __tablename__ = "thematic_groups"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    name: Mapped[str] = mapped_column(String(200))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    keywords: Mapped[list[str]] = mapped_column(JSON)
    semantic_signature: Mapped[str] = mapped_column(Text)  # TF-IDF –ø–æ–¥–ø–∏—Å—å —Ç–µ–º—ã
    articles_count: Mapped[int] = mapped_column(Integer, default=0)
    avg_semantic_density: Mapped[float] = mapped_column(Float, default=0.0)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="themes")
    posts: Mapped[List["WordPressPost"]] = relationship("WordPressPost", back_populates="thematic_group")

    __table_args__ = (
        Index("idx_thematic_groups_domain_semantic", "domain_id", "avg_semantic_density"),
    )


class WordPressPost(Base):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å—Ç–∞—Ç–µ–π WordPress —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–ª—è–º–∏."""

    __tablename__ = "wordpress_posts"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    thematic_group_id: Mapped[int] = mapped_column(Integer, ForeignKey("thematic_groups.id"), nullable=True, index=True)
    wp_post_id: Mapped[int] = mapped_column(Integer)

    # –ö–æ–Ω—Ç–µ–Ω—Ç–Ω—ã–µ –ø–æ–ª—è
    title: Mapped[str] = mapped_column(Text)
    content: Mapped[str] = mapped_column(Text)
    excerpt: Mapped[str] = mapped_column(Text, nullable=True)
    link: Mapped[str] = mapped_column(Text, index=True)

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –¥–ª—è LLM
    semantic_summary: Mapped[str] = mapped_column(Text, nullable=True)  # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è LLM
    key_concepts: Mapped[list[str]] = mapped_column(JSON, default=list)  # –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
    entity_mentions: Mapped[list[dict]] = mapped_column(JSON, default=list)  # –£–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
    content_type: Mapped[str] = mapped_column(String(50), nullable=True)  # —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–≥–∞–π–¥, –æ–±–∑–æ—Ä, –Ω–æ–≤–æ—Å—Ç—å)
    difficulty_level: Mapped[str] = mapped_column(String(20), nullable=True)  # —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    target_audience: Mapped[str] = mapped_column(String(100), nullable=True)  # —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è

    # –ú–µ—Ç—Ä–∏–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    content_quality_score: Mapped[float] = mapped_column(Float, default=0.0)
    semantic_richness: Mapped[float] = mapped_column(Float, default=0.0)  # –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏–∫–∏
    linkability_score: Mapped[float] = mapped_column(Float, default=0.0)  # –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ —Å—Ç–∞—Ç—É—Å—ã
    published_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_analyzed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="posts")
    thematic_group: Mapped["ThematicGroup"] = relationship("ThematicGroup", back_populates="posts")
    embeddings: Mapped[List["ArticleEmbedding"]] = relationship("ArticleEmbedding", back_populates="post", cascade="all, delete-orphan")

    __table_args__ = (
        Index("idx_wp_posts_domain_theme", "domain_id", "thematic_group_id"),
        Index("idx_wp_posts_semantic_scores", "semantic_richness", "linkability_score"),
        Index("idx_wp_posts_published", "published_at"),
    )


class ArticleEmbedding(Base):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏."""

    __tablename__ = "article_embeddings"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)

    # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embedding_type: Mapped[str] = mapped_column(String(50))  # 'title', 'content', 'summary', 'full'
    vector_model: Mapped[str] = mapped_column(String(100))  # –º–æ–¥–µ–ª—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    embedding_vector: Mapped[str] = mapped_column(Text)  # JSON –≤–µ–∫—Ç–æ—Ä–∞
    dimension: Mapped[int] = mapped_column(Integer)  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context_window: Mapped[int] = mapped_column(Integer, nullable=True)  # —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞
    preprocessing_info: Mapped[dict] = mapped_column(JSON, default=dict)  # –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ
    quality_metrics: Mapped[dict] = mapped_column(JSON, default=dict)  # –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    post: Mapped["WordPressPost"] = relationship("WordPressPost", back_populates="embeddings")

    __table_args__ = (
        Index("idx_embeddings_post_type", "post_id", "embedding_type"),
    )


class SemanticConnection(Base):
    """–ú–æ–¥–µ–ª—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π."""

    __tablename__ = "semantic_connections"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    source_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    target_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)

    # –¢–∏–ø—ã —Å–≤—è–∑–µ–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
    connection_type: Mapped[str] = mapped_column(String(50))  # 'semantic', 'topical', 'hierarchical', 'sequential', 'complementary'
    strength: Mapped[float] = mapped_column(Float)  # —Å–∏–ª–∞ —Å–≤—è–∑–∏ (0.0 - 1.0)
    confidence: Mapped[float] = mapped_column(Float)  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–≤—è–∑–∏

    # –ù–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    usage_count: Mapped[int] = mapped_column(Integer, default=0)  # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–≤—è–∑—å –±—ã–ª–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞
    success_rate: Mapped[float] = mapped_column(Float, default=0.0)  # —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è
    evolution_score: Mapped[float] = mapped_column(Float, default=0.0)  # —ç–≤–æ–ª—é—Ü–∏—è —Å–≤—è–∑–∏ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
    connection_context: Mapped[str] = mapped_column(Text, nullable=True)  # –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–≤—è–∑–∏
    suggested_anchor: Mapped[str] = mapped_column(String(200), nullable=True)  # –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π –∞–Ω–∫–æ—Ä
    alternative_anchors: Mapped[list[str]] = mapped_column(JSON, default=list)  # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∞–Ω–∫–æ—Ä—ã
    bidirectional: Mapped[bool] = mapped_column(default=False)  # –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å–≤—è–∑—å

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
    semantic_tags: Mapped[list[str]] = mapped_column(JSON, default=list)  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏
    theme_intersection: Mapped[str] = mapped_column(String(200), nullable=True)  # –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ç–µ–º

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    validated_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    last_recommended_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    __table_args__ = (
        Index("idx_semantic_connections_strength", "strength"),
        Index("idx_semantic_connections_usage", "usage_count", "success_rate"),
        Index("idx_semantic_connections_evolution", "evolution_score"),
    )


class LinkRecommendation(Base):
    """–ú–æ–¥–µ–ª—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π –∏ —ç–≤–æ–ª—é—Ü–∏–µ–π."""

    __tablename__ = "link_recommendations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)
    source_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)
    target_post_id: Mapped[int] = mapped_column(Integer, ForeignKey("wordpress_posts.id"), index=True)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    anchor_text: Mapped[str] = mapped_column(String(300))
    reasoning: Mapped[str] = mapped_column(Text)
    quality_score: Mapped[float] = mapped_column(Float, default=0.0)  # –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

    # –ù–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    generation_count: Mapped[int] = mapped_column(Integer, default=1)  # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞—Å—å
    improvement_iterations: Mapped[int] = mapped_column(Integer, default=0)  # –∏—Ç–µ—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–∏—è
    status: Mapped[str] = mapped_column(String(50), default='active')  # active, deprecated, improved

    # –°–≤—è–∑—å —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é
    semantic_connection_id: Mapped[int] = mapped_column(Integer, ForeignKey("semantic_connections.id"), nullable=True)

    # –≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    previous_version_id: Mapped[int] = mapped_column(Integer, ForeignKey("link_recommendations.id"), nullable=True)
    improvement_reason: Mapped[str] = mapped_column(Text, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")
    source_post: Mapped["WordPressPost"] = relationship("WordPressPost", foreign_keys=[source_post_id])
    target_post: Mapped["WordPressPost"] = relationship("WordPressPost", foreign_keys=[target_post_id])
    semantic_connection: Mapped["SemanticConnection"] = relationship("SemanticConnection")
    previous_version: Mapped["LinkRecommendation"] = relationship("LinkRecommendation", remote_side=[id])

    __table_args__ = (
        Index("idx_link_recommendations_quality", "quality_score"),
        Index("idx_link_recommendations_status", "status"),
        Index("idx_link_recommendations_generation", "generation_count"),
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å–≤—è–∑–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫-—Ü–µ–ª—å –≤ —Ä–∞–º–∫–∞—Ö –¥–æ–º–µ–Ω–∞
        Index("idx_link_recommendations_unique", "domain_id", "source_post_id", "target_post_id"),
    )


class ThematicClusterAnalysis(Base):
    """–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –∏—Ö —ç–≤–æ–ª—é—Ü–∏–∏."""

    __tablename__ = "thematic_cluster_analysis"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)

    # –ö–ª–∞—Å—Ç–µ—Ä
    cluster_name: Mapped[str] = mapped_column(String(200))
    cluster_keywords: Mapped[list[str]] = mapped_column(JSON)
    cluster_description: Mapped[str] = mapped_column(Text)

    # –°—Ç–∞—Ç—å–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    article_ids: Mapped[list[int]] = mapped_column(JSON)  # ID —Å—Ç–∞—Ç–µ–π –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    article_count: Mapped[int] = mapped_column(Integer)

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    coherence_score: Mapped[float] = mapped_column(Float)  # —Å–≤—è–∑–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∞
    diversity_score: Mapped[float] = mapped_column(Float)  # —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    linkability_potential: Mapped[float] = mapped_column(Float)  # –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –ª–∏–Ω–∫–æ–≤–∫–∏

    # –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
    related_clusters: Mapped[dict] = mapped_column(JSON, default=dict)  # —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –∏—Ö –≤–µ—Å–∞
    cross_cluster_opportunities: Mapped[list[dict]] = mapped_column(JSON, default=list)  # –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö —Å–≤—è–∑–µ–π

    # –≠–≤–æ–ª—é—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
    evolution_stage: Mapped[str] = mapped_column(String(50), default='emerging')  # emerging, mature, declining
    growth_trend: Mapped[float] = mapped_column(Float, default=0.0)  # —Ç—Ä–µ–Ω–¥ —Ä–æ—Å—Ç–∞

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")

    __table_args__ = (
        Index("idx_thematic_cluster_coherence", "coherence_score"),
        Index("idx_thematic_cluster_linkability", "linkability_potential"),
        Index("idx_thematic_cluster_evolution", "evolution_stage", "growth_trend"),
    )


class CumulativeInsight(Base):
    """–ú–æ–¥–µ–ª—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""

    __tablename__ = "cumulative_insights"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)

    # –¢–∏–ø –∏–Ω—Å–∞–π—Ç–∞
    insight_type: Mapped[str] = mapped_column(String(100))  # 'pattern', 'gap', 'opportunity', 'trend'
    insight_category: Mapped[str] = mapped_column(String(100))  # 'semantic', 'structural', 'thematic'

    # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–∞
    title: Mapped[str] = mapped_column(String(300))
    description: Mapped[str] = mapped_column(Text)
    evidence: Mapped[dict] = mapped_column(JSON)  # –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ

    # –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
    impact_score: Mapped[float] = mapped_column(Float)  # –≤–∞–∂–Ω–æ—Å—Ç—å –∏–Ω—Å–∞–π—Ç–∞
    confidence_level: Mapped[float] = mapped_column(Float)  # —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∏–Ω—Å–∞–π—Ç–µ
    actionability: Mapped[float] = mapped_column(Float)  # –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å

    # –°–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
    related_posts: Mapped[list[int]] = mapped_column(JSON, default=list)
    related_clusters: Mapped[list[int]] = mapped_column(JSON, default=list)
    related_connections: Mapped[list[int]] = mapped_column(JSON, default=list)

    # –°—Ç–∞—Ç—É—Å –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
    status: Mapped[str] = mapped_column(String(50), default='discovered')  # discovered, validated, applied
    applied_count: Mapped[int] = mapped_column(Integer, default=0)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    validated_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain")

    __table_args__ = (
        Index("idx_cumulative_insights_impact", "impact_score"),
        Index("idx_cumulative_insights_type", "insight_type", "insight_category"),
        Index("idx_cumulative_insights_status", "status"),
    )


class AnalysisHistory(Base):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏."""

    __tablename__ = "analysis_history"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    domain_id: Mapped[int] = mapped_column(Integer, ForeignKey("domains.id"), index=True)

    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    posts_analyzed: Mapped[int] = mapped_column(Integer)
    connections_found: Mapped[int] = mapped_column(Integer)
    recommendations_generated: Mapped[int] = mapped_column(Integer)

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    recommendations: Mapped[list[dict]] = mapped_column(JSON)
    thematic_analysis: Mapped[dict] = mapped_column(JSON, default=dict)  # –∞–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏–∫
    semantic_metrics: Mapped[dict] = mapped_column(JSON, default=dict)  # —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    quality_assessment: Mapped[dict] = mapped_column(JSON, default=dict)  # –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞

    # LLM –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    llm_model_used: Mapped[str] = mapped_column(String(100))
    llm_context_size: Mapped[int] = mapped_column(Integer, nullable=True)
    processing_time_seconds: Mapped[float] = mapped_column(Float, nullable=True)

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    completed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    domain_ref: Mapped["Domain"] = relationship("Domain", back_populates="analyses")

    __table_args__ = (
        Index("idx_analysis_history_domain_date", "domain_id", "created_at"),
    )


class ModelConfiguration(Base):
    """–ú–æ–¥–µ–ª—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π."""

    __tablename__ = "model_configurations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    model_name: Mapped[str] = mapped_column(String(200), unique=True, index=True)
    display_name: Mapped[str] = mapped_column(String(300))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    model_type: Mapped[str] = mapped_column(String(50))  # ollama, openai, anthropic

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    default_parameters: Mapped[dict] = mapped_column(JSON, default=dict)  # temperature, top_p, etc.
    context_size: Mapped[int] = mapped_column(Integer, default=4096)
    max_tokens: Mapped[int] = mapped_column(Integer, default=2048)

    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
    seo_optimized_params: Mapped[dict] = mapped_column(JSON, default=dict)
    benchmark_params: Mapped[dict] = mapped_column(JSON, default=dict)
    creative_params: Mapped[dict] = mapped_column(JSON, default=dict)

    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    avg_tokens_per_second: Mapped[float] = mapped_column(Float, nullable=True)
    memory_usage_mb: Mapped[float] = mapped_column(Float, nullable=True)
    quality_score: Mapped[float] = mapped_column(Float, nullable=True)

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    is_active: Mapped[bool] = mapped_column(default=True)
    is_available: Mapped[bool] = mapped_column(default=False)  # –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è
    last_checked_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    updated_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    benchmark_runs: Mapped[List["BenchmarkRun"]] = relationship("BenchmarkRun", back_populates="model_config")

    __table_args__ = (
        Index("idx_model_configs_active_available", "is_active", "is_available"),
    )


class BenchmarkRun(Base):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""

    __tablename__ = "benchmark_runs"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    name: Mapped[str] = mapped_column(String(300))
    description: Mapped[str] = mapped_column(Text, nullable=True)
    benchmark_type: Mapped[str] = mapped_column(String(100))  # seo_basic, seo_advanced, performance

    # –°—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª–∏
    model_config_id: Mapped[int] = mapped_column(Integer, ForeignKey("model_configurations.id"), index=True)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞
    test_cases_config: Mapped[dict] = mapped_column(JSON)  # –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤
    iterations: Mapped[int] = mapped_column(Integer, default=3)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results: Mapped[dict] = mapped_column(JSON)  # –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    metrics: Mapped[dict] = mapped_column(JSON)  # –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    started_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    completed_at: Mapped[datetime] = mapped_column(DateTime, nullable=True)
    duration_seconds: Mapped[float] = mapped_column(Float, nullable=True)

    # –°—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    status: Mapped[str] = mapped_column(String(50), default='pending')  # pending, running, completed, failed
    error_message: Mapped[str] = mapped_column(Text, nullable=True)

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    overall_score: Mapped[float] = mapped_column(Float, nullable=True)
    quality_score: Mapped[float] = mapped_column(Float, nullable=True)
    performance_score: Mapped[float] = mapped_column(Float, nullable=True)
    efficiency_score: Mapped[float] = mapped_column(Float, nullable=True)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    model_config: Mapped["ModelConfiguration"] = relationship("ModelConfiguration", back_populates="benchmark_runs")
    comparisons: Mapped[List["BenchmarkComparison"]] = relationship("BenchmarkComparison", foreign_keys="BenchmarkComparison.run_id", back_populates="run")

    __table_args__ = (
        Index("idx_benchmark_runs_model_status", "model_config_id", "status"),
        Index("idx_benchmark_runs_type_score", "benchmark_type", "overall_score"),
    )


class BenchmarkComparison(Base):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏."""

    __tablename__ = "benchmark_comparisons"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    comparison_name: Mapped[str] = mapped_column(String(300))

    # –°—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º—ã–µ –∑–∞–ø—É—Å–∫–∏
    run_id: Mapped[int] = mapped_column(Integer, ForeignKey("benchmark_runs.id"), index=True)
    baseline_run_id: Mapped[int] = mapped_column(Integer, ForeignKey("benchmark_runs.id"), nullable=True)

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_results: Mapped[dict] = mapped_column(JSON)  # –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    performance_delta: Mapped[float] = mapped_column(Float, nullable=True)  # –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    quality_delta: Mapped[float] = mapped_column(Float, nullable=True)  # –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞

    # –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    summary: Mapped[str] = mapped_column(Text, nullable=True)
    recommendations: Mapped[list[str]] = mapped_column(JSON, default=list)

    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    run: Mapped["BenchmarkRun"] = relationship("BenchmarkRun", foreign_keys=[run_id], back_populates="comparisons")
    baseline_run: Mapped["BenchmarkRun"] = relationship("BenchmarkRun", foreign_keys=[baseline_run_id])

    __table_args__ = (
        Index("idx_benchmark_comparisons_runs", "run_id", "baseline_run_id"),
    )


class Recommendation(Base):
    """–ú–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)."""

    __tablename__ = "recommendations"

    id: Mapped[int] = mapped_column(Integer, primary_key=True)
    text: Mapped[str] = mapped_column(Text)
    links: Mapped[list[str]] = mapped_column(JSON)


class RecommendRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫."""

    text: str


class WPRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ WordPress-—Å–∞–π—Ç–∞."""

    domain: str
    client_id: Optional[str] = None
    comprehensive: Optional[bool] = False  # –§–ª–∞–≥ –¥–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏


class BenchmarkRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""

    name: str
    description: Optional[str] = None
    benchmark_type: str = "seo_advanced"  # seo_basic, seo_advanced, performance
    models: List[str] = []  # —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    iterations: int = 3
    client_id: Optional[str] = None


class ModelConfigRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏."""

    model_name: str
    display_name: Optional[str] = None
    description: Optional[str] = None
    default_parameters: Optional[dict] = None
    seo_optimized_params: Optional[dict] = None
    benchmark_params: Optional[dict] = None


OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è SEO –∑–∞–¥–∞—á: qwen2.5:7b-instruct-turbo - –ª—É—á—à–∞—è –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct-turbo")

# üéØ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –¢–û–ö–ï–ù–û–í –¥–ª—è –º–æ–¥–µ–ª–∏ qwen2.5:7b
# –ú–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 8192 —Ç–æ–∫–µ–Ω–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞
OPTIMAL_CONTEXT_SIZE = 3072      # –ö–†–ò–¢–ò–ß–ù–û: –°–Ω–∏–∂–∞–µ–º –¥–æ 3K –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (75% –æ—Ç 4096)
OPTIMAL_PREDICTION_SIZE = 800    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞
OPTIMAL_TEMPERATURE = 0.3        # –ë–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
OPTIMAL_TOP_P = 0.85            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
OPTIMAL_TOP_K = 50              # –†–∞—Å—à–∏—Ä—è–µ–º –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
OPTIMAL_REPEAT_PENALTY = 1.08   # –°–Ω–∏–∂–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è

DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql+asyncpg://seo_user:seo_pass@localhost/seo_db",
)

engine = create_async_engine(DATABASE_URL)
AsyncSessionLocal = async_sessionmaker(bind=engine, expire_on_commit=False)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–∏—Å—Ç–µ–º—ã
chroma_client = None
tfidf_vectorizer = None

def initialize_rag_system() -> None:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é RAG-—Å–∏—Å—Ç–µ–º—É —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º."""
    global chroma_client, tfidf_vectorizer
    try:
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π RAG-—Å–∏—Å—Ç–µ–º—ã...")

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        tfidf_vectorizer = TfidfVectorizer(
            max_features=2000,  # —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            ngram_range=(1, 3),  # –¥–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã
            min_df=1,
            max_df=0.85,
            stop_words=list(RUSSIAN_STOP_WORDS),  # —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            analyzer='word',
            lowercase=True,
            token_pattern=r'\b[–∞-—è—ë]{2,}\b|\b[a-z]{2,}\b'  # —Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞
        )

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        chroma_client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=chromadb.Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
        chroma_client = None
        tfidf_vectorizer = None


class AdvancedRAGManager:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π RAG-–º–µ–Ω–µ–¥–∂–µ—Ä —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º."""

    def __init__(self) -> None:
        self.domain_collections = {}
        self.thematic_clusters = {}
        self.semantic_cache = {}


class IntelligentThoughtGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –º—ã—Å–ª–µ–π –ò–ò —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º numpy –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    
    def __init__(self) -> None:
        self.thought_history: List[AIThought] = []
        self.concept_embeddings: Dict[str, np.ndarray] = {}
        self.reasoning_patterns: Dict[str, List[str]] = defaultdict(list)
        self.semantic_network: Dict[str, Set[str]] = defaultdict(set)
        
    def extract_key_concepts(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –∏ NLP."""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è SEO-–∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        seo_patterns = [
            r'\b(?:SEO|—Å–µ–æ|–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è|—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ|–ø–æ–∏—Å–∫–æ–≤–∏–∫)\b',
            r'\b(?:–∫–ª—é—á–µ–≤\w+\s+—Å–ª–æ–≤\w+|keywords?)\b',
            r'\b(?:–∞–Ω–∫–æ—Ä\w*|anchor\w*)\b',
            r'\b(?:—Å—Å—ã–ª–∫\w+|link\w*)\b',
            r'\b(?:–∫–æ–Ω—Ç–µ–Ω—Ç|content)\b',
            r'\b(?:—Å–µ–º–∞–Ω—Ç–∏–∫\w+|semantic\w*)\b'
        ]
        
        concepts = []
        text_lower = text.lower()
        
        for pattern in seo_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            concepts.extend(matches)
            
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        words = word_tokenize(text_lower)
        filtered_words = [word for word in words if word not in RUSSIAN_STOP_WORDS and len(word) > 3]
        concepts.extend(filtered_words[:5])  # –ë–µ—Ä–µ–º —Ç–æ–ø-5 —Å–ª–æ–≤
        
        return list(set(concepts))[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
    
    def calculate_semantic_similarity(self, concepts1: List[str], concepts2: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π."""
        if not concepts1 or not concepts2:
            return 0.0
            
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        all_concepts = list(set(concepts1 + concepts2))
        
        if len(all_concepts) < 2:
            return 1.0 if concepts1 == concepts2 else 0.0
            
        vector1 = np.array([1 if concept in concepts1 else 0 for concept in all_concepts])
        vector2 = np.array([1 if concept in concepts2 else 0 for concept in all_concepts])
        
        # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        if np.linalg.norm(vector1) == 0 or np.linalg.norm(vector2) == 0:
            return 0.0
            
        return float(cosine_similarity([vector1], [vector2])[0][0])
    
    def generate_reasoning_chain(self, stage: str, concepts: List[str], context: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞."""
        reasoning_templates = {
            "analyzing": [
                f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: {', '.join(concepts[:3])}",
                f"–û–ø—Ä–µ–¥–µ–ª—è—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: {context[:100]}...",
                "–û—Ü–µ–Ω–∏–≤–∞—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è SEO"
            ],
            "connecting": [
                f"–ò—â—É —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏: {' ‚Üî '.join(concepts[:2])}",
                "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å —Å—Ç–∞—Ç–µ–π",
                "–í—ã—è–≤–ª—è—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏"
            ],
            "evaluating": [
                "–û—Ü–µ–Ω–∏–≤–∞—é –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π",
                f"–ü—Ä–æ–≤–µ—Ä—è—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π: {', '.join(concepts[:2])}",
                "–†–∞—Å—Å—á–∏—Ç—ã–≤–∞—é –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é SEO-—Ü–µ–Ω–Ω–æ—Å—Ç—å"
            ],
            "optimizing": [
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä—É—é –∞–Ω–∫–æ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                "–£—á–∏—Ç—ã–≤–∞—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç",
                "–§–æ—Ä–º–∏—Ä—É—é —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"
            ]
        }
        
        return reasoning_templates.get(stage, ["–í—ã–ø–æ–ª–Ω—è—é –∞–Ω–∞–ª–∏–∑..."])
    
    async def generate_intelligent_thought(
        self, 
        stage: str, 
        context: str, 
        additional_data: Dict = None
    ) -> AIThought:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –º—ã—Å–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π."""
        
        concepts = self.extract_key_concepts(context)
        reasoning_chain = self.generate_reasoning_chain(stage, concepts, context)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≤–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏
        semantic_weight = 0.5  # –±–∞–∑–æ–≤—ã–π –≤–µ—Å
        if self.thought_history:
            last_thought = self.thought_history[-1]
            similarity = self.calculate_semantic_similarity(
                concepts, last_thought.related_concepts
            )
            semantic_weight = 0.3 + (similarity * 0.7)  # 0.3-1.0 –¥–∏–∞–ø–∞–∑–æ–Ω
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ –∏—Ö –∫–∞—á–µ—Å—Ç–≤–∞
        confidence = min(0.9, 0.4 + (len(concepts) * 0.05) + (len(reasoning_chain) * 0.1))
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –º—ã—Å–ª–∏
        thought_content = self._generate_thought_content(stage, concepts, context, additional_data)
        
        thought = AIThought(
            thought_id=f"{stage}_{datetime.now().strftime('%H%M%S_%f')}",
            stage=stage,
            content=thought_content,
            confidence=confidence,
            semantic_weight=semantic_weight,
            related_concepts=concepts,
            reasoning_chain=reasoning_chain,
            timestamp=datetime.now()
        )
        
        self.thought_history.append(thought)
        self._update_semantic_network(concepts)
        
        return thought
    
    def _generate_thought_content(
        self, 
        stage: str, 
        concepts: List[str], 
        context: str, 
        additional_data: Dict = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –º—ã—Å–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∞–ø–∞ –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π."""
        
        stage_templates = {
            "analyzing": f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é {len(concepts)} –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. " +
                        f"–§–æ–∫—É—Å –Ω–∞: {', '.join(concepts[:2])}. " +
                        f"–ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞: {len(context.split())//10} —Å–µ–≥–º–µ–Ω—Ç–æ–≤.",
            
            "connecting": f"üîó –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏. " +
                         f"–ù–∞–π–¥–µ–Ω–æ {len(self.semantic_network)} —É–∑–ª–æ–≤ –≤ —Å–µ—Ç–∏. " +
                         f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π: {sum(len(connections) for connections in self.semantic_network.values())}",
            
            "evaluating": f"‚öñÔ∏è –û—Ü–µ–Ω–∏–≤–∞—é –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –ø–æ {len(concepts)} –∫—Ä–∏—Ç–µ—Ä–∏—è–º. " +
                         f"–°—Ä–µ–¥–Ω–∏–π –≤–µ—Å —Å–≤—è–∑–µ–π: {np.mean([0.6, 0.7, 0.8]):.2f}",
            
            "optimizing": f"‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. " +
                         f"–ü—Ä–∏–º–µ–Ω—è—é {len(self.reasoning_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. " +
                         f"–¶–µ–ª–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: {', '.join(concepts[:3])}"
        }
        
        base_content = stage_templates.get(stage, "ü§î –í—ã–ø–æ–ª–Ω—è—é –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
        
        if additional_data:
            if "articles_count" in additional_data:
                base_content += f" | –°—Ç–∞—Ç–µ–π: {additional_data['articles_count']}"
            if "recommendations_count" in additional_data:
                base_content += f" | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {additional_data['recommendations_count']}"
                
        return base_content
    
    def _update_semantic_network(self, concepts: List[str]) -> None:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π."""
        for i, concept1 in enumerate(concepts):
            for concept2 in concepts[i+1:]:
                self.semantic_network[concept1].add(concept2)
                self.semantic_network[concept2].add(concept1)
    
    def get_network_insights(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏."""
        if not self.semantic_network:
            return {"status": "empty", "insights": []}
            
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        concept_connections = {
            concept: len(connections) 
            for concept, connections in self.semantic_network.items()
        }
        
        top_concepts = sorted(concept_connections.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "status": "active",
            "total_concepts": len(self.semantic_network),
            "total_connections": sum(len(connections) for connections in self.semantic_network.values()) // 2,
            "top_concepts": top_concepts,
            "network_density": len(self.semantic_network) / max(1, sum(len(connections) for connections in self.semantic_network.values())),
            "insights": [
                f"–î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è: {top_concepts[0][0] if top_concepts else '–Ω/–¥'}",
                f"–°—Ä–µ–¥–Ω—è—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: {np.mean(list(concept_connections.values())):.2f}",
                f"–ö–æ–Ω—Ü–µ–ø—Ü–∏–π —Å –≤—ã—Å–æ–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç—å—é: {sum(1 for _, count in concept_connections.items() if count > 3)}"
            ]
        }

    async def create_semantic_knowledge_base(
        self,
        domain: str,
        posts: List[Dict],
        client_id: Optional[str] = None
    ) -> bool:
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π."""
        if not chroma_client:
            print("‚ùå RAG-—Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return False

        try:
            if client_id:
                await websocket_manager.send_step(client_id, "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", 1, 5, "–ê–Ω–∞–ª–∏–∑ —Ç–µ–º–∞—Ç–∏–∫ –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π...")

            print(f"üß† –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è {domain}...")

            # –û—á–∏—â–∞–µ–º –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_name = domain.replace(".", "_").replace("-", "_")

            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                chroma_client.delete_collection(name=collection_name)
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}: {e}")

            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            collection = chroma_client.create_collection(
                name=collection_name,
                metadata={
                    "domain": domain,
                    "created_at": datetime.now().isoformat(),
                    "rag_version": "2.0",
                    "semantic_analysis": True
                }
            )

            if client_id:
                await websocket_manager.send_step(client_id, "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞", 2, 5, "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π...")

            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π
            enriched_posts = await self._enrich_posts_with_semantics(posts, domain)

            if client_id:
                await websocket_manager.send_step(client_id, "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", 3, 5, "–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤...")

            documents = []
            metadatas = []
            ids = []

            for i, post in enumerate(enriched_posts):
                # –°–æ–∑–¥–∞–µ–º –±–æ–≥–∞—Ç—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
                semantic_text = self._create_llm_friendly_context(post)

                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, —á–∏—Å–ª–∞ –∏ –±—É–ª–µ–≤—ã –∑–Ω–∞—á–µ–Ω–∏—è
                key_concepts = post.get('key_concepts', [])
                key_concepts_str = ', '.join(key_concepts[:5]) if key_concepts else ""  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É

                documents.append(semantic_text)
                metadatas.append({
                    "title": (post.get('title', '') or '')[:300],  # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É
                    "link": post.get('link', '') or '',
                    "content_snippet": (post.get('content', '') or '')[:500],
                    "domain": domain,
                    "post_index": i,
                    "semantic_summary": (post.get('semantic_summary', '') or '')[:500],
                    "key_concepts_str": key_concepts_str,  # –°—Ç—Ä–æ–∫–∞ –≤–º–µ—Å—Ç–æ —Å–ø–∏—Å–∫–∞
                    "key_concepts_count": len(key_concepts),  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∫ —á–∏—Å–ª–æ
                    "content_type": post.get('content_type', 'article'),
                    "difficulty_level": post.get('difficulty_level', 'medium'),
                    "linkability_score": float(post.get('linkability_score', 0.5))  # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —ç—Ç–æ float
                })
                ids.append(f"{collection_name}_{i}")

            if not documents:
                print(f"‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {domain}")
                return False

            if client_id:
                await websocket_manager.send_step(client_id, "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", 4, 5, "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º...")

            # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
            vectorizer = TfidfVectorizer(
                max_features=1000,
                ngram_range=(1, 3),
                min_df=1,
                max_df=0.8,
                stop_words=list(RUSSIAN_STOP_WORDS),
                analyzer='word'
            )

            tfidf_matrix = vectorizer.fit_transform(documents)
            dense_embeddings = tfidf_matrix.toarray().tolist()

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids,
                embeddings=dense_embeddings
            )

            if client_id:
                await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 5, 5, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")

            self.domain_collections[domain] = collection_name

            print(f"üß† –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞: {len(documents)} —Å—Ç–∞—Ç–µ–π –¥–ª—è {domain}")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–∞–∑—ã: {e}")
            return False

    async def _enrich_posts_with_semantics(self, posts: List[Dict], domain: str) -> List[Dict]:
        """–û–±–æ–≥–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
        enriched = []

        for post in posts:
            try:
                title = post.get('title', '')
                content = post.get('content', '')

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
                key_concepts = self._extract_key_concepts(title + ' ' + content)

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_type = self._classify_content_type(title, content)

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                difficulty = self._assess_difficulty(content)

                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫
                linkability = self._calculate_linkability_score(title, content, key_concepts)

                # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ
                semantic_summary = self._create_semantic_summary(title, content, key_concepts)

                enriched_post = {
                    **post,
                    'key_concepts': key_concepts,
                    'content_type': content_type,
                    'difficulty_level': difficulty,
                    'linkability_score': linkability,
                    'semantic_summary': semantic_summary
                }

                enriched.append(enriched_post)

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ {post.get('title', 'unknown')}: {e}")
                enriched.append(post)  # –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å

        return enriched

    def _extract_key_concepts(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
        words = word_tokenize(text.lower())
        words = [w for w in words if w.isalpha() and len(w) > 3 and w not in RUSSIAN_STOP_WORDS]

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        return sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)[:10]

    def _classify_content_type(self, title: str, content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        title_lower = title.lower()

        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if any(word in title_lower for word in ['–∫–∞–∫', '–≥–∞–π–¥', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è']):
            return 'guide'
        elif any(word in title_lower for word in ['–æ–±–∑–æ—Ä', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ç–µ—Å—Ç']):
            return 'review'
        elif any(word in title_lower for word in ['–Ω–æ–≤–æ—Å—Ç–∏', '–∞–Ω–æ–Ω—Å', '—Ä–µ–ª–∏–∑']):
            return 'news'
        elif len(content) < 1000:
            return 'short_article'
        else:
            return 'article'

    def _assess_difficulty(self, content: str) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        words = word_tokenize(content)
        avg_word_length = sum(len(w) for w in words) / len(words) if words else 0

        if avg_word_length < 5:
            return 'easy'
        elif avg_word_length < 7:
            return 'medium'
        else:
            return 'advanced'

    def _calculate_linkability_score(self, title: str, content: str, concepts: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫."""
        score = 0.0

        # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        score += min(len(content) / 2000, 0.4)  # –¥–æ 0.4 –∑–∞ –¥–ª–∏–Ω—É

        # –°–∫–æ—Ä –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        score += min(len(concepts) / 20, 0.3)  # –¥–æ 0.3 –∑–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

        # –°–∫–æ—Ä –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        if any(word in title.lower() for word in ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–ø–æ—á–µ–º—É']):
            score += 0.2

        # –°–∫–æ—Ä –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (–ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        if content.count('.') > 5:  # –º–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            score += 0.1

        return min(score, 1.0)

    def _create_semantic_summary(self, title: str, content: str, concepts: List[str]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–ª—è LLM."""
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 - 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
        sentences = content.split('.')[:3]
        summary = '. '.join(sentences).strip()

        if concepts:
            summary += f" –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã: {', '.join(concepts[:5])}."

        return summary[:500]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

    def _create_llm_friendly_context(self, post: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç LLM-–¥—Ä—É–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏."""
        title = post.get('title', '')
        summary = post.get('semantic_summary', '')
        concepts = post.get('key_concepts', [])
        content_type = post.get('content_type', 'article')
        difficulty = post.get('difficulty_level', 'medium')

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        context_parts = [
            f"–ó–ê–ì–û–õ–û–í–û–ö: {title}",
            f"–¢–ò–ü: {content_type}",
            f"–°–õ–û–ñ–ù–û–°–¢–¨: {difficulty}",
            f"–û–ü–ò–°–ê–ù–ò–ï: {summary}"
        ]

        if concepts:
            context_parts.append(f"–ö–õ–Æ–ß–ï–í–´–ï_–¢–ï–ú–´: {', '.join(concepts[:7])}")

        return ' | '.join(context_parts)

    async def get_semantic_recommendations(
        self,
        domain: str,
        limit: int = 25,
        min_linkability: float = 0.2  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
    ) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        if domain not in self.domain_collections:
            return []

        try:
            collection = chroma_client.get_collection(name=self.domain_collections[domain])

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            results = collection.get(
                limit=limit * 2,  # –±–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                include=['metadatas', 'documents']
            )

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
            filtered_articles = []
            for i, metadata in enumerate(results['metadatas']):
                linkability_score = metadata.get('linkability_score', 0.0)
                if linkability_score >= min_linkability:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º key_concepts –∏–∑ —Å—Ç—Ä–æ–∫–∏
                    key_concepts_str = metadata.get('key_concepts_str', '')
                    key_concepts = [kc.strip() for kc in key_concepts_str.split(',') if kc.strip()] if key_concepts_str else []

                    filtered_articles.append({
                        'title': metadata['title'],
                        'link': metadata['link'],
                        'content': metadata.get('content_snippet', ''),
                        'semantic_summary': metadata.get('semantic_summary', ''),
                        'key_concepts': key_concepts,
                        'content_type': metadata.get('content_type', 'article'),
                        'difficulty_level': metadata.get('difficulty_level', 'medium'),
                        'linkability_score': linkability_score,
                        'domain': metadata['domain']
                    })

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Å–æ–∑–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
            filtered_articles.sort(key=lambda x: x['linkability_score'], reverse=True)

            print(f"üéØ –ü–æ–ª—É—á–µ–Ω–æ {len(filtered_articles)} —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
            return filtered_articles[:limit]

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return []


class CumulativeIntelligenceManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π."""

    def __init__(self) -> None:
        self.connection_cache = {}
        self.cluster_cache = {}
        self.insight_cache = {}

    async def analyze_existing_connections(self, domain: str, session: AsyncSession) -> dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–≤—è–∑–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        domain_result = await session.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = domain_result.scalar_one_or_none()
        if not domain_obj:
            return {"existing_connections": 0, "existing_recommendations": 0}

        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
        connections_result = await session.execute(
            select(SemanticConnection)
            .join(WordPressPost, SemanticConnection.source_post_id == WordPressPost.id)
            .where(WordPressPost.domain_id == domain_obj.id)
        )
        existing_connections = connections_result.scalars().all()

        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations_result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_obj.id)
            .where(LinkRecommendation.status == 'active')
        )
        existing_recommendations = recommendations_result.scalars().all()

        return {
            "existing_connections": len(existing_connections),
            "existing_recommendations": len(existing_recommendations),
            "connections": existing_connections,
            "recommendations": existing_recommendations,
            "domain_id": domain_obj.id
        }

    async def discover_thematic_clusters(self, domain_id: int, posts: list, session: AsyncSession) -> list:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã."""
        from sklearn.cluster import KMeans
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np

        if len(posts) < 3:
            return []

        # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        texts = [f"{post['title']} {post.get('content', '')[:500]}" for post in posts]
        vectorizer = TfidfVectorizer(
            max_features=100,
            ngram_range=(1, 2),
            stop_words=list(RUSSIAN_STOP_WORDS)
        )

        try:
            tfidf_matrix = vectorizer.fit_transform(texts)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            n_clusters = min(max(2, len(posts) // 5), 8)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
            clusters = []
            feature_names = vectorizer.get_feature_names_out()

            for cluster_id in range(n_clusters):
                cluster_posts = [posts[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
                if len(cluster_posts) < 2:
                    continue

                # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                cluster_center = kmeans.cluster_centers_[cluster_id]
                top_features_idx = cluster_center.argsort()[-10:][::-1]
                cluster_keywords = [feature_names[i] for i in top_features_idx]

                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
                coherence_score = float(np.mean([cluster_center[i] for i in top_features_idx[:5]]))
                diversity_score = float(len(set(cluster_keywords)) / len(cluster_keywords))
                linkability_potential = coherence_score * diversity_score * len(cluster_posts) / len(posts)

                cluster_analysis = ThematicClusterAnalysis(
                    domain_id=domain_id,
                    cluster_name=f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id + 1}: {', '.join(cluster_keywords[:3])}",
                    cluster_keywords=cluster_keywords,
                    cluster_description=f"–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä –∏–∑ {len(cluster_posts)} —Å—Ç–∞—Ç–µ–π",
                    article_ids=[post.get('id', 0) for post in cluster_posts],
                    article_count=len(cluster_posts),
                    coherence_score=coherence_score,
                    diversity_score=diversity_score,
                    linkability_potential=linkability_potential,
                    evolution_stage='emerging'
                )

                session.add(cluster_analysis)
                clusters.append({
                    'analysis': cluster_analysis,
                    'posts': cluster_posts,
                    'keywords': cluster_keywords
                })

            await session.commit()
            return clusters

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return []

    async def generate_cumulative_insights(self, domain_id: int, analysis_data: dict, session: AsyncSession):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞."""
        insights = []

        # –ò–Ω—Å–∞–π—Ç –æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Å–≤—è–∑–µ–π
        if analysis_data['existing_connections'] > 0:
            connection_density = analysis_data['existing_connections'] / max(analysis_data.get('total_posts', 1), 1)

            if connection_density < 0.1:
                insight = CumulativeInsight(
                    domain_id=domain_id,
                    insight_type='gap',
                    insight_category='structural',
                    title='–ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–≤—è–∑–µ–π',
                    description=f'–î–æ–º–µ–Ω –∏–º–µ–µ—Ç —Ç–æ–ª—å–∫–æ {analysis_data["existing_connections"]} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫—É.',
                    evidence={'connection_density': connection_density, 'total_connections': analysis_data['existing_connections']},
                    impact_score=0.8,
                    confidence_level=0.9,
                    actionability=0.9
                )
                session.add(insight)
                insights.append(insight)

        # –ò–Ω—Å–∞–π—Ç –æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏
        clusters = analysis_data.get('clusters', [])
        if len(clusters) > 1:
            avg_linkability = sum(c['analysis'].linkability_potential for c in clusters) / len(clusters)

            if avg_linkability > 0.3:
                insight = CumulativeInsight(
                    domain_id=domain_id,
                    insight_type='opportunity',
                    insight_category='thematic',
                    title='–í—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–µ–∂—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π',
                    description=f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(clusters)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –≤—ã—Å–æ–∫–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º –ø–µ—Ä–µ–ª–∏–Ω–∫–æ–≤–∫–∏.',
                    evidence={'clusters_count': len(clusters), 'avg_linkability': avg_linkability},
                    impact_score=0.7,
                    confidence_level=0.8,
                    actionability=0.8
                )
                session.add(insight)
                insights.append(insight)

        await session.commit()
        return insights

    async def deduplicate_and_evolve_recommendations(
        self,
        new_recommendations: list,
        domain_id: int,
        session: AsyncSession
    ) -> list:
        """–î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ—Ç –∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        existing_result = await session.execute(
            select(LinkRecommendation)
            .where(LinkRecommendation.domain_id == domain_id)
            .where(LinkRecommendation.status == 'active')
        )
        existing_recommendations = {
            (rec.source_post_id, rec.target_post_id): rec
            for rec in existing_result.scalars().all()
        }

        evolved_recommendations = []

        for new_rec in new_recommendations:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Å—Ç—ã –≤ –ë–î
            source_result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_id)
                .where(WordPressPost.link == new_rec['from'])
            )
            source_post = source_result.scalar_one_or_none()

            target_result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_id)
                .where(WordPressPost.link == new_rec['to'])
            )
            target_post = target_result.scalar_one_or_none()

            if not source_post or not target_post:
                continue

            key = (source_post.id, target_post.id)

            if key in existing_recommendations:
                # –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                existing_rec = existing_recommendations[key]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–ª—É—á—à–∏–ª–∞—Å—å –ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
                new_quality = self._calculate_quality_score(new_rec)
                if new_quality > existing_rec.quality_score:
                    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
                    improved_rec = LinkRecommendation(
                        domain_id=domain_id,
                        source_post_id=source_post.id,
                        target_post_id=target_post.id,
                        anchor_text=new_rec['anchor'],
                        reasoning=new_rec['comment'],
                        quality_score=new_quality,
                        generation_count=existing_rec.generation_count + 1,
                        improvement_iterations=existing_rec.improvement_iterations + 1,
                        previous_version_id=existing_rec.id,
                        improvement_reason=f"–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {new_quality:.2f} > {existing_rec.quality_score:.2f}"
                    )

                    # –ü–æ–º–µ—á–∞–µ–º —Å—Ç–∞—Ä—É—é –∫–∞–∫ —É–ª—É—á—à–µ–Ω–Ω—É—é
                    existing_rec.status = 'improved'

                    session.add(improved_rec)
                    evolved_recommendations.append(new_rec)
                else:
                    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    existing_rec.generation_count += 1
                    existing_rec.updated_at = datetime.utcnow()
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
                quality_score = self._calculate_quality_score(new_rec)
                link_rec = LinkRecommendation(
                    domain_id=domain_id,
                    source_post_id=source_post.id,
                    target_post_id=target_post.id,
                    anchor_text=new_rec['anchor'],
                    reasoning=new_rec['comment'],
                    quality_score=quality_score
                )

                session.add(link_rec)
                evolved_recommendations.append(new_rec)

        await session.commit()
        return evolved_recommendations

    def _calculate_quality_score(self, recommendation: dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        score = 0.0

        # –û—Ü–µ–Ω–∫–∞ –∞–Ω–∫–æ—Ä–∞
        anchor = recommendation.get('anchor', '')
        if len(anchor) > 5:
            score += 0.3
        if any(word in anchor.lower() for word in ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ']):
            score += 0.2

        # –û—Ü–µ–Ω–∫–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
        comment = recommendation.get('comment', '')
        if len(comment) > 20:
            score += 0.3
        if len(comment) > 50:
            score += 0.2

        return min(score, 1.0)


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –±—É–¥–µ—Ç –Ω–∏–∂–µ


async def generate_links(text: str) -> list[str]:
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç Ollama –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—ã—Ö —Å—Å—ã–ª–æ–∫."""
    prompt = (
        "–ü—Ä–µ–¥–ª–æ–∂–∏ –¥–æ –ø—è—Ç–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        "–ö–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É –≤—ã–≤–µ–¥–∏ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ /article/–Ω–∞–∑–≤–∞–Ω–∏–µ-—Å—Ç–∞—Ç—å–∏, "
        "–æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö –∏–∑ —Ç–µ–∫—Å—Ç–∞. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–ª–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è. "
        f"–¢–µ–∫—Å—Ç: {text}"
    )
    async with httpx.AsyncClient(timeout=720.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 12 –º–∏–Ω—É—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        response = await client.post(
            OLLAMA_URL,
            json={"model": OLLAMA_MODEL, "prompt": prompt, "stream": False},
            timeout=720,  # 12 –º–∏–Ω—É—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å
        )
    response.raise_for_status()
    data = response.json()
    lines = [line.strip("- \n") for line in data.get("response", "").splitlines()]
    links = [line for line in lines if line]
    return links[:5]


async def fetch_and_store_wp_posts(domain: str, client_id: Optional[str] = None) -> tuple[list[dict[str, str]], dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ WordPress —Å —É–º–Ω–æ–π –¥–µ–ª—å—Ç–∞-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π."""
    print(f"üß† –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞ {domain}")

    if client_id:
        await websocket_manager.send_step(client_id, "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π", 1, 5, "–ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")

    # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –¥–æ–º–µ–Ω
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(Domain).where(Domain.name == domain)
        )
        domain_obj = result.scalar_one_or_none()

        if not domain_obj:
            domain_obj = Domain(
                name=domain,
                display_name=domain,
                description=f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–π –¥–æ–º–µ–Ω –¥–ª—è {domain}",
                language="ru"
            )
            session.add(domain_obj)
            await session.commit()
            await session.refresh(domain_obj)
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –¥–æ–º–µ–Ω: {domain}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        existing_posts_result = await session.execute(
            select(WordPressPost).where(WordPressPost.domain_id == domain_obj.id)
        )
        existing_posts = {post.wp_post_id: post for post in existing_posts_result.scalars().all()}
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(existing_posts)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")

    if client_id:
        await websocket_manager.send_step(client_id, "–ó–∞–≥—Ä—É–∑–∫–∞ —Å WordPress", 2, 5, "–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

    url = f"https://{domain}/wp-json/wp/v2/posts?per_page=50"
    async with httpx.AsyncClient(timeout=120.0) as client:
        response = await client.get(url)
    if response.status_code >= 400:
        raise HTTPException(status_code=400, detail="–°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ WordPress")
    data = response.json()
    if not isinstance(data, list):
        raise HTTPException(status_code=400, detail="–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç WordPress")

    if client_id:
        await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π", 3, 5, f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(data)} —Å—Ç–∞—Ç–µ–π...")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–ª—å—Ç–∞-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    delta_stats = {
        'new_posts': 0,
        'updated_posts': 0,
        'unchanged_posts': 0,
        'removed_posts': 0,
        'total_posts': len(data)
    }

    posts = []
    processed_wp_ids = set()
    seen_urls = set()
    seen_titles = set()

    async with AsyncSessionLocal() as session:
        for item in data:
            try:
                wp_post_id = item["id"]
                processed_wp_ids.add(wp_post_id)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = item.get("content", {}).get("rendered", "")
                excerpt = item.get("excerpt", {}).get("rendered", "")

                # –û—á–∏—â–∞–µ–º HTML
                clean_content = BeautifulSoup(content, 'html.parser').get_text()
                clean_excerpt = BeautifulSoup(excerpt, 'html.parser').get_text() if excerpt else ""

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –Ω–∞—à–µ–º—É –¥–æ–º–µ–Ω—É
                post_link = item["link"]
                if domain.lower() not in post_link.lower():
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç–∞—Ç—å—é —Å —á—É–∂–æ–≥–æ –¥–æ–º–µ–Ω–∞: {post_link}")
                    continue

                # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
                if post_link in seen_urls:
                    print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç URL –ø—Ä–æ–ø—É—â–µ–Ω: {post_link}")
                    continue
                seen_urls.add(post_link)

                # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                title = item["title"]["rendered"]
                title_normalized = title.lower().strip()
                if title_normalized in seen_titles:
                    print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω: {title}")
                    continue
                seen_titles.add(title_normalized)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ—Å—Ç
                existing_post = existing_posts.get(wp_post_id)
                # –ü–æ–ª—É—á–∞–µ–º –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                # modified_str = item.get("modified", datetime.now().isoformat())
                # post_modified = datetime.fromisoformat(modified_str.replace('Z', '+00:00'))

                if existing_post:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ –ø–æ—Å—Ç
                    if (existing_post.title == title and existing_post.content == clean_content and existing_post.link == post_link):                        # –ü–æ—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
                        delta_stats['unchanged_posts'] += 1
                        print(f"‚ö° –ü–æ—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è: {title}")

                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–∑ –ø–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                        posts.append({
                            "title": title,
                            "link": post_link,
                            "content": clean_content[:800].strip()
                        })
                        continue
                    else:
                        # –ü–æ—Å—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è - –æ–±–Ω–æ–≤–ª—è–µ–º
                        existing_post.title = title
                        existing_post.content = clean_content
                        existing_post.excerpt = clean_excerpt
                        existing_post.link = post_link
                        existing_post.updated_at = datetime.utcnow()
                        existing_post.last_analyzed_at = None  # type: ignore # –°–±—Ä–æ—Å –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

                        delta_stats['updated_posts'] += 1
                        print(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω –ø–æ—Å—Ç: {title}")
                else:
                    # –ù–æ–≤—ã–π –ø–æ—Å—Ç - —Å–æ–∑–¥–∞–µ–º
                    wp_post = WordPressPost(
                        domain_id=domain_obj.id,
                        wp_post_id=wp_post_id,
                        title=title,
                        content=clean_content,
                        excerpt=clean_excerpt,
                        link=post_link,
                        published_at=datetime.fromisoformat(
                            item["date"].replace('Z', '+00:00')
                        ) if item.get("date") else None,
                        last_analyzed_at=None  # –ù–æ–≤—ã–π –ø–æ—Å—Ç —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞
                    )
                    session.add(wp_post)

                    delta_stats['new_posts'] += 1
                    print(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ—Å—Ç: {title}")

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                posts.append({
                    "title": title,
                    "link": post_link,
                    "content": clean_content[:800].strip()
                })

            except Exception as exc:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞ {item.get('id', 'unknown')}: {exc}")
                continue

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –Ω–∞ —Å–∞–π—Ç–µ
        for wp_post_id, existing_post in existing_posts.items():
            if wp_post_id not in processed_wp_ids:
                await session.delete(existing_post)
                delta_stats['removed_posts'] += 1
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –ø–æ—Å—Ç: {existing_post.title}")

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –≤ –ë–î
        posts_count_result = await session.execute(
            select(func.count(WordPressPost.id))
            .where(WordPressPost.domain_id == domain_obj.id)
        )
        actual_posts_count = posts_count_result.scalar()

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–º–µ–Ω–∞
        domain_obj.total_posts = actual_posts_count if actual_posts_count is not None else 0
        domain_obj.updated_at = datetime.utcnow()

        print(f"üìä –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–º–µ–Ω–∞: {actual_posts_count} –ø–æ—Å—Ç–æ–≤ –≤ –ë–î")

        await session.commit()

        if client_id:
            await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è", 4, 5, "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")

        print(f"üß† –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(f"   ‚ûï –ù–æ–≤—ã–µ –ø–æ—Å—Ç—ã: {delta_stats['new_posts']}")
        print(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ: {delta_stats['updated_posts']}")
        print(f"   ‚ö° –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {delta_stats['unchanged_posts']}")
        print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω–Ω—ã–µ: {delta_stats['removed_posts']}")
        print(f"   üìä –í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")

        if client_id:
            await websocket_manager.send_step(client_id, "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞", 5, 5,
                f"–ù–æ–≤—ã—Ö: {delta_stats['new_posts']}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {delta_stats['updated_posts']}, –£–¥–∞–ª–µ–Ω–æ: {delta_stats['removed_posts']}")

    return posts, delta_stats


async def generate_comprehensive_domain_recommendations(domain: str, client_id: Optional[str] = None) -> list[dict[str, str]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π."""
    print(f"üß† –ó–∞–ø—É—Å–∫ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")

    analysis_start_time = datetime.now()
    all_recommendations = []

    try:
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–ê–Ω–∞–ª–∏–∑ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π", 1, 12, "–ò–∑—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π...")

        async with AsyncSessionLocal() as session:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å–≤—è–∑–∏
            existing_analysis = await cumulative_manager.analyze_existing_connections(domain, session)
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ {existing_analysis['existing_connections']} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π")
            print(f"üìã –ù–∞–π–¥–µ–Ω–æ {existing_analysis['existing_recommendations']} –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

            domain_obj = await session.get(Domain, existing_analysis['domain_id'])
            if not domain_obj:
                error_msg = f"‚ùå –î–æ–º–µ–Ω {domain} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ë–î"
                print(error_msg)
                if client_id:
                    await websocket_manager.send_error(client_id, error_msg)
                return [], 0.0

            # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –ø–æ—Å—Ç—ã —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            result = await session.execute(
                select(WordPressPost)
                .where(WordPressPost.domain_id == domain_obj.id)
                .order_by(WordPressPost.linkability_score.desc())
            )
            all_posts = result.scalars().all()

        if not all_posts:
            error_msg = "‚ùå –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg)
            return [], 0.0

        print(f"üìä –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {len(all_posts)} —Å—Ç–∞—Ç–µ–π –∏–∑ –ë–î")

        # –®–∞–≥ 2: –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", 2, 12, f"–ê–Ω–∞–ª–∏–∑ {len(all_posts)} —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–∞–º...")

        full_dataset = []
        for post in all_posts:
            full_dataset.append({
                "id": post.id,
                "title": post.title,
                "link": post.link,
                "content": post.content,
                "semantic_summary": post.semantic_summary or "",
                "key_concepts": post.key_concepts or [],
                "content_type": post.content_type or "article",
                "difficulty_level": post.difficulty_level or "medium",
                "linkability_score": post.linkability_score or 0.5,
                "semantic_richness": post.semantic_richness or 0.5
            })

        async with AsyncSessionLocal() as session:
            # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            clusters = await cumulative_manager.discover_thematic_clusters(
                domain_obj.id, full_dataset, session
            )
            print(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(clusters)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

            existing_analysis['clusters'] = clusters
            existing_analysis['total_posts'] = len(all_posts)

        # –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤
        if client_id:
            await websocket_manager.send_step(client_id, "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å–∞–π—Ç–æ–≤", 3, 12, "–ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —É–ª—É—á—à–µ–Ω–∏—è...")

        async with AsyncSessionLocal() as session:
            insights = await cumulative_manager.generate_cumulative_insights(
                domain_obj.id, existing_analysis, session
            )
            print(f"üí° –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(insights)} –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤")

        # –®–∞–≥ 4: –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑", 4, 12, "–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
            await websocket_manager.send_ai_thinking(
                client_id,
                "–°–æ–∑–¥–∞—é –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π –∏ –≤—ã—Å—Ç—Ä–∞–∏–≤–∞—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ...",
                "vectorizing",
                "üßÆ"
            )

        success = await rag_manager.create_semantic_knowledge_base(domain, full_dataset, client_id)
        if not success:
            error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
            print(error_msg)
            if client_id:
                await websocket_manager.send_error(client_id, error_msg)
            return [], 0.0

        # –®–∞–≥ 5: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥
        if client_id:
            await websocket_manager.send_step(client_id, "–£–º–Ω—ã–π –±–∞—Ç—á–∏–Ω–≥", 5, 12, "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º...")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –¥–ª—è –±–æ–ª–µ–µ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        cluster_batches = []
        for cluster in clusters:
            cluster_posts = cluster['posts']
            if len(cluster_posts) >= 2:
                cluster_batches.append({
                    'posts': cluster_posts,
                    'cluster_info': cluster['analysis'],
                    'keywords': cluster['keywords']
                })

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –±–∞—Ç—á–µ–π (–º–∞–∫—Å–∏–º—É–º 5)
        if len(clusters) > 1:
            cross_cluster_count = 0
            max_cross_clusters = 5  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

            for i, cluster1 in enumerate(clusters):
                if cross_cluster_count >= max_cross_clusters:
                    break
                for cluster2 in clusters[i+1:]:
                    if cross_cluster_count >= max_cross_clusters:
                        break
                    # –ú–∏–∫—Å —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    mixed_posts = cluster1['posts'][:2] + cluster2['posts'][:2]
                    cluster_batches.append({
                        'posts': mixed_posts,
                        'cluster_info': None,  # –º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                        'keywords': cluster1['keywords'] + cluster2['keywords'],
                        'cross_cluster': True
                    })
                    cross_cluster_count += 1

        print(f"üß† –°–æ–∑–¥–∞–Ω–æ {len(cluster_batches)} –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –±–∞—Ç—á–µ–π")

        # –®–∞–≥ 6 - 9: –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ –±–∞—Ç—á–∏
        for batch_idx, batch_info in enumerate(cluster_batches, 1):
            if client_id:
                batch_type = "–ú–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π" if batch_info.get('cross_cluster') else "–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π"
                await websocket_manager.send_step(
                    client_id,
                    f"{batch_type} –∞–Ω–∞–ª–∏–∑ {batch_idx}/{len(cluster_batches)}",
                    5 + batch_idx,
                    12,
                    f"–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–º..."
                )

            batch_recommendations = await process_cumulative_batch_with_ollama(
                domain, batch_info, existing_analysis, batch_idx, len(cluster_batches), client_id
            )

            all_recommendations.extend(batch_recommendations)
            print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –±–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω–æ {len(batch_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

        # –®–∞–≥ 10: –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —ç–≤–æ–ª—é—Ü–∏—è
        if client_id:
            await websocket_manager.send_step(client_id, "–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", 10, 12, "–≠–≤–æ–ª—é—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")

        async with AsyncSessionLocal() as session:
            evolved_recommendations = await cumulative_manager.deduplicate_and_evolve_recommendations(
                all_recommendations, domain_obj.id, session
            )
            print(f"üß¨ –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–æ {len(evolved_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

        # –®–∞–≥ 11: –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
        if client_id:
            await websocket_manager.send_step(client_id, "–§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ", 11, 12, "–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏...")
            await websocket_manager.send_ai_thinking(
                client_id,
                "–ü—Ä–∏–º–µ–Ω—è—é –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ —É—Å–ø–µ—à–Ω—ã—Ö —Å–≤—è–∑—è—Ö –∏ —Ä–∞–Ω–∂–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏...",
                "ranking",
                "üéØ"
            )

        final_recommendations = rank_recommendations_with_cumulative_intelligence(
            evolved_recommendations, existing_analysis, insights
        )

        # –®–∞–≥ 12: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        total_analysis_time = (datetime.now() - analysis_start_time).total_seconds()

        if client_id:
            await websocket_manager.send_step(
                client_id,
                "–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                12,
                12,
                f"–ì–æ—Ç–æ–≤–æ! {len(final_recommendations)} —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            )

        print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(final_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∑–∞ {total_analysis_time:.1f}—Å")
        return final_recommendations, total_analysis_time

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}"
        print(error_msg)
        if client_id:
            await websocket_manager.send_error(client_id, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏", str(e))

        error_time = (datetime.now() - analysis_start_time).total_seconds()
        return [], error_time


async def process_batch_with_ollama(
    domain: str,
    batch: List[Dict],
    full_dataset: List[Dict],
    batch_idx: int,
    total_batches: int,
    client_id: Optional[str] = None
) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ Ollama —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""

    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –±–∞—Ç—á–∞
    batch_context = f"""–ê–ù–ê–õ–ò–ó –ë–ê–¢–ß–ê {batch_idx}/{total_batches} –î–û–ú–ï–ù–ê {domain}

–°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ({len(batch)}):
"""

    for i, article in enumerate(batch, 1):
        key_concepts_str = ', '.join(article['key_concepts'][:4]) if article['key_concepts'] else '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã'

        batch_context += f"""
{i}. {article['title']}
   URL: {article['link']}
   –¢–∏–ø: {article['content_type']} | –°–≤—è–∑–Ω–æ—Å—Ç—å: {article['linkability_score']:.2f}
   –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {key_concepts_str}
   –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {article['semantic_summary'][:150]}
   –ö–æ–Ω—Ç–µ–Ω—Ç: {article['content'][:200]}...

"""

    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥—Ä—É–≥–∏—Ö —Å—Ç–∞—Ç–µ–π
    batch_context += f"""–î–û–°–¢–£–ü–ù–´–ï –¶–ï–õ–ò ({len(full_dataset)} —Å—Ç–∞—Ç–µ–π):
"""

    targets_added = 0
    for article in full_dataset[:15]:  # –¢–æ–ø-15 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        if article not in batch and targets_added < 10:  # –ú–∞–∫—Å–∏–º—É–º 10 —Ü–µ–ª–µ–π
            batch_context += f"‚Ä¢ {article['title'][:50]} | {article['link'][:60]}\n"
            targets_added += 1

    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    comprehensive_prompt = f"""{batch_context}

–ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏ –ª–æ–≥–∏—á–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —Å—Ç–∞—Ç—å—è–º–∏

–ö–†–ò–¢–ï–†–ò–ò:
‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–≤—è–∑—å
‚Ä¢ –ü–æ–ª—å–∑–∞ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
‚Ä¢ SEO-—Ü–µ–Ω–Ω–æ—Å—Ç—å

–ü–†–ê–í–ò–õ–ê –∞–Ω–∫–æ—Ä–æ–≤:
‚Ä¢ –û–ø–∏—Å—ã–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
‚Ä¢ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: "—Å–∞–π—Ç", "—Ä–µ—Å—É—Ä—Å", "–ø–æ—Ä—Ç–∞–ª"
‚Ä¢ –ü—Ä–∏–º–µ—Ä—ã: "–ø–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "–ø–æ–ª–Ω—ã–π –æ–±–∑–æ—Ä"

–ù–∞–π–¥–∏ –í–°–ï –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –¥–ª—è —Å—Ç–∞—Ç–µ–π —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞.

–§–û–†–ú–ê–¢: –ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä | –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–û–¢–í–ï–¢:"""

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –º—ã—Å–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
        if client_id:
            analyzing_thought = await thought_generator.generate_intelligent_thought(
                stage="analyzing",
                context=f"–ë–∞—Ç—á {batch_idx} –∏–∑ {len(batch)} —Å—Ç–∞—Ç–µ–π. –î–æ–º–µ–Ω: {domain}. " +
                       f"–°—Ç–∞—Ç—å–∏: {', '.join([article.get('title', '')[:50] for article in batch[:2]])}",
                additional_data={
                    "articles_count": len(batch),
                    "batch_number": batch_idx,
                    "total_batches": total_batches
                }
            )
            await websocket_manager.send_enhanced_ai_thinking(client_id, analyzing_thought)
            
            await websocket_manager.send_ollama_info(client_id, {
                "status": "processing_batch",
                "batch": f"{batch_idx}/{total_batches}",
                "articles_in_batch": len(batch),
                "total_context_size": len(comprehensive_prompt),
                "model": OLLAMA_MODEL
            })

        print(f"ü§ñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –±–∞—Ç—á {batch_idx} —á–µ—Ä–µ–∑ Ollama (—Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(comprehensive_prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")

        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=600.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º-–∞—É—Ç –¥–æ 10 –º–∏–Ω—É—Ç
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": comprehensive_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,    # –ù–µ–º–Ω–æ–≥–æ –ø–æ–≤—ã—à–∞–µ–º –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_ctx": 4096,       # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                        "num_predict": 800,    # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                        "top_p": 0.8,
                        "top_k": 40,
                        "repeat_penalty": 1.05,
                        "num_thread": 6        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
                    }
                },
                timeout=600  # 10 –º–∏–Ω—É—Ç –Ω–∞ –±–∞—Ç—á
            )

        request_time = (datetime.now() - start_time).total_seconds()

        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "batch_completed",
                "batch": f"{batch_idx}/{total_batches}",
                "processing_time": f"{request_time:.1f}s",
                "response_length": len(response.text) if response.status_code == 200 else 0
            })

        if response.status_code != 200:
            print(f"‚ùå Ollama –æ—à–∏–±–∫–∞ –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}: –∫–æ–¥ {response.status_code}")
            return []

        data = response.json()
        content = data.get("response", "")

        print(f"üìù –ë–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º—ã—Å–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if client_id:
            optimizing_thought = await thought_generator.generate_intelligent_thought(
                stage="optimizing",
                context=f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}. " +
                       f"–†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤. " +
                       f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {request_time:.1f}—Å",
                additional_data={
                    "response_size": len(content),
                    "processing_time": request_time,
                    "batch_number": batch_idx
                }
            )
            await websocket_manager.send_enhanced_ai_thinking(client_id, optimizing_thought)

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞
        batch_recommendations = parse_ollama_recommendations(content, domain, full_dataset)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º—ã—Å–ª—å –¥–ª—è —ç—Ç–∞–ø–∞ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if client_id and batch_recommendations:
            evaluating_thought = await thought_generator.generate_intelligent_thought(
                stage="evaluating",
                context=f"–ù–∞–π–¥–µ–Ω–æ {len(batch_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –±–∞—Ç—á–∞ {batch_idx}. " +
                       f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–≤—è–∑–µ–π.",
                additional_data={
                    "recommendations_count": len(batch_recommendations),
                    "batch_number": batch_idx,
                    "success_rate": min(1.0, len(batch_recommendations) / len(batch))
                }
            )
            await websocket_manager.send_enhanced_ai_thinking(client_id, evaluating_thought)

        return batch_recommendations

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–∞—Ç—á–∞ {batch_idx}: {e}")
        return []


async def process_cumulative_batch_with_ollama(
    domain: str,
    batch_info: dict,
    existing_analysis: dict,
    batch_idx: int,
    total_batches: int,
    client_id: Optional[str] = None
) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π."""

    posts = batch_info['posts']
    keywords = batch_info.get('keywords', [])
    is_cross_cluster = batch_info.get('cross_cluster', False)

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π
    batch_context = f"""–ö–£–ú–£–õ–Ø–¢–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –î–û–ú–ï–ù–ê {domain}

–ö–û–ù–¢–ï–ö–°–¢ –ù–ê–ö–û–ü–õ–ï–ù–ù–´–• –ó–ù–ê–ù–ò–ô:
‚Ä¢ –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π: {existing_analysis['existing_connections']}
‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {existing_analysis['existing_recommendations']}
‚Ä¢ –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(existing_analysis.get('clusters', []))}

–°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê ({len(posts)}):
"""

    for i, post in enumerate(posts, 1):
        post_concepts = ', '.join(post.get('key_concepts', [])[:3])
        batch_context += f"""
{i}. {post['title']}
   URL: {post['link']}
   –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏: {post_concepts}
   –¢–∏–ø: {post.get('content_type', 'article')}
   –°–≤—è–∑–Ω–æ—Å—Ç—å: {post.get('linkability_score', 0.5):.2f}
   –ö–æ–Ω—Ç–µ–Ω—Ç: {post.get('content', '')[:150]}...

"""

    if is_cross_cluster:
        batch_context += f"""
üîó –ú–ï–ñ–ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó
–ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è: {', '.join(keywords[:8])}
–ó–ê–î–ê–ß–ê: –ù–∞–π—Ç–∏ –≥–ª—É–±–æ–∫–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏
"""
    else:
        batch_context += f"""
üéØ –í–ù–£–¢–†–ò–ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó
–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(keywords[:6])}
–ó–ê–î–ê–ß–ê: –ù–∞–π—Ç–∏ –ª–æ–≥–∏—á–Ω—ã–µ —Å–≤—è–∑–∏ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
"""

    # –£–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    cumulative_prompt = f"""{batch_context}

–ü–†–ò–ù–¶–ò–ü–´ –ö–£–ú–£–õ–Ø–¢–ò–í–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:
‚úÖ –ò–∑–±–µ–≥–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö {existing_analysis['existing_recommendations']} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å –ù–û–í–´–ï —Å–≤—è–∑–∏, –¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
‚úÖ –£—á–∏—Ç—ã–≤–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ, –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏

–ö–†–ò–¢–ï–†–ò–ò –ö–ê–ß–ï–°–¢–í–ê:
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å
‚Ä¢ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è
‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤—è–∑–µ–π
‚Ä¢ SEO-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å

–§–û–†–ú–ê–¢: –ò–°–¢–û–ß–ù–ò–ö -> –¶–ï–õ–¨ | –∞–Ω–∫–æ—Ä | –≥–ª—É–±–æ–∫–æ–µ_–æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

–û–¢–í–ï–¢:"""

    try:
        if client_id:
            await websocket_manager.send_ollama_info(client_id, {
                "status": "processing_cumulative_batch",
                "batch": f"{batch_idx}/{total_batches}",
                "batch_type": "–º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π" if is_cross_cluster else "–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π",
                "articles_count": len(posts),
                "context_size": len(cumulative_prompt),
                "existing_connections": existing_analysis['existing_connections']
            })

        print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞ {batch_idx} ({'–º–µ–∂–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π' if is_cross_cluster else '–≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π'})")

        start_time = datetime.now()
        async with httpx.AsyncClient(timeout=600.0) as client:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 10 –º–∏–Ω—É—Ç
            response = await client.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": cumulative_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.4,    # –ß—É—Ç—å –≤—ã—à–µ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "num_ctx": 4096,
                        "num_predict": 600,
                        "top_p": 0.85,
                        "top_k": 50,
                        "repeat_penalty": 1.1,
                        "num_thread": 6
                    }
                },
                timeout=600  # 10 –º–∏–Ω—É—Ç
            )

        request_time = (datetime.now() - start_time).total_seconds()

        if response.status_code != 200:
            print(f"‚ùå Ollama –æ—à–∏–±–∫–∞ –¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –±–∞—Ç—á–∞ {batch_idx}: –∫–æ–¥ {response.status_code}")
            return []

        data = response.json()
        content = data.get("response", "")

        print(f"üìù –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á {batch_idx}: –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {request_time:.1f}—Å")

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        batch_recommendations = parse_ollama_recommendations(content, domain, posts)

        return batch_recommendations

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–∞—Ç—á–∞ {batch_idx}: {e}")
        return []


def rank_recommendations_with_cumulative_intelligence(
    recommendations: List[Dict],
    existing_analysis: dict,
    insights: List
) -> List[Dict]:
    """–†–∞–Ω–∂–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞."""

    def cumulative_quality_score(rec):
        score = 0.0

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        anchor = rec.get('anchor', '')
        comment = rec.get('comment', '')

        # –î–ª–∏–Ω–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∫–æ—Ä–∞
        if len(anchor) > 5:
            score += 0.2
        if len(anchor) > 15:
            score += 0.1

        # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∞–Ω–∫–æ—Ä–µ
        quality_words = ['–ø–æ–¥—Ä–æ–±–Ω—ã–π', '–ø–æ–ª–Ω—ã–π', '–¥–µ—Ç–∞–ª—å–Ω—ã–π', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–æ–±–∑–æ—Ä', '–≥–∞–π–¥', '–∞–Ω–∞–ª–∏–∑']
        anchor_quality = sum(1 for word in quality_words if word in anchor.lower()) * 0.15
        score += anchor_quality

        # –ì–ª—É–±–∏–Ω–∞ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
        if len(comment) > 30:
            score += 0.2
        if len(comment) > 60:
            score += 0.1

        # –ë–æ–Ω—É—Å –∑–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏
        semantic_words = ['—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏', '—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏', '–ª–æ–≥–∏—á–µ—Å–∫–∏', '–¥–æ–ø–æ–ª–Ω—è–µ—Ç', '—É–≥–ª—É–±–ª—è–µ—Ç', '—Ä–∞—Å—à–∏—Ä—è–µ—Ç']
        semantic_bonus = sum(1 for word in semantic_words if word in comment.lower()) * 0.1
        score += semantic_bonus

        # –ë–æ–Ω—É—Å –∑–∞ –Ω–æ–≤–∏–∑–Ω—É (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö)
        # –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞, —Ç–∞–∫ –∫–∞–∫ —Ç–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ë–î
        score += 0.1  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–æ–≤—ã–µ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏

        return min(score, 1.0)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É
    ranked_recommendations = sorted(recommendations, key=cumulative_quality_score, reverse=True)

    print(f"üß† –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(ranked_recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")

    return ranked_recommendations[:30]  # –¢–æ–ø-30 —Å–∞–º—ã—Ö —Ü–µ–Ω–Ω—ã—Ö


def parse_ollama_recommendations(content: str, domain: str, posts: List[Dict]) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Ollama."""

    # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    patterns = [
        r"(?P<source>[^->]+) -> (?P<target>[^|]+) \| (?P<anchor>[^|]+) \| (?P<comment>.+)",
        r"(?P<source>[^->]+) -> (?P<target>[^|]+) \| (?P<anchor>[^|]+)",
        r"(?P<source>[^->]+) -> (?P<target>[^|]+)",
        r"(?P<source>[^->]+) -> (?P<target>[^|]+) \| (?P<anchor>[^|]+)",
        r"(?P<source>[^->]+) -> (?P<target>[^|]+) \| (?P<comment>.+)",
        r"(?P<source>[^->]+) -> (?P<target>[^|]+)",
    ]

    recommendations = []
    for line in content.split("\n"):
        line = line.strip()
        if not line:
            continue

        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                source = match.group("source").strip()
                target = match.group("target").strip()
                anchor = match.group("anchor").strip() if match.group("anchor") else ""
                comment = match.group("comment").strip() if match.group("comment") else ""

                # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
                if not target.startswith("http"):
                    target = f"https://{domain}/{target}"

                # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–∫–æ—Ä–∞
                if not anchor:
                    anchor = target.split("/")[-1].replace("-", " ").replace("_", " ")
                    anchor = anchor.strip()
                    if not anchor:
                        anchor = source.split("/")[-1].replace("-", " ").replace("_", " ")
                        anchor = anchor.strip()

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤ –±–∞–∑–µ
                source_post = next((post for post in posts if post['link'] == source), None)
                target_post = next((post for post in posts if post['link'] == target), None)
                if not source_post or not target_post:
                    continue

                recommendations.append({
                    "from": source,
                    "to": target,
                    "anchor": anchor,
                    "comment": comment
                })
                break

    return recommendations


@app.on_event("startup")
async def on_startup() -> None:
    """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG-—Å–∏—Å—Ç–µ–º—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ."""
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG-—Å–∏—Å—Ç–µ–º—É
    initialize_rag_system()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã
    initialize_global_managers()

    # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º –º–æ–¥–µ–ª—å Ollama –≤ —Ñ–æ–Ω–µ (—Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
    async def delayed_warmup():
        await asyncio.sleep(30)  # –ñ–¥–µ–º 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ Ollama
        print("üöÄ –ù–∞—á–∏–Ω–∞—é –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ–≤ Ollama...")
        success = await warmup_ollama()
        if success:
            print("üî• –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≥—Ä–µ—Ç–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        else:
            print("‚ö†Ô∏è –ü—Ä–æ–≥—Ä–µ–≤ –Ω–µ —É–¥–∞–ª—Å—è, –Ω–æ —Å–µ—Ä–≤–∏—Å –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Ä–∞–±–æ—Ç—É")

    asyncio.create_task(delayed_warmup())


def initialize_global_managers():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã."""
    global cumulative_manager, rag_manager, thought_generator, websocket_manager
    
    websocket_manager = WebSocketManager()
    cumulative_manager = CumulativeIntelligenceManager()
    rag_manager = AdvancedRAGManager()
    thought_generator = IntelligentThoughtGenerator()
    
    print("‚úÖ –í—Å–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")


@app.get("/api/v1/ai_insights/semantic_network", response_model=List[dict])
async def get_semantic_network_insights(domain: str, client_id: Optional[str] = None) -> List[dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Å–∞–π—Ç—ã –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    print(f"üß† –ó–∞–ø—Ä–æ—Å –∏–Ω—Å–∞–π—Ç–æ–≤ –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")

    async with AsyncSessionLocal() as session:
        insights = await cumulative_manager.generate_semantic_network_insights(domain, session)

    print(f"üí° –ü–æ–ª—É—á–µ–Ω–æ {len(insights)} –∏–Ω—Å–∞–π—Ç–æ–≤ –æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ—Ç–∏")
    return insights


@app.get("/api/v1/ai_insights/enhanced_analytics", response_model=List[dict])
async def get_enhanced_analytics_insights(domain: str, client_id: Optional[str] = None) -> List[dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    print(f"üìà –ó–∞–ø—Ä–æ—Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain} (client: {client_id})")

    async with AsyncSessionLocal() as session:
        insights = await cumulative_manager.generate_enhanced_analytics_insights(domain, session)

    print(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(insights)} —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å–∞–π—Ç–æ–≤")
    return insights


@app.get("/api/v1/models/available")
async def get_available_models():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama."""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            try:
                response = await client.get("http://ollama:11434/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    for model in data.get("models", []):
                        models.append({
                            "name": model["name"],
                            "size": model.get("size", 0),
                            "digest": model.get("digest", ""),
                            "modified_at": model.get("modified_at", "")
                        })
                    
                    return {
                        "status": "success", 
                        "models": models,
                        "ollama_status": "connected",
                        "total_models": len(models)
                    }
                else:
                    return {
                        "status": "error", 
                        "models": [],
                        "ollama_status": "unavailable",
                        "error": f"Ollama returned status {response.status_code}"
                    }
            except httpx.ConnectError:
                # –ï—Å–ª–∏ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏
                return {
                    "status": "partial", 
                    "models": [
                        {"name": "qwen2.5:7b-turbo", "size": 4300000000, "status": "expected"},
                        {"name": "qwen2.5:7b-instruct-turbo", "size": 4300000000, "status": "expected"},
                        {"name": "qwen2.5:7b", "size": 4300000000, "status": "expected"},
                        {"name": "qwen2.5:7b-instruct", "size": 4300000000, "status": "expected"}
                    ],
                    "ollama_status": "connecting",
                    "message": "Ollama –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è..."
                }
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return {
            "status": "error", 
            "models": [],
            "ollama_status": "error",
            "error": str(e)
        }


@app.get("/api/v1/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API."""
    return {"status": "ok", "timestamp": datetime.now().isoformat()}


@app.get("/api/v1/domains")
async def get_domains():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–º–µ–Ω–æ–≤."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(select(Domain).order_by(Domain.name))
        domains = result.scalars().all()
        
        domain_data = []
        for domain in domains:
            domain_data.append({
                "id": domain.id,
                "name": domain.name,
                "display_name": domain.display_name,
                "total_posts": domain.total_posts,
                "total_analyses": domain.total_analyses,
                "last_analysis_at": domain.last_analysis_at.isoformat() if domain.last_analysis_at else None,
                "is_indexed": domain.total_posts > 0,
                "created_at": domain.created_at.isoformat(),
                "language": domain.language
            })
        
        return {"domains": domain_data}


@app.get("/api/v1/analysis_history")
async def get_analysis_history(domain: Optional[str] = None, limit: int = 50):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤."""
    async with AsyncSessionLocal() as session:
        query = select(AnalysisHistory).order_by(AnalysisHistory.created_at.desc()).limit(limit)
        
        if domain:
            domain_result = await session.execute(select(Domain).where(Domain.name == domain))
            domain_obj = domain_result.scalar_one_or_none()
            if domain_obj:
                query = query.where(AnalysisHistory.domain_id == domain_obj.id)
        
        result = await session.execute(query)
        analyses = result.scalars().all()
        
        history_data = []
        for analysis in analyses:
            history_data.append({
                "id": analysis.id,
                "domain_id": analysis.domain_id,
                "posts_analyzed": analysis.posts_analyzed,
                "connections_found": analysis.connections_found,
                "recommendations_generated": analysis.recommendations_generated,
                "llm_model_used": analysis.llm_model_used,
                "processing_time_seconds": analysis.processing_time_seconds,
                "created_at": analysis.created_at.isoformat(),
                "completed_at": analysis.completed_at.isoformat() if analysis.completed_at else None
            })
        
        return {"history": history_data}


@app.get("/api/v1/benchmark_history")
async def get_benchmark_history(limit: int = 20):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""
    async with AsyncSessionLocal() as session:
        result = await session.execute(
            select(BenchmarkRun)
            .order_by(BenchmarkRun.created_at.desc())
            .limit(limit)
        )
        benchmarks = result.scalars().all()
        
        benchmark_data = []
        for benchmark in benchmarks:
            benchmark_data.append({
                "id": benchmark.id,
                "name": benchmark.name,
                "benchmark_type": benchmark.benchmark_type,
                "status": benchmark.status,
                "overall_score": benchmark.overall_score,
                "quality_score": benchmark.quality_score,
                "performance_score": benchmark.performance_score,
                "duration_seconds": benchmark.duration_seconds,
                "created_at": benchmark.created_at.isoformat(),
                "completed_at": benchmark.completed_at.isoformat() if benchmark.completed_at else None
            })
        
        return {"benchmarks": benchmark_data}


async def warmup_ollama():
    """–ü—Ä–æ–≥—Ä–µ–≤–∞–µ—Ç –º–æ–¥–µ–ª—å Ollama –≤ —Ñ–æ–Ω–µ."""
    try:
        async with OLLAMA_SEMAPHORE:
            async with httpx.AsyncClient(timeout=60.0) as client:
                # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≥—Ä–µ–≤–∞
                response = await client.post(
                    OLLAMA_URL,
                    json={
                        "model": OLLAMA_MODEL,
                        "prompt": "–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞?",
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "num_ctx": 1024,
                            "num_predict": 50
                        }
                    },
                    timeout=60
                )
                if response.status_code == 200:
                    print(f"üî• Ollama –º–æ–¥–µ–ª—å {OLLAMA_MODEL} —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≥—Ä–µ—Ç–∞")
                    return True
                else:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≥—Ä–µ–≤–∞: —Å—Ç–∞—Ç—É—Å {response.status_code}")
                    return False
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≥—Ä–µ–≤–∞ Ollama: {e}")
        return False


# –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è WordPress –∞–Ω–∞–ª–∏–∑–∞
@app.post("/api/v1/wp_index")
async def wp_index_endpoint(request: WPRequest) -> dict:
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è WordPress —Å–∞–π—Ç–∞."""
    try:
        # Fetch and store WordPress posts
        posts, delta_stats = await fetch_and_store_wp_posts(request.domain, request.client_id)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if request.comprehensive:
            recommendations, analysis_time = await generate_comprehensive_domain_recommendations(
                request.domain, request.client_id
            )
        else:
            # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            recommendations = []
            analysis_time = 0.0

        return {
            "status": "success",
            "domain": request.domain,
            "posts_found": len(posts),
            "recommendations": recommendations,
            "delta_stats": delta_stats,
            "analysis_time": analysis_time
        }

    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {request.domain}: {e}"
        print(error_msg)
        return {
            "status": "error",
            "error": str(e),
            "domain": request.domain
        }


@app.websocket("/ws/{client_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str):
    """WebSocket –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∞–Ω–∞–ª–∏–∑–∞."""
    await websocket_manager.connect(websocket, client_id)
    try:
        while True:
            # –ñ–¥–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞ –∏–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            try:
                await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
            except asyncio.TimeoutError:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º ping –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                await websocket.send_json({"type": "ping", "timestamp": datetime.now().isoformat()})
    except WebSocketDisconnect:
        websocket_manager.disconnect(client_id)
    except Exception as e:
        print(f"‚ùå WebSocket –æ—à–∏–±–∫–∞ {client_id}: {e}")
        websocket_manager.disconnect(client_id)


# –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–∞–Ω–Ω—ã—Ö
@app.get("/api/v1/domains/{domain}/posts")
async def get_domain_posts(domain: str, limit: int = 50):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å—Ç—ã –¥–ª—è –¥–æ–º–µ–Ω–∞."""
    async with AsyncSessionLocal() as session:
        domain_result = await session.execute(select(Domain).where(Domain.name == domain))
        domain_obj = domain_result.scalar_one_or_none()
        
        if not domain_obj:
            raise HTTPException(status_code=404, detail="Domain not found")
        
        result = await session.execute(
            select(WordPressPost)
            .where(WordPressPost.domain_id == domain_obj.id)
            .order_by(WordPressPost.created_at.desc())
            .limit(limit)
        )
        posts = result.scalars().all()
        
        posts_data = []
        for post in posts:
            posts_data.append({
                "id": post.id,
                "title": post.title,
                "link": post.link,
                "content_type": post.content_type,
                "difficulty_level": post.difficulty_level,
                "linkability_score": post.linkability_score,
                "semantic_richness": post.semantic_richness,
                "created_at": post.created_at.isoformat(),
                "key_concepts": post.key_concepts[:5] if post.key_concepts else []
            })
        
        return {"posts": posts_data, "total": len(posts_data)}


@app.get("/api/v1/ollama_status")
async def get_ollama_status():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama."""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get("http://ollama:11434/api/tags")
            if response.status_code == 200:
                data = response.json()
                models = data.get("models", [])
                return {
                    "status": "ready",
                    "connection": "connected",
                    "models_count": len(models),
                    "available_models": [model["name"] for model in models[:5]],  # –ü–µ—Ä–≤—ã–µ 5 –º–æ–¥–µ–ª–µ–π
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "status": "error",
                    "connection": "failed",
                    "error": f"HTTP {response.status_code}",
                    "timestamp": datetime.now().isoformat()
                }
    except httpx.ConnectError:
        return {
            "status": "connecting",
            "connection": "connecting",
            "message": "Ollama –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è...",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "connection": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

